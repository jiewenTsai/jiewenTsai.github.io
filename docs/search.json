[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "jw tsai",
    "section": "",
    "text": "About this jw tsai."
  },
  {
    "objectID": "posts/note0716.html",
    "href": "posts/note0716.html",
    "title": "Note0716 (mediation analysis with pymc)",
    "section": "",
    "text": "Code\nimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n#dat = pd.read_csv('data1463_fin3.csv')\n\n\n為了之後還可以跑這個模型，根據原本的資料重新模擬一筆資料。\n(但這邊沒考慮到原本數值之間的相關情形) 已考慮進去！！用多元常態分配模擬了。\n\nacd1eap \\(\\sim N(2.8510193121856177e-05, 0.8921482479092293)\\)\nscleap \\(\\sim N(1.9250494837216173e-06, 0.7154395945457549)\\)\nc1 \\(\\sim Ber(1,0.19822282980177716)\\)\nc2 \\(\\sim Ber(1,0.11483253588516747)\\)\nc3 \\(\\sim Ber(1,0.11551606288448393)\\)\n\n\n\nCode\n'''\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\n'''\n\n\n'\\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\\n'\n\n\n\n\nCode\ndat_mn = np.array(\n    [2.8510193121856177e-05,\n     1.9250494837216173e-06,\n     0.19822282980177716,\n     0.11483253588516747,\n     0.11551606288448393]\n)\ndat_cov = np.array([\n    [ 0.7959285 ,  0.16304568, -0.01541732, -0.01689872, -0.02973076],\n    [ 0.16304568,  0.51185381, -0.05390401, -0.00531492, -0.02327021],\n    [-0.01541732, -0.05390401,  0.15903925, -0.022778  , -0.02291358],\n    [-0.01689872, -0.00531492, -0.022778  ,  0.10171555, -0.01327408],\n    [-0.02973076, -0.02327021, -0.02291358, -0.01327408,  0.10224199]\n])\ndat = np.random.multivariate_normal(dat_mn, dat_cov, 1000)\ndat = pd.DataFrame(dat, columns=['acd1_eap', 'scl_eap', 'c1', 'c2', 'c3'])\ndat['c1'] = dat['c1'] &gt; 0.5\ndat['c2'] = dat['c2'] &gt; 0.5\ndat['c3'] = dat['c3'] &gt; 0.5\n\n\n\n\nCode\ndat\n\n\n\n\n\n\n\n\n\n\nacd1_eap\nscl_eap\nc1\nc2\nc3\n\n\n\n\n0\n0.346316\n-0.792151\nTrue\nFalse\nFalse\n\n\n1\n-0.305374\n1.621382\nFalse\nTrue\nFalse\n\n\n2\n0.563012\n0.828286\nFalse\nFalse\nFalse\n\n\n3\n0.058392\n-0.276411\nFalse\nFalse\nFalse\n\n\n4\n0.181640\n-1.291429\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.717853\n-0.833434\nTrue\nFalse\nFalse\n\n\n996\n-0.812274\n-0.038150\nFalse\nFalse\nFalse\n\n\n997\n-0.461906\n-0.635552\nFalse\nFalse\nFalse\n\n\n998\n0.294100\n0.594206\nFalse\nFalse\nFalse\n\n\n999\n-0.543131\n-0.928300\nTrue\nFalse\nFalse\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\n\nCode\n'''\ndat_dict = {\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \n}\ndat = pd.DataFrame(dat_dict)\n'''\n\n\n\"\\ndat_dict = {\\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \\n}\\ndat = pd.DataFrame(dat_dict)\\n\"\n\n\n\n\nCode\nwith pm.Model() as model:\n    acd1eap = pm.ConstantData('acd1eap', dat.acd1_eap)\n    scleap = pm.ConstantData('scleap', dat.scl_eap)\n    c1 = pm.ConstantData('c1', dat.c1)\n    c2 = pm.ConstantData('c2', dat.c2)\n    c3 = pm.ConstantData('c3', dat.c3)\n\n    # intercept\n    acd1eap_Intercept = pm.Normal('acd1eap_Intercept', mu=0, sigma=100)\n    scleap_Intercept = pm.Normal('scleap_Intercept', mu=0, sigma=100)\n    \n    # noise\n    acd1eap_Sigma = pm.HalfCauchy(\"acd1eap_Sigma\", 1)\n    scleap_Sigma = pm.HalfCauchy(\"scleap_Sigma\", 1)\n\n    # slope\n    acd1eap_scleap = pm.Normal('acd1eap_scleap', mu=0, sigma=100)\n    acd1eap_c1 = pm.Normal('acd1eap_c1', mu=0, sigma=100)\n    acd1eap_c2 = pm.Normal('acd1eap_c2', mu=0, sigma=100)\n    acd1eap_c3 = pm.Normal('acd1eap_c3', mu=0, sigma=100)\n    scleap_c1 = pm.Normal('scleap_c1', mu=0, sigma=100)\n    scleap_c2 = pm.Normal('scleap_c2', mu=0, sigma=100)\n    scleap_c3 = pm.Normal('scleap_c3', mu=0, sigma=100)\n\n    # likelihood\n    pm.Normal(\"y_likelihood\", mu=acd1eap_Intercept + acd1eap_scleap * scleap  + acd1eap_c1 * c1 + acd1eap_c2 * c2 + acd1eap_c3 * c3, sigma =  acd1eap_Sigma, observed = acd1eap  )\n    pm.Normal('m_likelihood', mu=scleap_Intercept + scleap_c1 * c1 + scleap_c2 * c2 + scleap_c3 * c3, sigma = scleap_Sigma, observed = scleap)\n    \n    trace_med = pm.sample(2000, chains=4, cores=4)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [acd1eap_Intercept, scleap_Intercept, acd1eap_Sigma, scleap_Sigma, acd1eap_scleap, acd1eap_c1, acd1eap_c2, acd1eap_c3, scleap_c1, scleap_c2, scleap_c3]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\nCode\npm.model_to_graphviz(model)\n\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n&lt;graphviz.graphs.Digraph at 0x17209fb80&gt;\n\n\n\n\nCode\naz.summary(trace_med)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nacd1eap_Intercept\n0.042\n0.036\n-0.025\n0.109\n0.000\n0.000\n9687.0\n6732.0\n1.0\n\n\nscleap_Intercept\n0.045\n0.030\n-0.013\n0.102\n0.000\n0.000\n10276.0\n6795.0\n1.0\n\n\nacd1eap_scleap\n0.374\n0.038\n0.305\n0.445\n0.000\n0.000\n13151.0\n6209.0\n1.0\n\n\nacd1eap_c1\n0.002\n0.067\n-0.126\n0.122\n0.001\n0.001\n11284.0\n6502.0\n1.0\n\n\nacd1eap_c2\n-0.008\n0.085\n-0.169\n0.148\n0.001\n0.001\n12240.0\n6047.0\n1.0\n\n\nacd1eap_c3\n-0.049\n0.088\n-0.210\n0.120\n0.001\n0.001\n12138.0\n5986.0\n1.0\n\n\nscleap_c1\n-0.181\n0.055\n-0.285\n-0.081\n0.001\n0.000\n11615.0\n6883.0\n1.0\n\n\nscleap_c2\n0.031\n0.071\n-0.091\n0.173\n0.001\n0.001\n11680.0\n7188.0\n1.0\n\n\nscleap_c3\n-0.076\n0.073\n-0.225\n0.053\n0.001\n0.001\n12466.0\n6045.0\n1.0\n\n\nacd1eap_Sigma\n0.881\n0.020\n0.845\n0.919\n0.000\n0.000\n14248.0\n6463.0\n1.0\n\n\nscleap_Sigma\n0.734\n0.017\n0.703\n0.765\n0.000\n0.000\n12699.0\n6039.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{tsai2023,\n  author = {Tsai, JW},\n  title = {Note0716 (Mediation Analysis with Pymc)},\n  date = {2023-07-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTsai, J. (2023, July 16). Note0716 (mediation analysis with\npymc)."
  },
  {
    "objectID": "posts/Lorem Ipsum.html",
    "href": "posts/Lorem Ipsum.html",
    "title": "Lorem Ipsum",
    "section": "",
    "text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#what-is-lorem-ipsum",
    "href": "posts/Lorem Ipsum.html#what-is-lorem-ipsum",
    "title": "Lorem Ipsum",
    "section": "",
    "text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#why-do-we-use-it",
    "href": "posts/Lorem Ipsum.html#why-do-we-use-it",
    "title": "Lorem Ipsum",
    "section": "Why do we use it?",
    "text": "Why do we use it?\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using ‘Content here, content here’, making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for ‘lorem ipsum’ will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like)."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#where-does-it-come-from",
    "href": "posts/Lorem Ipsum.html#where-does-it-come-from",
    "title": "Lorem Ipsum",
    "section": "Where does it come from?",
    "text": "Where does it come from?\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of “de Finibus Bonorum et Malorum” (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, “Lorem ipsum dolor sit amet..”, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from “de Finibus Bonorum et Malorum” by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#where-can-i-get-some",
    "href": "posts/Lorem Ipsum.html#where-can-i-get-some",
    "title": "Lorem Ipsum",
    "section": "Where can I get some?",
    "text": "Where can I get some?\nThere are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don’t look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn’t anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc."
  },
  {
    "objectID": "posts/note0322.html",
    "href": "posts/note0322.html",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "",
    "text": "Goal. In this note, we will demonstrate how to use the output from brms to make (simple slope) testings and plots."
  },
  {
    "objectID": "posts/note0322.html#make-data",
    "href": "posts/note0322.html#make-data",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make data",
    "text": "Make data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayestestR)\nlibrary(rstan)\nlibrary(mvtnorm)\n\n\nNow we have to make a data set including 4 variables: Y, X, M, and W.\nSuppose these four variables follow a multivariate-normal distribution as follows,\nLet X is a treatment (binary data), and M is a response time data (lognormal).\n\\[\\begin{equation}\n\\begin{bmatrix}\nY \\\\ X \\\\M \\\\W\n\\end{bmatrix}\n= \\text{MVN}\\left(\n\\begin{bmatrix}\n0 \\\\0 \\\\ 0\\\\ 0\n\\end{bmatrix},\n\\begin{bmatrix}\n1 & 0.1 & -0.8 & 0.8  \\\\\n0.1 & 1 & -0.6 & 0\\\\\n-0.8 & -0.6 & 1 & 0.6\\\\\n0.8 & 0 & 0.6 & 1\n\\end{bmatrix}\n\\right)\n\\end{equation}\\]\n\n\nCode\nset.seed(12345)\nreal_sigma &lt;- matrix(c(1, 0.1, -0.8, 0.8,\n                      0.1, 1, -0.6, 0,\n                      -0.8, -0.6, 1, 0.6,\n                      0.8, 0, 0.6, 1), nrow = 4)\nreal_mean &lt;- c(0,0,0,0)\n\nreal_data &lt;- rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma)\n\n\nWarning in rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma): sigma is\nnumerically not positive semidefinite\n\n\nCode\ndat &lt;- data.frame(\n  ID = paste0('s', str_pad(1:1000, width = 4, side = 'left', pad = 0)),\n  Y = real_data[,1],\n  X = real_data[,2] &gt; mean(real_data[,2]),\n  M = exp(real_data[,3]),\n  W = real_data[,4]\n)\n\nhead(dat)\n\n\n     ID          Y     X         M          W\n1 s0001  0.3617174  TRUE 0.4790760 -0.1639895\n2 s0002  0.1110080 FALSE 2.0490296  0.2046531\n3 s0003  0.6187169 FALSE 2.6247616  1.4401394\n4 s0004  1.0347476  TRUE 0.5084054  0.6434533\n5 s0005 -1.1477318 FALSE 4.8633851  0.2686688\n6 s0006  0.2802449  TRUE 0.1476185 -1.2412565"
  },
  {
    "objectID": "posts/note0322.html#fit-bayesian-model-in-brms",
    "href": "posts/note0322.html#fit-bayesian-model-in-brms",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Fit Bayesian model in brms",
    "text": "Fit Bayesian model in brms\nNow we specify the formula as follows (in Bayesian).\n\\[\\begin{align}\n\\text{Likelihood.}\\\\\nY &\\sim N(\\mu_y, \\sigma_y^2) \\\\\nM &\\sim \\log N(\\mu_m, \\sigma_m^2) \\\\\n\n\\mu_y &= \\beta_{01} + \\beta_x X + \\beta_m M + \\beta_w W + \\beta _{mw}M \\cdot W \\\\\n\\mu_m &= \\beta_{02} + \\beta_x X \\\\ \\\\\n\n\\text{Priors.}\\\\\n\n\\sigma_y^2, \\sigma_m^2 & \\sim \\text{Exp}(1) \\\\\n\\beta_{01}, ..., \\beta _{x} &\\sim N(0,5)\n\\end{align}\\]\n\n\nCode\nbf1 &lt;- bf(Y~X+M+W+M*W, family = gaussian())\nbf2 &lt;- bf(M~X, family = lognormal())\npriors &lt;- prior(normal(0,5), class = b, resp = Y) + \n  prior(normal(0,5), class = b, resp = M) + \n  prior(exponential(1), class = sigma, resp = Y) +\n  prior(exponential(1), class = sigma, resp = M) \n\n\n\nfit &lt;- brm(\n  bf1+bf2+set_rescor(FALSE), \n  data = dat,\n  cores = 4\n)\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nCode\nprint(fit, digits = 3)\n\n\n Family: MV(gaussian, lognormal) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: Y ~ X + M + W + M * W \n         M ~ X \n   Data: dat (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nY_Intercept    0.648     0.034    0.580    0.714 1.001     3884     3206\nM_Intercept    0.489     0.043    0.405    0.576 1.000     4405     2509\nY_XTRUE       -0.164     0.037   -0.237   -0.089 1.000     4565     3114\nY_M           -0.366     0.010   -0.385   -0.345 1.002     3451     3006\nY_W            0.605     0.019    0.567    0.642 1.001     4067     2995\nY_M:W          0.100     0.006    0.088    0.112 1.002     3095     3208\nM_XTRUE       -0.857     0.061   -0.978   -0.734 1.000     4898     2554\n\nFamily Specific Parameters: \n        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma_Y    0.570     0.013    0.545    0.595 1.000     6063     3219\nsigma_M    0.962     0.021    0.921    1.005 1.000     6107     3212\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/note0322.html#bayesian-testing",
    "href": "posts/note0322.html#bayesian-testing",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Bayesian testing",
    "text": "Bayesian testing\n\n\nCode\nfit |&gt; \n  describe_posterior(\n    effects = \"all\",\n    component = \"all\",\n    #test = c(\"p_direction\", \"p_significance\"),\n    centrality = \"all\"\n  )\n\n\nWarning: Multivariate response models are not yet supported for tests `rope` and\n  `p_rope`.\n\n\nSummary of Posterior Distribution M\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        M |   0.49 |  0.49 |  0.49 | [ 0.41,  0.58] | 100% | 1.000 | 4369.00\nXTRUE       |        M |  -0.86 | -0.86 | -0.87 | [-0.98, -0.73] | 100% | 1.000 | 4838.00\n\n# Fixed effects sigma M\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        M |   0.96 | 0.96 | 0.96 | [ 0.92,  1.00] | 100% | 1.000 | 6057.00\n\n# Fixed effects Y\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        Y |   0.65 |  0.65 |  0.65 | [ 0.58,  0.71] | 100% | 1.001 | 3824.00\nXTRUE       |        Y |  -0.16 | -0.16 | -0.16 | [-0.24, -0.09] | 100% | 1.000 | 4552.00\nM           |        Y |  -0.37 | -0.37 | -0.37 | [-0.39, -0.35] | 100% | 1.000 | 3427.00\nW           |        Y |   0.60 |  0.60 |  0.60 | [ 0.57,  0.64] | 100% | 1.000 | 4040.00\nM:W         |        Y |   0.10 |  0.10 |  0.10 | [ 0.09,  0.11] | 100% | 0.999 | 3072.00\n\n# Fixed effects sigma Y\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        Y |   0.57 | 0.57 | 0.57 | [ 0.55,  0.60] | 100% | 1.000 | 5961.00\n\n\nThe function hypothesis() can be used to test specific parameter.\n\n\nCode\nfit_hypo &lt;- hypothesis(\n  fit, \n  class = 'b',\n  alpha = .05,\n  hypothesis = \n  c(\n    Low = \"Y_M - Y_M:W = 0\",\n    Medium = \"Y_M = 0\",\n    High = \"Y_M + Y_M:W = 0\")\n  ) \nfit_hypo\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1        Low    -0.47      0.01    -0.49    -0.44         NA        NA    *\n2     Medium    -0.37      0.01    -0.39    -0.35         NA        NA    *\n3       High    -0.27      0.01    -0.28    -0.25         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "posts/note0322.html#make-plots",
    "href": "posts/note0322.html#make-plots",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make plots",
    "text": "Make plots\n\n\nCode\n## plotting ----\ncond_plot &lt;- conditional_effects(fit)\n\n\n\n\nCode\ncond_plot$`Y.Y_M:W` |&gt;\n  ggplot(aes(x = M, y = Y), ) +\n  \n  geom_ribbon(aes(x = effect1__, y = estimate__, linetype = effect2__,\n                  ymin = lower__, ymax = upper__, fill = factor(effect2__)), alpha = 0.5) +\n  geom_line(aes(x = effect1__, y = estimate__, linetype = effect2__)) +\n  scale_fill_manual(name = 'W effects',\n                    values = c(\"coral4\", \"coral3\", \"coral2\"),\n                    labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\"),\n                    ) +\n  scale_linetype_manual(name = 'W effects',\n                        values = c(\"solid\", \"dotted\", \"dashed\"),\n                        labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\")) +\n  labs(x = \"the M\", \n       y = \"the Y\") +\n  ggtitle('M * W') +\n  annotate(\"text\", x=10, y=-8, label= \"Low \\n b=-0.47, [-0.49, -0.44]\") +\n  annotate(\"text\", x=25, y=-7, label= \"Average \\n b=-0.37, [-0.38, -0.35]\") +\n  annotate(\"text\", x=20, y=0, label= \"High \\n b=-0.27, [-0.28, -0.25]\") +\n  \n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n \n\n\nLorem Ipsum\n\n\n3 min\n\n\n\n\n\n\n\nMar 27, 2024\n\n\nUnderstand DIC\n\n\n6 min\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nHow to conduct simple slope analysis and make plot with brms\n\n\n4 min\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nNote0716 (mediation analysis with pymc)\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dic_study_2.html",
    "href": "posts/dic_study_2.html",
    "title": "Understand DIC",
    "section": "",
    "text": "This mini-study aims to understand how the DIC (deviance information criterion) index works.\nThe common idea of an information criterion is \\(D + 2pD\\). The \\(D\\) (deviance) can also be presented as \\(-2\\) log-likelihood. Besides, a version of pD (effective number of parameters) of DIC (as same as JAGS program) is defined as the variance of log-likelihood.\nCode\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading iris data set.\nfrom sklearn.datasets import load_iris\niris = load_iris()"
  },
  {
    "objectID": "posts/dic_study_2.html#goals.",
    "href": "posts/dic_study_2.html#goals.",
    "title": "Understand DIC",
    "section": "Goals.",
    "text": "Goals.\n這個研究就簡單拿 iris 資料集來測試。 我們知道 iris 有 4 個 features: 花萼長度 (Sepal.Length), 花萼寬度 (Sepal.Width), 花瓣長度 (Petal.Length), 花瓣寬度 (Petal.Width)。\n我們今天就簡單用 Sepal.Length ~ Sepal.Width 這個模式來看看 DIC 怎麼算 此外，為了增加一點參數，我們再使用 3 個 target，建立階層線性模式。\n因此，模式如下：\n\\[\n\\begin{align}\n\\text{Likelihood:} \\\\\n\\text{Length} &\\sim N(\\mu _w, \\sigma^2) \\\\\n\\mu _w &= \\beta _0 + \\beta _{1i} \\text{Width} \\\\\n\\\\\n\\text{Priors:} \\\\\n\\beta _0, \\beta _{1i} &\\sim N(0,5) \\\\\n\\sigma &\\sim \\text{Exp}(1)\n\\end{align}\n\\]\n這邊的 \\(\\beta _{1i}\\) 是每一個 level 對應的參數。所以應該會有 3 個。"
  },
  {
    "objectID": "posts/dic_study_2.html#jags",
    "href": "posts/dic_study_2.html#jags",
    "title": "Understand DIC",
    "section": "jags",
    "text": "jags\nLet’s see how to run this model in jags.\nFirstly, we call the iris data set (from R default {datasets})\ndata(iris)\nSecondly, we define the data list and model string in the {R2jags} package. The {R2jags} allows users to write a jags model just like an R function.\n\nThe data list.\n\ndat_list = list(\n  sepal_length = iris$Sepal.Length,\n  sepal_width = iris$Sepal.Width,\n  species = iris$Species,\n  n = 150\n)\n\nThe model string.\n\nmod_string &lt;- \\(){\n  ## priors\n  beta0 ~ dnorm(0,1/5^2)\n  sigma ~ dexp(1)\n  for (j in 1:3){\n    beta1[j] ~ dnorm(0,1/5^2)\n  }\n  \n  ## likelihood\n  for (i in 1:n){\n    mu_w[i] &lt;- beta0 + beta1[species[i]] * sepal_width[i]\n    sepal_length[i] ~ dnorm(mu_w[i], 1/sigma^2) \n  }\n}\nFinally, we run this model through the jags function.\nfit &lt;- jags(data = dat_list, \n     parameters.to.save = c('beta0','beta1','sigma'),\n     model.file = (mod_string)\n     )\nThen, the output of this jags model is shown as follows:\n&gt; print(fit, digits = 3)\nInference for Bugs model at \"/var/folders/1f/8r50hwmn6m5dwrngfgysq4p40000gn/T//Rtmpbvfmga/modelab6b34cc72fd.txt\", fit using jags,\n 3 chains, each with 2000 iterations (first 1000 discarded)\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nbeta0      3.338   0.333   2.680   3.117   3.336   3.559   3.981 1.001  2400\nbeta1[1]   0.488   0.098   0.298   0.424   0.490   0.552   0.685 1.002  1700\nbeta1[2]   0.938   0.120   0.700   0.856   0.938   1.019   1.175 1.001  2400\nbeta1[3]   1.091   0.113   0.870   1.015   1.091   1.168   1.310 1.002  1900\nsigma      0.444   0.026   0.397   0.426   0.444   0.461   0.496 1.002  1600\ndeviance 180.851   3.199 176.629 178.531 180.100 182.442 188.810 1.002  1400\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts/dic_study_2.html#pymc",
    "href": "posts/dic_study_2.html#pymc",
    "title": "Understand DIC",
    "section": "pymc",
    "text": "pymc\nNow, we use the sync to replicate these results. We are interested in two things,\n\nRQ1. to compare the parameters of beta0, beta1, and sigma.\nRQ2. to compute the (expected) deviance, pD, and DIC.\n\n\n\nCode\niris_data = pd.DataFrame(iris['data'])\niris_data.columns = iris['feature_names']\niris_data\n\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\n\nCode\n#seed=1234\n\ntarget_index, target = pd.Series(iris['target']).factorize()\n#width_index, width = iris_data[1].factorize()\n\n\ndict = {\n    'target': iris['target_names'], \n    'target_index': target_index,\n    #'width_index': width_index\n}\ndict\n\n\n{'target': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'target_index': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}\n\n\n\nRQ1. Compare jags and pymc\n\n\nCode\nwith pm.Model(coords=dict) as iris_model:   \n    ## data\n    sepal_length = pm.Data('sepal_length', iris_data['sepal length (cm)'])\n    sepal_width = pm.Data('sepal_width', iris_data['sepal width (cm)'])\n    \n\n    ## priors\n    beta0 = pm.Normal('β0', 0,5)\n    beta1 = pm.Normal('β1', 0,5, shape=3)\n    sigma = pm.Exponential('σ',1)\n\n    ## likelihood\n    mu_w = beta0 + beta1[target_index] * sepal_width\n    Length = pm.Normal('length', mu_w, sigma, observed=sepal_length)\n\n    ## sampling\n    iris_post = pm.sample( draws=3000, chains=4, cores=4) \n    pm.compute_log_likelihood(iris_post)\n    #ra_4pl_predict = pm.sample_posterior_predictive(ra_4pl_post)\n\n\n/Users/garden/Library/Python/3.9/lib/python/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [β0, β1, σ]\nSampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [16000/16000 00:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(iris_post)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nβ0\n3.344\n0.329\n2.739\n3.968\n0.007\n0.005\n2393.0\n2772.0\n1.0\n\n\nβ1[0]\n0.487\n0.097\n0.296\n0.657\n0.002\n0.001\n2425.0\n2847.0\n1.0\n\n\nβ1[1]\n0.935\n0.119\n0.713\n1.159\n0.002\n0.002\n2469.0\n2887.0\n1.0\n\n\nβ1[2]\n1.089\n0.111\n0.877\n1.294\n0.002\n0.002\n2421.0\n2802.0\n1.0\n\n\nσ\n0.444\n0.026\n0.395\n0.492\n0.000\n0.000\n4241.0\n3826.0\n1.0\n\n\n\n\n\n\n\n\nConcluding remarks. For the RQ1, the outputs from jags and pymc show no significant differences.\n\n\nRQ2. Computing DIC.\nFirstly, let’s see the data structure of log_likelihood from the pm.compute_log_likelihood() function. It’s a three-way dimensions tensor. The first dim is for (4) chains, the second for (3000) draws, and the third for length of data (150).\n\n\nCode\niris_post.log_likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 3000, length_dim_0: 150)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 ... 2994 2995 2996 2997 2998 2999\n  * length_dim_0  (length_dim_0) int64 0 1 2 3 4 5 6 ... 144 145 146 147 148 149\nData variables:\n    length        (chain, draw, length_dim_0) float64 -0.1501 -0.2302 ... -1.553\nAttributes:\n    created_at:                 2024-03-27T11:15:05.525056\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.0xarray.DatasetDimensions:chain: 4draw: 3000length_dim_0: 150Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])length_dim_0(length_dim_0)int640 1 2 3 4 5 ... 145 146 147 148 149array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149])Data variables: (1)length(chain, draw, length_dim_0)float64-0.1501 -0.2302 ... -1.878 -1.553array([[[-0.15007587, -0.23018022, -0.17186102, ..., -0.15036302,\n         -2.03497421, -1.25868256],\n        [-0.14763492, -0.18836443, -0.24012757, ..., -0.15958522,\n         -1.93430897, -1.20251279],\n        [-0.13546664, -0.24928963, -0.11306264, ..., -0.12683694,\n         -2.32605535, -1.40839025],\n        ...,\n        [-0.14045847, -0.13434047, -0.13613556, ..., -0.14624427,\n         -2.03811634, -1.58747839],\n        [-0.15619394, -0.1480345 , -0.15840489, ..., -0.16924811,\n         -1.99564914, -1.57251438],\n        [-0.06604883, -0.06365481, -0.3631115 , ..., -0.06431746,\n         -1.54910513, -1.23877687]],\n\n       [[-0.2091668 , -0.22398206, -0.26267452, ..., -0.21925268,\n         -1.68602165, -1.25524037],\n        [-0.0207    , -0.00833144, -0.18972243, ..., -0.03910661,\n         -1.76095645, -1.53025309],\n        [-0.07599954, -0.0308677 , -0.08252084, ..., -0.02220397,\n         -1.67739141, -1.4524553 ],\n...\n        [-0.21175487, -0.3262845 , -0.14786837, ..., -0.20980681,\n         -2.44356451, -1.53501204],\n        [-0.10045583, -0.14453996, -0.20253765, ..., -0.12437101,\n         -2.18738956, -1.34751415],\n        [-0.06133903, -0.05676096, -0.27590568, ..., -0.06310933,\n         -1.92064681, -1.27690886]],\n\n       [[-0.12362904, -0.17491691, -0.13462518, ..., -0.09471016,\n         -1.73004549, -1.14761551],\n        [-0.04212361, -0.05487806, -0.19423224, ..., -0.10022129,\n         -2.30313771, -1.63956094],\n        [-0.04604174, -0.07543161, -0.13592791, ..., -0.06340857,\n         -2.12692774, -1.46189584],\n        ...,\n        [-0.10221615, -0.10216825, -0.2916557 , ..., -0.12732807,\n         -1.68477784, -1.35505866],\n        [-0.08520427, -0.07523728, -0.19593031, ..., -0.12138973,\n         -1.90040781, -1.57051397],\n        [-0.07718419, -0.07102337, -0.22110662, ..., -0.11846029,\n         -1.87751136, -1.5528609 ]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))length_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n      dtype='int64', name='length_dim_0', length=150))Attributes: (4)created_at :2024-03-27T11:15:05.525056arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.0\n\n\nSecondly, let’s try to compute the expected deviance (D) from this tensor. Due to the output from jags, we know the correct answer will be close to 180.851.\nNow, we need to compute the D (-2ll) for each point (there are a total of 150 points in this study\nTips. To sum up the dim we are interested in. In this case, we sum up the dim of length_dim_0 (axis=2). Then we can get 4*3000 draws for each points.\n\n\nCode\ny_ll = iris_post.log_likelihood['length'].sum(axis=2)\ny_deviance = -2*y_ll\ny_deviance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' (chain: 4, draw: 3000)&gt;\narray([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 2993 2994 2995 2996 2997 2998 2999xarray.DataArray'length'chain: 4draw: 3000180.1 180.2 181.5 181.0 181.9 183.1 ... 178.5 178.5 178.7 179.0 178.3array([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))Attributes: (0)\n\n\nThen get the posterior mean of it. It is 180.85.\n\n\nCode\ny_deviance.mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(180.83823109)xarray.DataArray'length'180.8array(180.83823109)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nThirdly, we need to compute the pD. We konw the pD will be close to 5.1.\n\n\nCode\ny_deviance.var()/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(5.37651436)xarray.DataArray'length'5.377array(5.37651436)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nFinally, we can compute the DIC. It will be close to 186.0. There are two kind of mthods to compute it,\n\nUsing log-likelihood. -2*y_ll.mean() + 2*y_ll.var()\nUsing deviance. y_deviance.mean() + y_deviance.var()/2\n\n\n\nCode\nDIC = y_deviance.mean() + y_deviance.var()/2\nDIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(186.21474545)xarray.DataArray'length'186.2array(186.21474545)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nYes!! Bingo!!"
  },
  {
    "objectID": "posts/dic_study_2.html#the-easy-function.",
    "href": "posts/dic_study_2.html#the-easy-function.",
    "title": "Understand DIC",
    "section": "The easy function.",
    "text": "The easy function.\nFurthermore, we write a function to output the strings like the jags program.\nIt will look like,\nDIC info (using the rule, pD = var(deviance)/2)\ndeviance = 180.85, pD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCode\ndef get_dic(posterior_tensor, var_names):\n    y_ll = posterior_tensor.log_likelihood[var_names].sum(axis=2).to_numpy()\n    y_deviance = -2*y_ll.mean()\n    y_pd = 2*y_ll.var()\n    y_dic = y_deviance + y_pd\n\n    y_print =   'DIC info (using the rule, pD = var(deviance)/2) \\n' +\\\n                'mean deviance = {:.3f}, pD = {:.3f} and DIC = {:.3f} \\n'.format(y_deviance, y_pd, y_dic) +\\\n                'DIC is an estimate of expected predictive error (lower deviance is better).'\n            \n    return print(y_print)\n\n\n\n\nCode\nget_dic(iris_post, var_names='length')\n\n\nDIC info (using the rule, pD = var(deviance)/2) \nmean deviance = 180.838, pD = 5.377 and DIC = 186.215 \nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi,\nI’m Jie-Wen Tsai.",
    "section": "",
    "text": "Education, National Chengchi University, Taiwan."
  },
  {
    "objectID": "index.html#education.",
    "href": "index.html#education.",
    "title": "Hi,\nI’m Jie-Wen Tsai.",
    "section": "",
    "text": "Education, National Chengchi University, Taiwan."
  },
  {
    "objectID": "index.html#interesting.",
    "href": "index.html#interesting.",
    "title": "Hi,\nI’m Jie-Wen Tsai.",
    "section": "Interesting.",
    "text": "Interesting.\n\nPsychometrics. CRAN Task View: Psychometric Models and Methods.\nBayesian data analysis. CRAN Task View: Bayesian Inference."
  }
]