[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Softwares",
    "section": "",
    "text": "ExtendedRtIrtModeling.jl.  Extended Response Time Item Response Models with Polya-Gamma Sampler and Bayesian Quantile Regression. See more: Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR"
  },
  {
    "objectID": "software.html#julia",
    "href": "software.html#julia",
    "title": "Softwares",
    "section": "",
    "text": "ExtendedRtIrtModeling.jl.  Extended Response Time Item Response Models with Polya-Gamma Sampler and Bayesian Quantile Regression. See more: Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR"
  },
  {
    "objectID": "software.html#quarto",
    "href": "software.html#quarto",
    "title": "Softwares",
    "section": "Quarto",
    "text": "Quarto\n\nquarto-revealjs-yangchou"
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Softwares",
    "section": "R",
    "text": "R\n\nbibliometrixExtra.  R for synonyms (Extended functions for bibliometrix)."
  },
  {
    "objectID": "software.html#typst",
    "href": "software.html#typst",
    "title": "Softwares",
    "section": "Typst",
    "text": "Typst\n\n(…)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jie-Wen Tsai.",
    "section": "",
    "text": "A ninja of educational measurement."
  },
  {
    "objectID": "index.html#see-also.",
    "href": "index.html#see-also.",
    "title": "Jie-Wen Tsai.",
    "section": "See Also.",
    "text": "See Also.\n\nNinja Notes."
  },
  {
    "objectID": "index.html#interesting.",
    "href": "index.html#interesting.",
    "title": "Jie-Wen Tsai.",
    "section": "Interesting.",
    "text": "Interesting.\n\nPsychometrics. CRAN Task View: Psychometric Models and Methods.\nBayesian data analysis. CRAN Task View: Bayesian Inference."
  },
  {
    "objectID": "index.html#education.",
    "href": "index.html#education.",
    "title": "Jie-Wen Tsai.",
    "section": "Education.",
    "text": "Education.\n\nEducation, National Chengchi University, Taiwan."
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html",
    "href": "posts_zh/journal_jedubehstats.html",
    "title": "「教育與行為統計學」期刊最有影響力的 20 篇文章（JEducBehavStat 2010-2023）",
    "section": "",
    "text": "Journal Of Educational And Behavioral Statistics 教育與行為統計學期刊 是 American Educational Research Association, AERA 美國教育研究協會的主要期刊。由 Sage 出版。一年 6 期。"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#主要資訊",
    "href": "posts_zh/journal_jedubehstats.html#主要資訊",
    "title": "「教育與行為統計學」期刊最有影響力的 20 篇文章（JEducBehavStat 2010-2023）",
    "section": "主要資訊",
    "text": "主要資訊\n\n我們來看一下「教育與行為統計學」期刊的一些有趣數據。在 2010-2023 年，這期刊共有 467 篇文章，其中合著作者平均 2.33 位。每年成長率 -9.1%，文章平均年齡 7.16 歲，每篇文章平均被引用 17.38 次。這些數字背後反映了這個領域的研究活力和影響力。"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#研究話題地圖",
    "href": "posts_zh/journal_jedubehstats.html#研究話題地圖",
    "title": "「教育與行為統計學」期刊最有影響力的 20 篇文章（JEducBehavStat 2010-2023）",
    "section": "研究話題地圖",
    "text": "研究話題地圖\n\n「教育與行為統計學」期刊從 2010 年以來，最關鍵的 20+ 篇文章，大致呈現出 6 條主要的研究話題。以及許多單獨的重要文章。我們也提供文獻縮寫（和上圖對應），doi，以及中文翻譯名稱。文獻排序和圖上一樣，由上至下。有興趣的同學可以用 doi 去找到對應的文章。\n\n話題１ 缺失資料和潛在變項（紅色，比重 23.5%）\n\nCAI L, 2010, DOI 10.3102/1076998609353115 用於確認性項目因素分析的 Metropolis-hastings Robbins-monro 算法 🌟 最出圈文章！(GCS: 156)\nVON DAVIER M, 2010, DOI 10.3102/1076998609346970 潛在回歸項目反應模型的隨機逼近方法\nSI YJ, 2013, DOI 10.3102/1076998613480394 大規模評估調查中不完整分類變量的非參數貝葉斯多重估算\nDRECHSLER J, 2015, DOI 10.3102/1076998614563393 多層次缺失數據的多重估算–嚴謹性與簡便性的比較\n\n\n\n話題２ 試題反應理論（藍色，比重 11.7%）\n\nJOHNSON TR, 2010, DOI 10.3102/1076998609340529 使用因子分析多二項對數項目反應模型來考慮反應風格的個體差異\nTHISSEN-ROE A, 2013, DOI 10.3102/1076998613481500 對喜歡類型項目的反應的雙決定模型\n\n\n\n話題３ 反應時間模式（綠色，比重 11.7%）\n\nFAN ZW, 2012, DOI 10.3102/1076998611422912 利用反應時間分布進行貓的項目選擇\nWANG C, 2013, DOI 10.3102/1076998612461831 在計算機化測試中聯合分析反應時間和準確性的半參數模型\n\n\n\n話題４ 測驗分數應用（紫色，比重 29.4%）\n\nHO AD, 2012, DOI 10.3102/1076998611411918 從 “能力”等級報告的測驗分數中估計成績差距\nLOCKWOOD JR, 2014, DOI 10.3102/1076998613509405 在用於估計治療效果的 ancova 模型中糾正測驗分數測量誤差\nLECKIE G, 2014, DOI 10.3102/1076998614546494 為兩級模型中的異質方差-協方差成分建模\nREARDON SF, 2017, DOI 10.3102/1076998616666279 使用異方差有序 probit 模型從粗略數據中恢覆連續測驗分數分布的矩\nLOCKWOOD JR, 2018, DOI 10.3102/1076998618795124 利用靈活的貝葉斯模型從粗略的群體水平成績數據中進行推斷\n\n\n\n話題５ 貝氏相關技術（橘色，比重 11.7%）\n\nCULPEPPER SA, 2015, DOI 10.3102/1076998615595403 利用 Gibbs 抽樣對 Dina 模型進行貝葉斯估計 🌟 圈內人最愛！(LCS: 11)\nWANG SY, 2018, DOI 10.3102/1076998617719727 利用認知診斷模型跟蹤技能習得： 帶有協變量的高階隱馬爾可夫模型\n\n\n\n話題６ 一些檢驗方法（紅色，比重 11.7%）\n\nROMERO M, 2015, DOI 10.3102/1076998615595628 答案覆制指數的最優性： 理論與實踐\nSINHARAY S, 2017, DOI 10.3102/1076998616673872 利用似然比檢驗和分數檢驗檢測項目預知能力"
  },
  {
    "objectID": "posts_zh/bayes.html",
    "href": "posts_zh/bayes.html",
    "title": "貝氏學習資源推薦",
    "section": "",
    "text": "CitationBibTeX citation:@online{tsai,\n  author = {TSAI, JW},\n  title = {貝氏學習資源推薦},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTSAI, J. (n.d.). 貝氏學習資源推薦."
  },
  {
    "objectID": "posts/note0716.html",
    "href": "posts/note0716.html",
    "title": "Note0716 (mediation analysis with pymc)",
    "section": "",
    "text": "Code\nimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n#dat = pd.read_csv('data1463_fin3.csv')\n\n\n為了之後還可以跑這個模型，根據原本的資料重新模擬一筆資料。\n(但這邊沒考慮到原本數值之間的相關情形) 已考慮進去！！用多元常態分配模擬了。\n\nacd1eap \\(\\sim N(2.8510193121856177e-05, 0.8921482479092293)\\)\nscleap \\(\\sim N(1.9250494837216173e-06, 0.7154395945457549)\\)\nc1 \\(\\sim Ber(1,0.19822282980177716)\\)\nc2 \\(\\sim Ber(1,0.11483253588516747)\\)\nc3 \\(\\sim Ber(1,0.11551606288448393)\\)\n\n\n\nCode\n'''\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\n'''\n\n\n'\\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\\n'\n\n\n\n\nCode\ndat_mn = np.array(\n    [2.8510193121856177e-05,\n     1.9250494837216173e-06,\n     0.19822282980177716,\n     0.11483253588516747,\n     0.11551606288448393]\n)\ndat_cov = np.array([\n    [ 0.7959285 ,  0.16304568, -0.01541732, -0.01689872, -0.02973076],\n    [ 0.16304568,  0.51185381, -0.05390401, -0.00531492, -0.02327021],\n    [-0.01541732, -0.05390401,  0.15903925, -0.022778  , -0.02291358],\n    [-0.01689872, -0.00531492, -0.022778  ,  0.10171555, -0.01327408],\n    [-0.02973076, -0.02327021, -0.02291358, -0.01327408,  0.10224199]\n])\ndat = np.random.multivariate_normal(dat_mn, dat_cov, 1000)\ndat = pd.DataFrame(dat, columns=['acd1_eap', 'scl_eap', 'c1', 'c2', 'c3'])\ndat['c1'] = dat['c1'] &gt; 0.5\ndat['c2'] = dat['c2'] &gt; 0.5\ndat['c3'] = dat['c3'] &gt; 0.5\n\n\n\n\nCode\ndat\n\n\n\n\n\n\n\n\n\n\nacd1_eap\nscl_eap\nc1\nc2\nc3\n\n\n\n\n0\n0.346316\n-0.792151\nTrue\nFalse\nFalse\n\n\n1\n-0.305374\n1.621382\nFalse\nTrue\nFalse\n\n\n2\n0.563012\n0.828286\nFalse\nFalse\nFalse\n\n\n3\n0.058392\n-0.276411\nFalse\nFalse\nFalse\n\n\n4\n0.181640\n-1.291429\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.717853\n-0.833434\nTrue\nFalse\nFalse\n\n\n996\n-0.812274\n-0.038150\nFalse\nFalse\nFalse\n\n\n997\n-0.461906\n-0.635552\nFalse\nFalse\nFalse\n\n\n998\n0.294100\n0.594206\nFalse\nFalse\nFalse\n\n\n999\n-0.543131\n-0.928300\nTrue\nFalse\nFalse\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\n\nCode\n'''\ndat_dict = {\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \n}\ndat = pd.DataFrame(dat_dict)\n'''\n\n\n\"\\ndat_dict = {\\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \\n}\\ndat = pd.DataFrame(dat_dict)\\n\"\n\n\n\n\nCode\nwith pm.Model() as model:\n    acd1eap = pm.ConstantData('acd1eap', dat.acd1_eap)\n    scleap = pm.ConstantData('scleap', dat.scl_eap)\n    c1 = pm.ConstantData('c1', dat.c1)\n    c2 = pm.ConstantData('c2', dat.c2)\n    c3 = pm.ConstantData('c3', dat.c3)\n\n    # intercept\n    acd1eap_Intercept = pm.Normal('acd1eap_Intercept', mu=0, sigma=100)\n    scleap_Intercept = pm.Normal('scleap_Intercept', mu=0, sigma=100)\n    \n    # noise\n    acd1eap_Sigma = pm.HalfCauchy(\"acd1eap_Sigma\", 1)\n    scleap_Sigma = pm.HalfCauchy(\"scleap_Sigma\", 1)\n\n    # slope\n    acd1eap_scleap = pm.Normal('acd1eap_scleap', mu=0, sigma=100)\n    acd1eap_c1 = pm.Normal('acd1eap_c1', mu=0, sigma=100)\n    acd1eap_c2 = pm.Normal('acd1eap_c2', mu=0, sigma=100)\n    acd1eap_c3 = pm.Normal('acd1eap_c3', mu=0, sigma=100)\n    scleap_c1 = pm.Normal('scleap_c1', mu=0, sigma=100)\n    scleap_c2 = pm.Normal('scleap_c2', mu=0, sigma=100)\n    scleap_c3 = pm.Normal('scleap_c3', mu=0, sigma=100)\n\n    # likelihood\n    pm.Normal(\"y_likelihood\", mu=acd1eap_Intercept + acd1eap_scleap * scleap  + acd1eap_c1 * c1 + acd1eap_c2 * c2 + acd1eap_c3 * c3, sigma =  acd1eap_Sigma, observed = acd1eap  )\n    pm.Normal('m_likelihood', mu=scleap_Intercept + scleap_c1 * c1 + scleap_c2 * c2 + scleap_c3 * c3, sigma = scleap_Sigma, observed = scleap)\n    \n    trace_med = pm.sample(2000, chains=4, cores=4)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [acd1eap_Intercept, scleap_Intercept, acd1eap_Sigma, scleap_Sigma, acd1eap_scleap, acd1eap_c1, acd1eap_c2, acd1eap_c3, scleap_c1, scleap_c2, scleap_c3]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\nCode\npm.model_to_graphviz(model)\n\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n&lt;graphviz.graphs.Digraph at 0x17209fb80&gt;\n\n\n\n\nCode\naz.summary(trace_med)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nacd1eap_Intercept\n0.042\n0.036\n-0.025\n0.109\n0.000\n0.000\n9687.0\n6732.0\n1.0\n\n\nscleap_Intercept\n0.045\n0.030\n-0.013\n0.102\n0.000\n0.000\n10276.0\n6795.0\n1.0\n\n\nacd1eap_scleap\n0.374\n0.038\n0.305\n0.445\n0.000\n0.000\n13151.0\n6209.0\n1.0\n\n\nacd1eap_c1\n0.002\n0.067\n-0.126\n0.122\n0.001\n0.001\n11284.0\n6502.0\n1.0\n\n\nacd1eap_c2\n-0.008\n0.085\n-0.169\n0.148\n0.001\n0.001\n12240.0\n6047.0\n1.0\n\n\nacd1eap_c3\n-0.049\n0.088\n-0.210\n0.120\n0.001\n0.001\n12138.0\n5986.0\n1.0\n\n\nscleap_c1\n-0.181\n0.055\n-0.285\n-0.081\n0.001\n0.000\n11615.0\n6883.0\n1.0\n\n\nscleap_c2\n0.031\n0.071\n-0.091\n0.173\n0.001\n0.001\n11680.0\n7188.0\n1.0\n\n\nscleap_c3\n-0.076\n0.073\n-0.225\n0.053\n0.001\n0.001\n12466.0\n6045.0\n1.0\n\n\nacd1eap_Sigma\n0.881\n0.020\n0.845\n0.919\n0.000\n0.000\n14248.0\n6463.0\n1.0\n\n\nscleap_Sigma\n0.734\n0.017\n0.703\n0.765\n0.000\n0.000\n12699.0\n6039.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{tsai2023,\n  author = {Tsai, JW},\n  title = {Note0716 (Mediation Analysis with Pymc)},\n  date = {2023-07-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTsai, J. (2023, July 16). Note0716 (mediation analysis with\npymc)."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html",
    "href": "posts/IntroJuliaConnectoR.html",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "",
    "text": "I’ve just updated my Julia package, ExtendedRtIrtModeling.jl, to version 0.2.0. There are a few new features in there that I’ll run through in the next few sections.\nBut that’s not all! If you’re an R user, I’ll introduce you to an R package called JuliaConnectoR that lets you run Julia programs in R. It’ll bridge the two languages seamlessly."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#using-in-julia",
    "href": "posts/IntroJuliaConnectoR.html#using-in-julia",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "Using in Julia",
    "text": "Using in Julia\nSee the Github page."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#using-in-r",
    "href": "posts/IntroJuliaConnectoR.html#using-in-r",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "Using in R",
    "text": "Using in R\nAll you have to do is to install and library the JuliaConnectoR as usual, and then you can use the juliaImport function to import any Julia package. It seems like the package’s version you get depends on which copy version you’ve had on your computer (confirmed). The great thing is, it’ll always download the newest version from Github, but not the stable one.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(JuliaConnectoR)\n\n\nWarning: package 'JuliaConnectoR' was built under R version 4.4.1\n\n\nI’ve got a toy data set to test, but I’ll run through the demo anyway. As you can see from the data below, the data set includes 25 columns: one for ID, 10 for item responses, 10 for (log-)response time, and four for explanatory variables.\n\n\nCode\ndemo &lt;- read.csv('https://raw.githubusercontent.com/jiewenTsai/ExtendedRtIrtModeling.jl/refs/heads/main/data/demo.csv')\nhead(demo)\n\n\n    id y1 y2 y3 y4 y5 y6 y7 y8 y9 y10    t1    t2    t3    t4    t5    t6    t7\n1 s001  0  0  0  0  1  0  0  0  0   0 2.961 4.225 3.322 2.164 2.273 2.631 2.505\n2 s002  0  0  0  0  1  1  1  0  0   0 3.848 3.996 4.434 3.246 2.663 3.819 2.158\n3 s003  0  0  0  0  0  1  1  1  0   0 3.122 3.273 4.489 3.891 3.410 3.879 2.951\n4 s004  0  1  0  0  0  0  1  0  0   0 3.515 3.162 4.151 3.371 2.885 3.026 1.439\n5 s005  0  0  0  0  0  0  1  0  1   0 3.060 3.962 4.058 3.696 2.732 2.560 2.517\n6 s006  0  0  0  0  1  0  0  0  0   0 3.546 3.360 3.382 3.262 1.931 1.629 1.463\n     t8    t9   t10 x1     x2     x3    x4\n1 3.924 2.198 2.878  0 -3.362 -0.200 1.063\n2 2.848 2.492 4.038  1 -0.081  2.347 1.063\n3 3.891 3.877 3.553  0 -0.829 -1.068 1.063\n4 3.243 2.694 3.603  1 -0.829 -0.676 1.063\n5 3.339 3.421 3.314  0 -0.829  0.097 1.063\n6 2.475 2.373 2.909  1 -1.251 -3.096 1.063\n\n\nCode\nglimpse(demo)\n\n\nRows: 300\nColumns: 25\n$ id  &lt;chr&gt; \"s001\", \"s002\", \"s003\", \"s004\", \"s005\", \"s006\", \"s007\", \"s008\", \"s…\n$ y1  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ y2  &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ y3  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ y4  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ y5  &lt;int&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, …\n$ y6  &lt;int&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, …\n$ y7  &lt;int&gt; 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, …\n$ y8  &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ y9  &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, …\n$ y10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ t1  &lt;dbl&gt; 2.961, 3.848, 3.122, 3.515, 3.060, 3.546, 2.579, 3.854, 4.516, 2.9…\n$ t2  &lt;dbl&gt; 4.225, 3.996, 3.273, 3.162, 3.962, 3.360, 3.448, 3.451, 4.943, 4.1…\n$ t3  &lt;dbl&gt; 3.322, 4.434, 4.489, 4.151, 4.058, 3.382, 3.753, 2.875, 2.945, 5.0…\n$ t4  &lt;dbl&gt; 2.164, 3.246, 3.891, 3.371, 3.696, 3.262, 3.575, 3.634, 4.444, 4.0…\n$ t5  &lt;dbl&gt; 2.273, 2.663, 3.410, 2.885, 2.732, 1.931, 2.846, 2.560, 2.718, 2.8…\n$ t6  &lt;dbl&gt; 2.631, 3.819, 3.879, 3.026, 2.560, 1.629, 3.153, 2.499, 1.442, 3.3…\n$ t7  &lt;dbl&gt; 2.505, 2.158, 2.951, 1.439, 2.517, 1.463, 3.370, 2.581, 1.666, 3.2…\n$ t8  &lt;dbl&gt; 3.924, 2.848, 3.891, 3.243, 3.339, 2.475, 2.424, 2.124, 3.301, 3.3…\n$ t9  &lt;dbl&gt; 2.198, 2.492, 3.877, 2.694, 3.421, 2.373, 2.039, 2.749, 1.962, 2.7…\n$ t10 &lt;dbl&gt; 2.878, 4.038, 3.553, 3.603, 3.314, 2.909, 2.923, 2.366, 4.804, 3.6…\n$ x1  &lt;int&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, …\n$ x2  &lt;dbl&gt; -3.362, -0.081, -0.829, -0.829, -0.829, -1.251, 1.174, -0.829, -1.…\n$ x3  &lt;dbl&gt; -0.200, 2.347, -1.068, -0.676, 0.097, -3.096, -1.944, -1.258, -1.4…\n$ x4  &lt;dbl&gt; 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 0.104, -0.587, -2.002, -…\n\n\nNext, let’s take a look at how accuracy and speed related to each other, barely using the raw data (mean of y and mean of t) to get a rough idea.\n\n\nCode\ntibble(\n  accuracy = rowMeans(demo[2:11]),\n  speed = -rowMeans(demo[12:21])\n) |&gt;\n  ggplot(aes(x=accuracy, y=speed)) +\n  geom_point() +\n  geom_jitter() +\n  geom_smooth(method = \"loess\") \n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nIntroduction to ExtendedRtIrtModeling.jl through JuliaConnectoR\n\n\n2 min\n\n\n\nNov 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstand DIC\n\n\n6 min\n\n\n\nMar 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to conduct simple slope analysis and make plot with brms\n\n\n4 min\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNote0716 (mediation analysis with pymc)\n\n\n3 min\n\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notesZh.html",
    "href": "notesZh.html",
    "title": "中文記事",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n貝氏學習資源推薦\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nChapter 10\n\n\n2 min\n\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n「多變量行為研究」期刊最有影響力的 20 篇文章（MultivariateBehavRes 2010-2023）\n\n\n1 min\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n「教育與行為統計學」期刊最有影響力的 20 篇文章（JEducBehavStat 2010-2023）\n\n\n1 min\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/note0322.html",
    "href": "posts/note0322.html",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "",
    "text": "Goal. In this note, we will demonstrate how to use the output from brms to make (simple slope) testings and plots."
  },
  {
    "objectID": "posts/note0322.html#make-data",
    "href": "posts/note0322.html#make-data",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make data",
    "text": "Make data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayestestR)\nlibrary(rstan)\nlibrary(mvtnorm)\n\n\nNow we have to make a data set including 4 variables: Y, X, M, and W.\nSuppose these four variables follow a multivariate-normal distribution as follows,\nLet X is a treatment (binary data), and M is a response time data (lognormal).\n\\[\\begin{equation}\n\\begin{bmatrix}\nY \\\\ X \\\\M \\\\W\n\\end{bmatrix}\n= \\text{MVN}\\left(\n\\begin{bmatrix}\n0 \\\\0 \\\\ 0\\\\ 0\n\\end{bmatrix},\n\\begin{bmatrix}\n1 & 0.1 & -0.8 & 0.8  \\\\\n0.1 & 1 & -0.6 & 0\\\\\n-0.8 & -0.6 & 1 & 0.6\\\\\n0.8 & 0 & 0.6 & 1\n\\end{bmatrix}\n\\right)\n\\end{equation}\\]\n\n\nCode\nset.seed(12345)\nreal_sigma &lt;- matrix(c(1, 0.1, -0.8, 0.8,\n                      0.1, 1, -0.6, 0,\n                      -0.8, -0.6, 1, 0.6,\n                      0.8, 0, 0.6, 1), nrow = 4)\nreal_mean &lt;- c(0,0,0,0)\n\nreal_data &lt;- rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma)\n\n\nWarning in rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma): sigma is\nnumerically not positive semidefinite\n\n\nCode\ndat &lt;- data.frame(\n  ID = paste0('s', str_pad(1:1000, width = 4, side = 'left', pad = 0)),\n  Y = real_data[,1],\n  X = real_data[,2] &gt; mean(real_data[,2]),\n  M = exp(real_data[,3]),\n  W = real_data[,4]\n)\n\nhead(dat)\n\n\n     ID          Y     X         M          W\n1 s0001  0.3617174  TRUE 0.4790760 -0.1639895\n2 s0002  0.1110080 FALSE 2.0490296  0.2046531\n3 s0003  0.6187169 FALSE 2.6247616  1.4401394\n4 s0004  1.0347476  TRUE 0.5084054  0.6434533\n5 s0005 -1.1477318 FALSE 4.8633851  0.2686688\n6 s0006  0.2802449  TRUE 0.1476185 -1.2412565"
  },
  {
    "objectID": "posts/note0322.html#fit-bayesian-model-in-brms",
    "href": "posts/note0322.html#fit-bayesian-model-in-brms",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Fit Bayesian model in brms",
    "text": "Fit Bayesian model in brms\nNow we specify the formula as follows (in Bayesian).\n\\[\\begin{align}\n\\text{Likelihood.}\\\\\nY &\\sim N(\\mu_y, \\sigma_y^2) \\\\\nM &\\sim \\log N(\\mu_m, \\sigma_m^2) \\\\\n\n\\mu_y &= \\beta_{01} + \\beta_x X + \\beta_m M + \\beta_w W + \\beta _{mw}M \\cdot W \\\\\n\\mu_m &= \\beta_{02} + \\beta_x X \\\\ \\\\\n\n\\text{Priors.}\\\\\n\n\\sigma_y^2, \\sigma_m^2 & \\sim \\text{Exp}(1) \\\\\n\\beta_{01}, ..., \\beta _{x} &\\sim N(0,5)\n\\end{align}\\]\n\n\nCode\nbf1 &lt;- bf(Y~X+M+W+M*W, family = gaussian())\nbf2 &lt;- bf(M~X, family = lognormal())\npriors &lt;- prior(normal(0,5), class = b, resp = Y) + \n  prior(normal(0,5), class = b, resp = M) + \n  prior(exponential(1), class = sigma, resp = Y) +\n  prior(exponential(1), class = sigma, resp = M) \n\n\n\nfit &lt;- brm(\n  bf1+bf2+set_rescor(FALSE), \n  data = dat,\n  cores = 4\n)\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nCode\nprint(fit, digits = 3)\n\n\n Family: MV(gaussian, lognormal) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: Y ~ X + M + W + M * W \n         M ~ X \n   Data: dat (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nY_Intercept    0.648     0.035    0.580    0.716 1.001     4338     3242\nM_Intercept    0.490     0.043    0.407    0.574 1.002     4917     3090\nY_XTRUE       -0.164     0.039   -0.241   -0.086 1.000     4874     3349\nY_M           -0.366     0.010   -0.386   -0.346 1.001     3476     3265\nY_W            0.604     0.020    0.565    0.641 1.000     4427     3384\nY_M:W          0.100     0.006    0.088    0.112 1.000     2928     2864\nM_XTRUE       -0.856     0.059   -0.970   -0.742 1.001     5208     2953\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma_Y    0.570     0.013    0.544    0.597 1.000     5277     2537\nsigma_M    0.962     0.021    0.921    1.005 1.002     5424     2974\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/note0322.html#bayesian-testing",
    "href": "posts/note0322.html#bayesian-testing",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Bayesian testing",
    "text": "Bayesian testing\n\n\nCode\nfit |&gt; \n  describe_posterior(\n    effects = \"all\",\n    component = \"all\",\n    #test = c(\"p_direction\", \"p_significance\"),\n    centrality = \"all\"\n  )\n\n\nWarning: Multivariate response models are not yet supported for tests `rope` and\n  `p_rope`.\n\n\nSummary of Posterior Distribution M\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        M |   0.49 |  0.49 |  0.49 | [ 0.41,  0.57] | 100% | 1.000 | 4898.00\nXTRUE       |        M |  -0.85 | -0.86 | -0.85 | [-0.97, -0.74] | 100% | 0.999 | 5258.00\n\n# Fixed effects sigma M\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        M |   0.96 | 0.96 | 0.96 | [ 0.92,  1.00] | 100% | 1.000 | 5345.00\n\n# Fixed effects Y\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        Y |   0.65 |  0.65 |  0.65 | [ 0.58,  0.72] | 100% | 1.000 | 4325.00\nXTRUE       |        Y |  -0.16 | -0.16 | -0.16 | [-0.24, -0.09] | 100% | 1.000 | 4867.00\nM           |        Y |  -0.37 | -0.37 | -0.36 | [-0.39, -0.35] | 100% | 1.001 | 3499.00\nW           |        Y |   0.60 |  0.60 |  0.60 | [ 0.56,  0.64] | 100% | 1.000 | 4384.00\nM:W         |        Y |   0.10 |  0.10 |  0.10 | [ 0.09,  0.11] | 100% | 1.000 | 2908.00\n\n# Fixed effects sigma Y\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        Y |   0.57 | 0.57 | 0.57 | [ 0.54,  0.60] | 100% | 1.000 | 5226.00\n\n\nThe function hypothesis() can be used to test specific parameter.\n\n\nCode\nfit_hypo &lt;- hypothesis(\n  fit, \n  class = 'b',\n  alpha = .05,\n  hypothesis = \n  c(\n    Low = \"Y_M - Y_M:W = 0\",\n    Medium = \"Y_M = 0\",\n    High = \"Y_M + Y_M:W = 0\")\n  ) \nfit_hypo\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1        Low    -0.47      0.02    -0.50    -0.44         NA        NA    *\n2     Medium    -0.37      0.01    -0.39    -0.35         NA        NA    *\n3       High    -0.27      0.01    -0.28    -0.25         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "posts/note0322.html#make-plots",
    "href": "posts/note0322.html#make-plots",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make plots",
    "text": "Make plots\n\n\nCode\n## plotting ----\ncond_plot &lt;- conditional_effects(fit)\n\n\n\n\nCode\ncond_plot$`Y.Y_M:W` |&gt;\n  ggplot(aes(x = M, y = Y), ) +\n  \n  geom_ribbon(aes(x = effect1__, y = estimate__, linetype = effect2__,\n                  ymin = lower__, ymax = upper__, fill = factor(effect2__)), alpha = 0.5) +\n  geom_line(aes(x = effect1__, y = estimate__, linetype = effect2__)) +\n  scale_fill_manual(name = 'W effects',\n                    values = c(\"coral4\", \"coral3\", \"coral2\"),\n                    labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\"),\n                    ) +\n  scale_linetype_manual(name = 'W effects',\n                        values = c(\"solid\", \"dotted\", \"dashed\"),\n                        labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\")) +\n  labs(x = \"the M\", \n       y = \"the Y\") +\n  ggtitle('M * W') +\n  annotate(\"text\", x=10, y=-8, label= \"Low \\n b=-0.47, [-0.49, -0.44]\") +\n  annotate(\"text\", x=25, y=-7, label= \"Average \\n b=-0.37, [-0.38, -0.35]\") +\n  annotate(\"text\", x=20, y=0, label= \"High \\n b=-0.27, [-0.28, -0.25]\") +\n  \n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "posts/dic_study_2.html",
    "href": "posts/dic_study_2.html",
    "title": "Understand DIC",
    "section": "",
    "text": "This mini-study aims to understand how the DIC (deviance information criterion) index works.\nThe common idea of an information criterion is \\(D + 2pD\\). The \\(D\\) (deviance) can also be presented as \\(-2\\) log-likelihood. Besides, a version of pD (effective number of parameters) of DIC (as same as JAGS program) is defined as the variance of log-likelihood.\nCode\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading iris data set.\nfrom sklearn.datasets import load_iris\niris = load_iris()"
  },
  {
    "objectID": "posts/dic_study_2.html#goals.",
    "href": "posts/dic_study_2.html#goals.",
    "title": "Understand DIC",
    "section": "Goals.",
    "text": "Goals.\n這個研究就簡單拿 iris 資料集來測試。 我們知道 iris 有 4 個 features: 花萼長度 (Sepal.Length), 花萼寬度 (Sepal.Width), 花瓣長度 (Petal.Length), 花瓣寬度 (Petal.Width)。\n我們今天就簡單用 Sepal.Length ~ Sepal.Width 這個模式來看看 DIC 怎麼算 此外，為了增加一點參數，我們再使用 3 個 target，建立階層線性模式。\n因此，模式如下：\n\\[\n\\begin{align}\n\\text{Likelihood:} \\\\\n\\text{Length} &\\sim N(\\mu _w, \\sigma^2) \\\\\n\\mu _w &= \\beta _0 + \\beta _{1i} \\text{Width} \\\\\n\\\\\n\\text{Priors:} \\\\\n\\beta _0, \\beta _{1i} &\\sim N(0,5) \\\\\n\\sigma &\\sim \\text{Exp}(1)\n\\end{align}\n\\]\n這邊的 \\(\\beta _{1i}\\) 是每一個 level 對應的參數。所以應該會有 3 個。"
  },
  {
    "objectID": "posts/dic_study_2.html#jags",
    "href": "posts/dic_study_2.html#jags",
    "title": "Understand DIC",
    "section": "jags",
    "text": "jags\nLet’s see how to run this model in jags.\nFirstly, we call the iris data set (from R default {datasets})\ndata(iris)\nSecondly, we define the data list and model string in the {R2jags} package. The {R2jags} allows users to write a jags model just like an R function.\n\nThe data list.\n\ndat_list = list(\n  sepal_length = iris$Sepal.Length,\n  sepal_width = iris$Sepal.Width,\n  species = iris$Species,\n  n = 150\n)\n\nThe model string.\n\nmod_string &lt;- \\(){\n  ## priors\n  beta0 ~ dnorm(0,1/5^2)\n  sigma ~ dexp(1)\n  for (j in 1:3){\n    beta1[j] ~ dnorm(0,1/5^2)\n  }\n  \n  ## likelihood\n  for (i in 1:n){\n    mu_w[i] &lt;- beta0 + beta1[species[i]] * sepal_width[i]\n    sepal_length[i] ~ dnorm(mu_w[i], 1/sigma^2) \n  }\n}\nFinally, we run this model through the jags function.\nfit &lt;- jags(data = dat_list, \n     parameters.to.save = c('beta0','beta1','sigma'),\n     model.file = (mod_string)\n     )\nThen, the output of this jags model is shown as follows:\n&gt; print(fit, digits = 3)\nInference for Bugs model at \"/var/folders/1f/8r50hwmn6m5dwrngfgysq4p40000gn/T//Rtmpbvfmga/modelab6b34cc72fd.txt\", fit using jags,\n 3 chains, each with 2000 iterations (first 1000 discarded)\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nbeta0      3.338   0.333   2.680   3.117   3.336   3.559   3.981 1.001  2400\nbeta1[1]   0.488   0.098   0.298   0.424   0.490   0.552   0.685 1.002  1700\nbeta1[2]   0.938   0.120   0.700   0.856   0.938   1.019   1.175 1.001  2400\nbeta1[3]   1.091   0.113   0.870   1.015   1.091   1.168   1.310 1.002  1900\nsigma      0.444   0.026   0.397   0.426   0.444   0.461   0.496 1.002  1600\ndeviance 180.851   3.199 176.629 178.531 180.100 182.442 188.810 1.002  1400\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts/dic_study_2.html#pymc",
    "href": "posts/dic_study_2.html#pymc",
    "title": "Understand DIC",
    "section": "pymc",
    "text": "pymc\nNow, we use the sync to replicate these results. We are interested in two things,\n\nRQ1. to compare the parameters of beta0, beta1, and sigma.\nRQ2. to compute the (expected) deviance, pD, and DIC.\n\n\n\nCode\niris_data = pd.DataFrame(iris['data'])\niris_data.columns = iris['feature_names']\niris_data\n\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\n\nCode\n#seed=1234\n\ntarget_index, target = pd.Series(iris['target']).factorize()\n#width_index, width = iris_data[1].factorize()\n\n\ndict = {\n    'target': iris['target_names'], \n    'target_index': target_index,\n    #'width_index': width_index\n}\ndict\n\n\n{'target': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'target_index': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}\n\n\n\nRQ1. Compare jags and pymc\n\n\nCode\nwith pm.Model(coords=dict) as iris_model:   \n    ## data\n    sepal_length = pm.Data('sepal_length', iris_data['sepal length (cm)'])\n    sepal_width = pm.Data('sepal_width', iris_data['sepal width (cm)'])\n    \n\n    ## priors\n    beta0 = pm.Normal('β0', 0,5)\n    beta1 = pm.Normal('β1', 0,5, shape=3)\n    sigma = pm.Exponential('σ',1)\n\n    ## likelihood\n    mu_w = beta0 + beta1[target_index] * sepal_width\n    Length = pm.Normal('length', mu_w, sigma, observed=sepal_length)\n\n    ## sampling\n    iris_post = pm.sample( draws=3000, chains=4, cores=4) \n    pm.compute_log_likelihood(iris_post)\n    #ra_4pl_predict = pm.sample_posterior_predictive(ra_4pl_post)\n\n\n/Users/garden/Library/Python/3.9/lib/python/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [β0, β1, σ]\nSampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [16000/16000 00:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(iris_post)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nβ0\n3.344\n0.329\n2.739\n3.968\n0.007\n0.005\n2393.0\n2772.0\n1.0\n\n\nβ1[0]\n0.487\n0.097\n0.296\n0.657\n0.002\n0.001\n2425.0\n2847.0\n1.0\n\n\nβ1[1]\n0.935\n0.119\n0.713\n1.159\n0.002\n0.002\n2469.0\n2887.0\n1.0\n\n\nβ1[2]\n1.089\n0.111\n0.877\n1.294\n0.002\n0.002\n2421.0\n2802.0\n1.0\n\n\nσ\n0.444\n0.026\n0.395\n0.492\n0.000\n0.000\n4241.0\n3826.0\n1.0\n\n\n\n\n\n\n\n\nConcluding remarks. For the RQ1, the outputs from jags and pymc show no significant differences.\n\n\nRQ2. Computing DIC.\nFirstly, let’s see the data structure of log_likelihood from the pm.compute_log_likelihood() function. It’s a three-way dimensions tensor. The first dim is for (4) chains, the second for (3000) draws, and the third for length of data (150).\n\n\nCode\niris_post.log_likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 3000, length_dim_0: 150)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 ... 2994 2995 2996 2997 2998 2999\n  * length_dim_0  (length_dim_0) int64 0 1 2 3 4 5 6 ... 144 145 146 147 148 149\nData variables:\n    length        (chain, draw, length_dim_0) float64 -0.1501 -0.2302 ... -1.553\nAttributes:\n    created_at:                 2024-03-27T11:15:05.525056\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.0xarray.DatasetDimensions:chain: 4draw: 3000length_dim_0: 150Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])length_dim_0(length_dim_0)int640 1 2 3 4 5 ... 145 146 147 148 149array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149])Data variables: (1)length(chain, draw, length_dim_0)float64-0.1501 -0.2302 ... -1.878 -1.553array([[[-0.15007587, -0.23018022, -0.17186102, ..., -0.15036302,\n         -2.03497421, -1.25868256],\n        [-0.14763492, -0.18836443, -0.24012757, ..., -0.15958522,\n         -1.93430897, -1.20251279],\n        [-0.13546664, -0.24928963, -0.11306264, ..., -0.12683694,\n         -2.32605535, -1.40839025],\n        ...,\n        [-0.14045847, -0.13434047, -0.13613556, ..., -0.14624427,\n         -2.03811634, -1.58747839],\n        [-0.15619394, -0.1480345 , -0.15840489, ..., -0.16924811,\n         -1.99564914, -1.57251438],\n        [-0.06604883, -0.06365481, -0.3631115 , ..., -0.06431746,\n         -1.54910513, -1.23877687]],\n\n       [[-0.2091668 , -0.22398206, -0.26267452, ..., -0.21925268,\n         -1.68602165, -1.25524037],\n        [-0.0207    , -0.00833144, -0.18972243, ..., -0.03910661,\n         -1.76095645, -1.53025309],\n        [-0.07599954, -0.0308677 , -0.08252084, ..., -0.02220397,\n         -1.67739141, -1.4524553 ],\n...\n        [-0.21175487, -0.3262845 , -0.14786837, ..., -0.20980681,\n         -2.44356451, -1.53501204],\n        [-0.10045583, -0.14453996, -0.20253765, ..., -0.12437101,\n         -2.18738956, -1.34751415],\n        [-0.06133903, -0.05676096, -0.27590568, ..., -0.06310933,\n         -1.92064681, -1.27690886]],\n\n       [[-0.12362904, -0.17491691, -0.13462518, ..., -0.09471016,\n         -1.73004549, -1.14761551],\n        [-0.04212361, -0.05487806, -0.19423224, ..., -0.10022129,\n         -2.30313771, -1.63956094],\n        [-0.04604174, -0.07543161, -0.13592791, ..., -0.06340857,\n         -2.12692774, -1.46189584],\n        ...,\n        [-0.10221615, -0.10216825, -0.2916557 , ..., -0.12732807,\n         -1.68477784, -1.35505866],\n        [-0.08520427, -0.07523728, -0.19593031, ..., -0.12138973,\n         -1.90040781, -1.57051397],\n        [-0.07718419, -0.07102337, -0.22110662, ..., -0.11846029,\n         -1.87751136, -1.5528609 ]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))length_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n      dtype='int64', name='length_dim_0', length=150))Attributes: (4)created_at :2024-03-27T11:15:05.525056arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.0\n\n\nSecondly, let’s try to compute the expected deviance (D) from this tensor. Due to the output from jags, we know the correct answer will be close to 180.851.\nNow, we need to compute the D (-2ll) for each point (there are a total of 150 points in this study\nTips. To sum up the dim we are interested in. In this case, we sum up the dim of length_dim_0 (axis=2). Then we can get 4*3000 draws for each points.\n\n\nCode\ny_ll = iris_post.log_likelihood['length'].sum(axis=2)\ny_deviance = -2*y_ll\ny_deviance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' (chain: 4, draw: 3000)&gt;\narray([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 2993 2994 2995 2996 2997 2998 2999xarray.DataArray'length'chain: 4draw: 3000180.1 180.2 181.5 181.0 181.9 183.1 ... 178.5 178.5 178.7 179.0 178.3array([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))Attributes: (0)\n\n\nThen get the posterior mean of it. It is 180.85.\n\n\nCode\ny_deviance.mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(180.83823109)xarray.DataArray'length'180.8array(180.83823109)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nThirdly, we need to compute the pD. We konw the pD will be close to 5.1.\n\n\nCode\ny_deviance.var()/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(5.37651436)xarray.DataArray'length'5.377array(5.37651436)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nFinally, we can compute the DIC. It will be close to 186.0. There are two kind of mthods to compute it,\n\nUsing log-likelihood. -2*y_ll.mean() + 2*y_ll.var()\nUsing deviance. y_deviance.mean() + y_deviance.var()/2\n\n\n\nCode\nDIC = y_deviance.mean() + y_deviance.var()/2\nDIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(186.21474545)xarray.DataArray'length'186.2array(186.21474545)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nYes!! Bingo!!"
  },
  {
    "objectID": "posts/dic_study_2.html#the-easy-function.",
    "href": "posts/dic_study_2.html#the-easy-function.",
    "title": "Understand DIC",
    "section": "The easy function.",
    "text": "The easy function.\nFurthermore, we write a function to output the strings like the jags program.\nIt will look like,\nDIC info (using the rule, pD = var(deviance)/2)\ndeviance = 180.85, pD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCode\ndef get_dic(posterior_tensor, var_names):\n    y_ll = posterior_tensor.log_likelihood[var_names].sum(axis=2).to_numpy()\n    y_deviance = -2*y_ll.mean()\n    y_pd = 2*y_ll.var()\n    y_dic = y_deviance + y_pd\n\n    y_print =   'DIC info (using the rule, pD = var(deviance)/2) \\n' +\\\n                'mean deviance = {:.3f}, pD = {:.3f} and DIC = {:.3f} \\n'.format(y_deviance, y_pd, y_dic) +\\\n                'DIC is an estimate of expected predictive error (lower deviance is better).'\n            \n    return print(y_print)\n\n\n\n\nCode\nget_dic(iris_post, var_names='length')\n\n\nDIC info (using the rule, pD = var(deviance)/2) \nmean deviance = 180.838, pD = 5.377 and DIC = 186.215 \nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts_zh/post0704.html",
    "href": "posts_zh/post0704.html",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "",
    "text": "今日這篇文章關於劍橋手冊 (Cambridge New Handbook of Mathematical Psychology)，只是為了記錄「數學心理學」的內容。日後也許還要記下CRC press的「IRT手冊」。讀這些手冊，倒也有趣，能了解當今發展的主題，看看未來還有什麼可做的。"
  },
  {
    "objectID": "posts_zh/post0704.html#卷一基礎與方法",
    "href": "posts_zh/post0704.html#卷一基礎與方法",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "卷一，基礎與方法。",
    "text": "卷一，基礎與方法。\n\n機率中的精選概念。 Selected concepts from probability, pp 1-84\n機率、隨機變數、選擇性。 Probability, random variables, and selectivity, pp 85-150\n函數方程式。 Functional equations, pp 151-193\n網路分析。 Network analysis, pp 194-273\n知識空間與學習空間。 Knowledge spaces and learning spaces, pp 274-321\n演化對局論（博弈論）。 Evolutionary game theory, pp 322-373\n選擇、偏好、效用：機率性和確定性表達。 Choice, preference, and utility: probabilistic and deterministic representations, pp 374-453\n認知的離散狀態模式。 Discrete state models of cognition, pp 454-503\n認知的貝氏階層模式。 Bayesian hierarchical models of cognition, pp 504-551\n模式評估和選擇。 Model evaluation and selection, pp 552-598"
  },
  {
    "objectID": "posts_zh/post0704.html#卷二建模與測量",
    "href": "posts_zh/post0704.html#卷二建模與測量",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "卷二，建模與測量。",
    "text": "卷二，建模與測量。\n\n決策建模的隨機方法。 Stochastic Methods for Modeling Decision-making, pp 1-70\n從理性觀點看加速選擇的擴散模式。 The Diffusion Model of Speeded Choice, from a Rational Perspective, pp 71-103\n基本心理架構的隨機基礎。 Stochastic Foundations of Elementary Mental Architectures, pp 104-127\n機率模式的可識別性，知識結構理論的例子。 Identifiability of Probabilistic Models, with Examples from Knowledge Structure Theory, pp 128-184\n認知與決策的量子模式。 Quantum Models of Cognition and Decision, pp 185-222\n計算認知神經科學。 Computational Cognitive Neuroscience, pp 223-270\n透過投票發現聚合屬性。 Discovering Aggregation Properties via Voting, pp 271-321\n基於相似性和特徵的分類：重現 RKBS 方法。 Categorization Based on Similarity and Features: The Reproducing Kernel Banach Space (RKBS) Approach, pp 322-373\n科學和幾何學中的意義公理。 The Axiom of Meaningfulness in Science and Geometry, pp 374-456"
  },
  {
    "objectID": "posts_zh/post0704.html#卷三知覺與認知過程",
    "href": "posts_zh/post0704.html#卷三知覺與認知過程",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "卷三，知覺與認知過程。",
    "text": "卷三，知覺與認知過程。\n\n初始視覺編碼的原理和結果。 Principles and Consequences of the Initial Visual Encoding, pp 1-41\n再選定範式中測量多感官整合。 Measuring Multisensory Integration in Selected Paradigms, pp 42-79\n費希納度量法：差異累積理論。 Fechnerian Scaling: Dissimilarity Cumulation Theory, pp 80-162\n人類學習的數學模式。 Mathematical Models of Human Learning, pp 163-217\n基於時變表達的記憶形式模式。 Formal Models of Memory Based on Temporally-Varying Representations, pp 218-264\n統計決策理論。 Statistical Decision Theory, pp 265-310\n停止信號任務中的反應抑制建模。 Modeling Response Inhibition in the Stop-Signal Task, pp 311-356\n近似貝氏計算。 Approximate Bayesian Computation, pp 357-384\n認知診斷模式。 Cognitive Diagnosis Models, pp 385-420\n神經影像學中的編碼模式。 Encoding Models in Neuroimaging, pp 421-472\n\n\n除了數學心理學，統計心理學或這裡所說的量化心理學、計量心理學，或許才是更為常見的研究領域。我們這裡記錄的是牛津手冊 (The Oxford Handbook of Quantitative Methods in Psycholog) 中的內容。只是他年代稍微早一點，是 2013 年出版的。\n\n卷一，基礎。 Foundations, 2013.\n卷二，統計分析。 Statistical Analysis, 2013."
  },
  {
    "objectID": "posts_zh/post0704.html#卷一基礎",
    "href": "posts_zh/post0704.html#卷一基礎",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "卷一，基礎。",
    "text": "卷一，基礎。\n\n前言。 Introduction\n量化方法的哲學。 The Philosophy of Quantitative Methods\n量化方法的倫理。 Quantitative Methods and Ethics\n特定群體。 Special Populations\n理論建構、模式建立、模式選擇。 Theory Construction, Model Building, and Model Selection\n量化心理學教學。 Teaching Quantitative Psychology\n當代測驗理論。 Modern Test Theory\nIRT 傳統與其應用。 The IRT Tradition and its Applications\n調查設計和測量發展。 Survey Design and Measure Development\n高風險測驗的建構和使用。 High-Stakes Test Construction and Test Use\n效果量和樣本數規劃。 Effect Size and Sample Size Planning\n因果推論的實驗設計：臨床試驗和迴歸不連續設計。 Experimental Design for Causal Inference: Clinical Trials and Regression Discontinuity Designs\n配對和傾向分數。 Matching and Propensity Scores\n反應時間實驗設計與分析。 Designs for and Analyses of Response Time Experiments\n觀察性方法。 Observational Methods\n流行病學方法、概念與分析，心理學中的進階應用為例子。 A Primer of Epidemiologic Methods, Concepts, and Analysis With Examples and More Advanced Applications Within Psychology\n方案評估：原理、程序與實踐。 Program Evaluation: Principles, Procedures, and Practices\n統計估計方法概述。 Overview of Statistical Estimation Methods\n穩健統計估計。 Robust Statistical Estimation\n貝氏統計方法。 Bayesian Statistical Methods\n數學建模。 Mathematical Modeling\n學術研究中的蒙地卡羅分析。 Monte Carlo Analysis in Academic Research\n網路分析：重要概念定義指南。 Network Analysis: A Definitional Guide to Important Concepts"
  },
  {
    "objectID": "posts_zh/post0704.html#卷二統計分析",
    "href": "posts_zh/post0704.html#卷二統計分析",
    "title": "「劍橋新版數學心理學手冊」與「牛津心理學量化方法手冊」的內容",
    "section": "卷二，統計分析。",
    "text": "卷二，統計分析。\n\n前言。 Introduction\n傳統、古典方法概述。 Overview of Traditional/Classical Statistical Approaches\n廣義線性模式。 Generalized Linear Models\n類別方法（類別資料分析）。 Categorical Methods\n配置頻次分析。 Configural Frequency Analysis\n非參數統計技術。 Nonparametric Statistical Techniques\n對應分析。 Correspondence Analysis\n空間分析。 Spatial Analysis\n影像資料分析。 Analysis of Imaging Data\n雙胞胎研究與行為遺傳學。 Twin Studies and Behavior Genetics\n基因量化分析。 Quantitative Analysis of Genes\n多元度量法。 Multidimensional Scaling\n潛在變項測量模式。 Latent Variable Measurement Models\n多層次迴歸和多層次結構方程式。 Multilevel Regression and Multilevel Structural Equation Modeling\n結構方程式。 Structural Equation Models\n中介分析的發展。 Developments in Mediation Analysis\n調節分析。 Moderation\n縱向資料分析（長期追蹤資料分析）。 Longitudinal Data Analysis\n動態系統和連續時間模式。 Dynamical Systems and Models of Continuous Time\n密集型追蹤資料。 Intensive Longitudinal Data\n動態因素分析：個人過程建模。 Dynamic Factor Analysis: Modeling Person-Specific Process\n時間序列分析。 Time Series Analysis\n事件史資料分析。 Analyzing Event History Data\n聚類和分類。 Clustering and Classification\n潛在類別分析和有限混合模式。 Latent Class Analysis and Finite Mixture Modeling\n分類計量學。 Taxometrics\n遺漏值方法。 Missing Data Methods\n次級資料分析。 Secondary Data Analysis\n資料採礦。 Data Mining\n後設分析與量化研究綜述。 Meta-Analysis and Quantitative Research Synthesis\n量化方法中的常見謬誤。 Common Fallacies in Quantitative Research Methodology"
  },
  {
    "objectID": "posts_zh/journal_mbr.html",
    "href": "posts_zh/journal_mbr.html",
    "title": "「多變量行為研究」期刊最有影響力的 20 篇文章（MultivariateBehavRes 2010-2023）",
    "section": "",
    "text": "Multivariate Behavioral Research 多變量行為研究，是 Society of Multivariate Experimental Psychology (SMEP) 多變量實驗心理學會的期刊，由 Taylor & Francis 出版。每年 6 期。"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#主要資訊",
    "href": "posts_zh/journal_mbr.html#主要資訊",
    "title": "「多變量行為研究」期刊最有影響力的 20 篇文章（MultivariateBehavRes 2010-2023）",
    "section": "主要資訊",
    "text": "主要資訊\n\n我們來看一下「多變量行為研究」期刊的一些有趣數據。在 2010-2023 年，這期刊共有 865 篇文章，其中合著作者平均 2.72 位。每年成長率 -6.48%，文章平均年齡 6.68 歲，每篇文章平均被引用 34.93 次。這些數字背後反映了這個領域的研究活力和影響力。"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#研究話題地圖",
    "href": "posts_zh/journal_mbr.html#研究話題地圖",
    "title": "「多變量行為研究」期刊最有影響力的 20 篇文章（MultivariateBehavRes 2010-2023）",
    "section": "研究話題地圖",
    "text": "研究話題地圖\n\n「多變量行為研究」期刊從 2010 年以來，最關鍵的 20+ 篇文章，大致呈現出 5 條主要的研究話題。以及許多單獨的重要文章。我們也提供文獻縮寫（和上圖對應），doi，以及中文翻譯名稱。文獻排序和圖上一樣，由上至下。有興趣的同學可以用 doi 去找到對應的文章。\n\n話題 1：因素分析（紅色，比重 12.5%）\n\nSASS DA, 2010, DOI 10.1080/00273170903504810 探索性因素分析中旋轉標準的比較研究\nMOORE TM, 2015, DOI 10.1080/00273171.2014.973990 部分指定目標矩陣的叠代： 探索性和貝葉斯確證因子分析中的應用\n\n\n\n話題 2：長期追蹤資料議題（藍色，比重 12.5%）\n\nMAXWELL SE, 2011, DOI 10.1080/00273171.2011.606716 縱向中介橫斷面分析中的偏差：自回歸模型下的部分中介和完全中介 🌟 最出圈文章！(GCS: 833) 🌟 圈內人最愛！(LCS: 15)\nREICHARDT CS, 2011, DOI 10.1080/00273171.2011.606740 評論：三波數據是否足以評估中介作用？\n\n\n\n話題 3：密集型追蹤資料、動態結構方程式（綠色，比重 43.75%）\n\nSTEELE JS, 2011, DOI 10.1080/00273171.2011.625305 自我調節和核心調節情感過程的潛微分方程模型\nCHOW SM, 2011, DOI 10.1080/00273171.2011.563697 具有時變參數的動態因素分析模型\nFERRER E, 2012, DOI 10.1080/00273171.2012.640605 利用個體內部和個體之間的變異模式分析情感二元互動的動態性\nVOELKLE MC, 2014, DOI 10.1080/00273171.2014.889593 為研究人與人之間和人與人之間的結構建立統一的框架： 在兩種研究範式之間架起一座橋梁\nJONGERLING J, 2015, DOI 10.1080/00273171.2014.1003772 多層次 ar(1) 模型：考慮特質分數、慣性和創新變異的個體間差異\nHAMAKER EL, 2018, DOI 10.1080/00273171.2018.1446819 密集縱向數據建模的前沿：Cogito 研究中情感測量的動態結構方程模型\nBRINGMANN LF, 2018, DOI 10.1080/00273171.2018.1439722 使用時變向量自回歸模型建模二人組中的非平穩情緒動態\n\n\n\n話題 4：貝氏資料分析議題（紫色，比重 18.75%）\n\nSONG HR, 2012, DOI 10.1080/00273171.2012.640593 隨機系數動態因子模型的貝葉斯估計\nSCHUURMAN NK, 2016, DOI 10.1080/00273171.2015.1065398 多級自回歸模型中協方差矩陣的逆 Wishart 先驗規範比較\nEPSKAMP S, 2018, DOI 10.1080/00273171.2018.1454823 橫截面和時間序列數據中的高斯圖形模型 🌟 圈內人最愛！(LCS: 15)\n\n\n\n話題 5：機器學習議題（橘色，比重 12.5%）\n\nMCNEISH DM, 2015, DOI 10.1080/00273171.2015.1036965 使用lasso進行預測因子選擇並緩解過度擬合： 行為科學中長期被忽視的方法\nWILLIAMS DR, 2019, DOI 10.1080/00273171.2019.1575716 關於心理網絡的非規則化估計"
  },
  {
    "objectID": "posts_zh/1stBayes10.html",
    "href": "posts_zh/1stBayes10.html",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\n這邊可以注意幾件事關於 julia。\n\n如果是要生成一個 scaler，那寫 rand(Normal()) 就可以了。寫 rand(Normal(),1) 會產生一個向量 vector，導致後面的函數無法接受。\nlogpdf.(Normal(___, ___)), y) 由於後面的 y 是一個向量，所以需要寫成 logpdf. 做廣播計算，否則計算會出問題。\nΘ = Float64[] 和 push!(Θ, theta) 的搭配。我不知道這樣做跟 Array{Float64}(undef, S) 誰的效能比較好？可能是後者吧。但因為沒有體感差別，所以不確定。\n\n\n\nCode\n#-------\n\n# 初始化接受计数器\n\"\"\"\n(s2, t2, mu, y, delta2, S)\n這幾個引數，可寫可不寫。\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # 調整這個參數，可改變接受率。\n    S = 10_000\n    theta = 0.0\n    Θ = Float64[]  # 用于存储 theta 的数组\n    accept_count = 0  # 初始化接受计数器\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # 增加接受计数器\n        end\n\n        push!(Θ, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Θ, accept_rate\nend\n\n\n# 调用函数\nΘ, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7848\n\n\n\n如果 δ2 = 0.1，接受率大約在 0.70 左右。\n如果 δ2 = 1.0，接受率大約在 0.50 - 0.60。\n如果 δ2 = 2.0，接受率大約在 0.3 左右。\n\n調高一點，\n來畫點圖。 這樣沒錯，跟書上的是一樣的。\n\n\nCode\nplot(Θ)"
  },
  {
    "objectID": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "href": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\n這邊可以注意幾件事關於 julia。\n\n如果是要生成一個 scaler，那寫 rand(Normal()) 就可以了。寫 rand(Normal(),1) 會產生一個向量 vector，導致後面的函數無法接受。\nlogpdf.(Normal(___, ___)), y) 由於後面的 y 是一個向量，所以需要寫成 logpdf. 做廣播計算，否則計算會出問題。\nΘ = Float64[] 和 push!(Θ, theta) 的搭配。我不知道這樣做跟 Array{Float64}(undef, S) 誰的效能比較好？可能是後者吧。但因為沒有體感差別，所以不確定。\n\n\n\nCode\n#-------\n\n# 初始化接受计数器\n\"\"\"\n(s2, t2, mu, y, delta2, S)\n這幾個引數，可寫可不寫。\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # 調整這個參數，可改變接受率。\n    S = 10_000\n    theta = 0.0\n    Θ = Float64[]  # 用于存储 theta 的数组\n    accept_count = 0  # 初始化接受计数器\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # 增加接受计数器\n        end\n\n        push!(Θ, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Θ, accept_rate\nend\n\n\n# 调用函数\nΘ, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7848\n\n\n\n如果 δ2 = 0.1，接受率大約在 0.70 左右。\n如果 δ2 = 1.0，接受率大約在 0.50 - 0.60。\n如果 δ2 = 2.0，接受率大約在 0.3 左右。\n\n調高一點，\n來畫點圖。 這樣沒錯，跟書上的是一樣的。\n\n\nCode\nplot(Θ)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "jw tsai",
    "section": "",
    "text": "About this jw tsai."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#the-modeling-part.",
    "href": "posts/IntroJuliaConnectoR.html#the-modeling-part.",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "The modeling part.",
    "text": "The modeling part.\nLet’s follow the example from the Github readme post to show how a basic RT-IRT model works. This package is version 0.2.0.\n\n\nCode\n## You have to give a name to the Julia Environment.\nex &lt;- juliaImport(\"ExtendedRtIrtModeling\")\n\n\nStarting Julia ...\n\n\nCode\njuliaEval('Pkg.status(\"ExtendedRtIrtModeling\")')\n\n\nStatus `~/.julia/environments/v1.10/Project.toml`\n  [1fd685a6] ExtendedRtIrtModeling v0.2.0 `https://github.com/jiewenTsai/ExtendedRtIrtModeling.jl#main`\n\n\nIn the original Github post, the user doesn’t have to fill in all five arguments of InputData because the values for \\(\\kappa\\) and logT are automatically calculated by Y and T. However, to get it working with R, I’ve come up with another struct called InputData4R, which is specifically for R users.\n\n\nCode\n## import your data set\nCond = ex$setCond(\n  nChain=3, \n  nIter=3000,\n  nSubj=300,\n  nItem=10,\n  nFeat=4\n  )\nData = ex$InputData4R(\n    Y = as.matrix(demo[2:11]),\n    # you must write this line!\n    κ = as.matrix(demo[2:11]-0.5),\n    T = as.matrix(exp(demo[12:21])),\n    logT = as.matrix(demo[12:21]),\n    X = as.matrix(demo[22:25])\n)\n\n## build a model and sample it!\nMCMC = ex$GibbsRtIrt(Cond, Data=Data)\n## Notice the `` style for sample!\nex$`sample!`(MCMC)\n\n\n&lt;Julia object of type ExtendedRtIrtModeling.GibbsRtIrt&gt;\nExtendedRtIrtModeling.GibbsRtIrt(ExtendedRtIrtModeling.SimConditions(300, 10, 4, 3000, 3, 1500, 1, 10, 0.5, 0.5), ExtendedRtIrtModeling.InputData4R([0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 1 0; 0 1 … 1 0], [-0.5 -0.5 … -0.5 -0.5; -0.5 -0.5 … -0.5 -0.5; … ; -0.5 -0.5 … 0.5 -0.5; -0.5 0.5 … 0.5 -0.5], [19.31727937946127 68.3745035485581 … 9.006981510434914 17.778680238058843; 46.89917102861648 54.38019363641357 … 12.085422811327593 56.71280369681312; … ; 27.194098966393437 42.182270397465956 … 25.584840292351714 119.46219960089533; 35.05786562994266 82.26946350420168 … 43.991656898689 24.754319523777408], [2.961 4.225 … 2.198 2.878; 3.848 3.996 … 2.492 4.038; … ; 3.303 3.742 … 3.242 4.783; 3.557 4.41 … 3.784 3.209], [0.0 -3.362 -0.2 1.063; 1.0 -0.081 2.347 1.063; … ; 1.0 -0.064 0.445 -0.318; 0.0 -0.449 -0.676 1.063]), Float64[], ExtendedRtIrtModeling.InputPara([0.3137423041364331 0.3415237894886167 … 0.11183446557866958 0.05790546531201218; 0.25638289531148994 0.3566247394185408 … 0.09091133809565621 0.05204210938445989; … ; 0.16332544992167716 0.32252191915665784 … 0.3726970452403918 0.20414636916984366; 0.16949649737031303 0.33905817515050696 … 0.09826986769390608 0.265528224767851], [-1.578676215807743; -0.9203745423441859; … ; -0.10664096875232022; -1.0594805763603632;;], [1.0445433708534635; 1.2644831163990256; … ; 1.5193557431767508; 1.4485813534579228;;], [-0.38647204662333506; -1.2538527924663074; … ; -0.7752956269598285; 0.14931123796642268;;], [0.43162195514686763; 0.21061616328551705; … ; -0.22905611375794022; 0.08899580003777864;;], [3.30761010661482; 3.927589114707531; … ; 2.923971130851645; 3.725603662587948;;], [0.39824774784188943; 0.29277495083742106; … ; 0.2886170757599006; 0.22597973246093783;;], Float64[], [0.0 0.0; -0.1287760810605439 0.2005212948416813; … ; 0.24017749307214029 -0.03657573094224067; 0.008839392786410502 -0.025342572043680473], Float64[], [1.2044553464348549 -0.02744833933113381; -0.02744833933113381 0.16019891442664336]), ExtendedRtIrtModeling.OutputPost([-2.6646011859395102 -0.7085410419736089 … -0.7565130647503794 0.3286140470016074; -3.0079544580340505 -1.1650659603554177 … -2.071385732680604 -0.054216474343903594; … ; -1.8557839828202527 -0.393936588592598 … -0.5882928400994076 0.15979568723647153; -3.1041380789618436 -0.35870758669354397 … -0.6967538992227875 0.06328643828884624;;; -0.579926218388121 -0.7694249965520048 … -3.1140451246568315 2.7402901051293598; -3.075016116622655 -1.2406363622468877 … -2.503322534120851 0.13695410653130946; … ; -2.1335402869236435 -0.43925842963308437 … -0.6019945753771228 -0.03565198116535539; -2.522017090166128 -0.5649540622562687 … -0.6170559255436903 0.19848121170487343;;; -0.21804544538641857 -0.676995006519194 … -3.4915357429600893 0.616787024738047; -3.2312874601622315 -1.0896767020878362 … -1.6598629898813122 0.24991302410254954; … ; -2.058305777627975 0.062354750660974304 … -0.7882966622933067 0.025527360107749916; -1.578676215807743 -0.9203745423441859 … -0.7752956269598285 0.14931123796642268], [-0.0425113941831008 0.1282335730526704 … 1.5331777368989858 1.7535271264893457; 0.3638056929725671 -0.21324689798075652 … 0.3052904922863609 0.24033474721531817; … ; 0.4500141724394335 0.3545728122723381 … 0.3455600652811882 0.28449342578509995; 0.47014791738809614 0.019075281312259804 … 0.3242380318762755 0.2275179082474999;;; 0.17732270162233285 -0.17177064495680683 … 0.38833920211163014 0.4249138574806367; 0.5220091561873619 0.08732510564249564 … 0.3201903557326946 0.23361730578487278; … ; 0.4851239544558618 0.15852466776851742 … 0.2954354859256739 0.28978480634782655; 0.47977068862041994 0.10186204831034498 … 0.25952115767849615 0.2565337283085566;;; 0.33191292942991374 0.16440927325200227 … 0.27058856993690944 0.267348416884789; 0.547237380074301 0.545919153533114 … 0.26898546105113474 0.30199737061250354; … ; 0.644551828361199 0.11662762772835153 … 0.3059250699897882 0.23977366140057393; 0.43162195514686763 0.21061616328551705 … 0.2886170757599006 0.22597973246093783], [0.0 -0.008071132892489258 … -0.04385131392470358 1.0198932915748098; 0.0 -0.14345204339751882 … 0.025654283556167064 0.20401456528600365; … ; 0.0 -0.0641979689318157 … -0.05137005697139327 0.16457547267983416; 0.0 -0.12147626570379585 … -0.0019527630286848394 0.16191524109541255;;; 0.0 -0.18717208184736112 … -0.07618324595309203 0.3322180492450072; 0.0 -0.09180706041632986 … 0.004693343977941498 0.20421324356814724; … ; 0.0 -0.12074793470842118 … -0.031644537136957834 0.15784539422938612; 0.0 -0.052943956404781334 … -0.031277558977574706 0.15679446856218407;;; 0.0 -0.17231495142403486 … -0.08223291509286375 0.22461367602728788; 0.0 -0.05853035695319146 … -0.018305599288515745 0.1666650137419096; … ; 0.0 -0.03999951218457838 … -0.054987581840916844 0.1894554448424559; 0.0 -0.1287760810605439 … -0.02744833933113381 0.16019891442664336], [-6569.736753186094; -4842.764638091079; … ; -4473.173949907039; -4487.210078406651;;; -5175.308294452186; -4733.926409728214; … ; -4481.404160259903; -4462.206318327685;;; -4977.26606658747; -4689.974515372751; … ; -4478.351068758786; -4489.820864529613], ExtendedRtIrtModeling.InputPara(Float64[], [-2.339140454802746, -0.8909902414379508, -1.1585403208427778, -1.3377675192797214, -1.3198749686220312, -2.1598378084914684, -1.5504785037230635, -0.8934914953299252, -1.4086145497748246, -1.0481788260462772  …  -0.5338470723604528, -0.5671380168192632, -0.30805574544862707, -0.7757490378144447, -0.4395734278679758, -0.642476359839492, -0.8945642160907551, -0.7441127733988433, -0.3685888055857756, -0.8378304510800633], [1.3649196787327174, 1.6242478162295646, 1.8786325288283046, 2.372861758278475, 1.1731708612135332, 1.6563893360202497, 1.929380648727127, 0.8488668227020303, 1.4099641876226139, 1.639109576090664], [-0.46014231920221, -0.9082292498630915, 0.0026549300628270676, -0.03611296191425276, -0.1819282742645006, -0.7138704020640786, -1.7037110085735492, -1.165853395209599, -0.6773706604262375, 0.036880122106446644], [0.5114266254432426, 0.1843530644597257, -0.07963821093280457, 0.40812578462251875, 0.20470059908612467, 0.8011944318530605, 0.4831962809053242, 0.6291542010773029, 0.22979667406161905, -0.023816094617968145  …  -0.6149134860670965, -0.29321154046497633, -0.15129580655926447, -0.4500161352318017, -0.03553249606029144, -0.5698173452921096, -0.014884078023460852, 0.28790323134123963, -0.18763900108587359, -0.2253154411481993], [3.3099877211930604, 3.8859688206968706, 4.584338076511737, 3.8260526200195115, 3.035070871167548, 3.2470790614614486, 2.987014600550239, 3.711404444961663, 2.8721389617221793, 3.742665330848496], [0.3789923746830642, 0.30514640194298825, 0.2964493508162058, 0.3485604176039491, 0.3337010438357073, 0.4805703515210315, 0.26507844934737856, 0.2938075495956295, 0.29601557353109936, 0.25207545571197304], Float64[], [0.0, -0.06682429491245291, 0.5396322579382782, 0.20926088101101464, -0.0055328740907218, 0.0, 0.1925441845060982, 0.003392391671879015, -0.036044007677121966, -0.021190721760734818], Float64[], [1.007975743291263, -0.042295263001564415, -0.042295263001564415, 0.17602999557781845])))\n\n\nCode\nex$coef(MCMC)\n\n\n&gt;&gt; Results for ExtendedRtIrtModeling.GibbsRtIrt.\n1) Item Parameters.\n┌───────┬─────────┬─────────┬─────────┬─────────┐\n│  Item │       a │       b │       λ │     σ²t │\n│ Int64 │ Float64 │ Float64 │ Float64 │ Float64 │\n├───────┼─────────┼─────────┼─────────┼─────────┤\n│     1 │   1.365 │  -0.460 │   3.310 │   0.379 │\n│     2 │   1.624 │  -0.908 │   3.886 │   0.305 │\n│     3 │   1.879 │   0.003 │   4.584 │   0.296 │\n│     4 │   2.373 │  -0.036 │   3.826 │   0.349 │\n│     5 │   1.173 │  -0.182 │   3.035 │   0.334 │\n│     6 │   1.656 │  -0.714 │   3.247 │   0.481 │\n│     7 │   1.929 │  -1.704 │   2.987 │   0.265 │\n│     8 │   0.849 │  -1.166 │   3.711 │   0.294 │\n│     9 │   1.410 │  -0.677 │   2.872 │   0.296 │\n│    10 │   1.639 │   0.037 │   3.743 │   0.252 │\n└───────┴─────────┴─────────┴─────────┴─────────┘\n2) Covariance of Person Parameters.\n┌──────┬────────┬────────┐\n│ Coef │      θ │      ζ │\n├──────┼────────┼────────┤\n│    θ │  1.008 │ -0.042 │\n│    ζ │ -0.042 │  0.176 │\n└──────┴────────┴────────┘\n3) Regression Coefficients.\n┌──────┬────────┬────────┐\n│ Coef │      θ │      ζ │\n├──────┼────────┼────────┤\n│   β0 │  0.000 │  0.000 │\n│   β1 │ -0.067 │  0.193 │\n│   β2 │  0.540 │  0.003 │\n│   β3 │  0.209 │ -0.036 │\n│   β4 │ -0.006 │ -0.021 │\n└──────┴────────┴────────┘\n4) Criterion.\n┌──────────┬──────────┐\n│ Deviance │      DIC │\n├──────────┼──────────┤\n│ 8960.638 │ 9604.430 │\n└──────────┴──────────┘\n\n\nCode\nex$precis(MCMC)\n\n\n&gt;&gt; Results for ExtendedRtIrtModeling.GibbsRtIrt.\n1) Item Response Model.\n┌────────────┬─────────┬─────────┬──────────┬─────────┬─────────┬─────────┬────────┐\n│ parameters │    mean │     std │      ess │    rhat │     q05 │     q95 │    Sig │\n│     Symbol │ Float64 │ Float64 │  Float64 │ Float64 │ Float64 │ Float64 │ String │\n├────────────┼─────────┼─────────┼──────────┼─────────┼─────────┼─────────┼────────┤\n│         a1 │   1.365 │   0.215 │ 2031.342 │   0.999 │   1.027 │   1.729 │      * │\n│         a2 │   1.624 │   0.280 │ 1023.142 │   1.000 │   1.213 │   2.120 │      * │\n│         a3 │   1.879 │   0.284 │ 1340.683 │   1.000 │   1.440 │   2.374 │      * │\n│         a4 │   2.373 │   0.411 │  796.093 │   1.008 │   1.796 │   3.093 │      * │\n│         a5 │   1.173 │   0.181 │ 2488.416 │   1.000 │   0.888 │   1.475 │      * │\n│         a6 │   1.656 │   0.266 │ 1307.783 │   1.001 │   1.256 │   2.143 │      * │\n│         a7 │   1.929 │   0.438 │  332.926 │   1.006 │   1.309 │   2.761 │      * │\n│         a8 │   0.849 │   0.173 │ 1622.491 │   1.000 │   0.564 │   1.137 │      * │\n│         a9 │   1.410 │   0.224 │ 1572.424 │   1.000 │   1.069 │   1.798 │      * │\n│        a10 │   1.639 │   0.236 │ 1900.847 │   1.000 │   1.275 │   2.048 │      * │\n│         b1 │  -0.460 │   0.127 │ 2354.512 │   1.000 │  -0.672 │  -0.258 │      * │\n│         b2 │  -0.908 │   0.133 │ 1369.807 │   1.000 │  -1.136 │  -0.700 │      * │\n│         b3 │   0.003 │   0.108 │ 2045.609 │   1.000 │  -0.172 │   0.180 │        │\n│         b4 │  -0.036 │   0.098 │ 1978.555 │   1.000 │  -0.195 │   0.122 │        │\n│         b5 │  -0.182 │   0.133 │ 2616.222 │   1.000 │  -0.404 │   0.038 │        │\n│         b6 │  -0.714 │   0.122 │ 2039.870 │   1.000 │  -0.920 │  -0.521 │      * │\n│         b7 │  -1.704 │   0.206 │  493.275 │   1.003 │  -2.063 │  -1.398 │      * │\n│         b8 │  -1.166 │   0.263 │ 1989.826 │   1.000 │  -1.635 │  -0.805 │      * │\n│         b9 │  -0.677 │   0.130 │ 2086.720 │   1.000 │  -0.899 │  -0.470 │      * │\n│        b10 │   0.037 │   0.110 │ 2444.179 │   1.000 │  -0.142 │   0.215 │        │\n└────────────┴─────────┴─────────┴──────────┴─────────┴─────────┴─────────┴────────┘\n2) Response Time Model.\n┌────────────┬─────────┬─────────┬──────────┬─────────┬─────────┬─────────┬────────┐\n│ parameters │    mean │     std │      ess │    rhat │     q05 │     q95 │    Sig │\n│     Symbol │ Float64 │ Float64 │  Float64 │ Float64 │ Float64 │ Float64 │ String │\n├────────────┼─────────┼─────────┼──────────┼─────────┼─────────┼─────────┼────────┤\n│         λ1 │   3.310 │   0.044 │ 1795.493 │   1.001 │   3.238 │   3.383 │      * │\n│         λ2 │   3.886 │   0.041 │ 1878.261 │   1.002 │   3.819 │   3.954 │      * │\n│         λ3 │   4.584 │   0.040 │ 1764.888 │   1.001 │   4.519 │   4.650 │      * │\n│         λ4 │   3.826 │   0.043 │ 1799.599 │   1.003 │   3.757 │   3.895 │      * │\n│         λ5 │   3.035 │   0.042 │ 1748.269 │   1.003 │   2.965 │   3.105 │      * │\n│         λ6 │   3.247 │   0.048 │ 2179.353 │   1.002 │   3.169 │   3.326 │      * │\n│         λ7 │   2.987 │   0.039 │ 1727.063 │   1.002 │   2.922 │   3.052 │      * │\n│         λ8 │   3.711 │   0.041 │ 1675.810 │   1.002 │   3.645 │   3.779 │      * │\n│         λ9 │   2.872 │   0.041 │ 1705.345 │   1.001 │   2.805 │   2.942 │      * │\n│        λ10 │   3.743 │   0.039 │ 1775.891 │   1.002 │   3.679 │   3.805 │      * │\n│       σ²t1 │   0.379 │   0.034 │ 4503.800 │   1.000 │   0.326 │   0.438 │      * │\n│       σ²t2 │   0.305 │   0.027 │ 4309.889 │   1.000 │   0.263 │   0.352 │      * │\n│       σ²t3 │   0.296 │   0.027 │ 4360.224 │   1.000 │   0.255 │   0.343 │      * │\n│       σ²t4 │   0.349 │   0.031 │ 4233.063 │   1.001 │   0.301 │   0.403 │      * │\n│       σ²t5 │   0.334 │   0.030 │ 4470.407 │   1.000 │   0.287 │   0.385 │      * │\n│       σ²t6 │   0.481 │   0.042 │ 3763.722 │   1.000 │   0.415 │   0.554 │      * │\n│       σ²t7 │   0.265 │   0.024 │ 4348.786 │   1.000 │   0.228 │   0.309 │      * │\n│       σ²t8 │   0.294 │   0.026 │ 4179.601 │   1.001 │   0.253 │   0.339 │      * │\n│       σ²t9 │   0.296 │   0.026 │ 4486.570 │   1.000 │   0.255 │   0.341 │      * │\n│      σ²t10 │   0.252 │   0.023 │ 4012.436 │   1.000 │   0.216 │   0.293 │      * │\n└────────────┴─────────┴─────────┴──────────┴─────────┴─────────┴─────────┴────────┘\n3) Structural Model.\n┌────────────┬─────────┬─────────┬──────────┬─────────┬─────────┬─────────┬────────┐\n│ parameters │    mean │     std │      ess │    rhat │     q05 │     q95 │    Sig │\n│     Symbol │ Float64 │ Float64 │  Float64 │ Float64 │ Float64 │ Float64 │ String │\n├────────────┼─────────┼─────────┼──────────┼─────────┼─────────┼─────────┼────────┤\n│     β[0,1] │   0.000 │   0.000 │      NaN │     NaN │   0.000 │   0.000 │      * │\n│     β[1,1] │  -0.067 │   0.064 │ 3287.917 │   1.000 │  -0.172 │   0.037 │        │\n│     β[2,1] │   0.540 │   0.044 │ 1181.655 │   1.001 │   0.469 │   0.612 │      * │\n│     β[3,1] │   0.209 │   0.035 │ 2558.694 │   1.001 │   0.153 │   0.266 │      * │\n│     β[4,1] │  -0.006 │   0.032 │ 3458.413 │   0.999 │  -0.057 │   0.046 │        │\n│     β[0,2] │   0.000 │   0.000 │      NaN │     NaN │   0.000 │   0.000 │      * │\n│     β[1,2] │   0.193 │   0.019 │ 3710.222 │   1.001 │   0.160 │   0.225 │      * │\n│     β[2,2] │   0.003 │   0.010 │ 3848.907 │   1.000 │  -0.013 │   0.019 │        │\n│     β[3,2] │  -0.036 │   0.010 │ 3981.757 │   1.000 │  -0.052 │  -0.020 │      * │\n│     β[4,2] │  -0.021 │   0.010 │ 4063.009 │   1.000 │  -0.037 │  -0.006 │      * │\n│     Σ[1,1] │   1.008 │   0.114 │ 2541.021 │   1.001 │   0.835 │   1.204 │      * │\n│     Σ[1,2] │  -0.042 │   0.028 │ 4110.644 │   1.000 │  -0.089 │   0.005 │        │\n│     Σ[2,1] │  -0.042 │   0.028 │ 4110.644 │   1.000 │  -0.089 │   0.005 │        │\n│     Σ[2,2] │   0.176 │   0.017 │ 4055.249 │   1.001 │   0.149 │   0.206 │      * │\n└────────────┴─────────┴─────────┴──────────┴─────────┴─────────┴─────────┴────────┘\n\n\nLastly, let’s check out this package.\n\n\nCode\n## See the objects in ex.\nls(ex)\n\n\n [1] \"coef\"                          \"drawItemDifficulty\"           \n [3] \"drawItemDiscrimination\"        \"drawItemIntensity\"            \n [5] \"drawItemIntensityCross\"        \"drawItemIntensityCrossQr\"     \n [7] \"drawItemTimeResidual\"          \"drawItemTimeResidualCross\"    \n [9] \"drawItemTimeResidualCrossQr\"   \"drawQrWeightsCrossQr\"         \n[11] \"drawQrWeightsLatentQr\"         \"drawRaPgRandomVariable\"       \n[13] \"drawSubjAbility\"               \"drawSubjAbilityNull\"          \n[15] \"drawSubjCoefficients\"          \"drawSubjCorrCross\"            \n[17] \"drawSubjCorrCrossQr\"           \"drawSubjCovariance\"           \n[19] \"drawSubjCovariance2One\"        \"drawSubjCovarianceLatent\"     \n[21] \"drawSubjCovarianceLatentQr\"    \"drawSubjCovarianceNull\"       \n[23] \"drawSubjCovarianceNull2One\"    \"drawSubjSpeed\"                \n[25] \"drawSubjSpeedCross\"            \"drawSubjSpeedCrossQr\"         \n[27] \"drawSubjSpeedLatent\"           \"drawSubjSpeedLatentQr\"        \n[29] \"drawSubjSpeedNull\"             \"eval\"                         \n[31] \"evaluate\"                      \"getBias\"                      \n[33] \"getDic\"                        \"getLogLikelihoodMlIrt\"        \n[35] \"getLogLikelihoodRtIrt\"         \"getLogLikelihoodRtIrtCross\"   \n[37] \"getLogLikelihoodRtIrtCrossQr\"  \"getLogLikelihoodRtIrtLatent\"  \n[39] \"getLogLikelihoodRtIrtLatentQr\" \"getLogLikelihoodRtIrtNull\"    \n[41] \"getPrecisTable\"                \"getRmse\"                      \n[43] \"getSubjCoefficients\"           \"getSubjCoefficientsLatent\"    \n[45] \"getSubjCoefficientsLatentQr\"   \"getSubjCoefficientsMlIrt\"     \n[47] \"GibbsMlIrt\"                    \"GibbsRtIrt\"                   \n[49] \"GibbsRtIrt2\"                   \"GibbsRtIrtCross\"              \n[51] \"GibbsRtIrtCross2\"              \"GibbsRtIrtCrossQr\"            \n[53] \"GibbsRtIrtLatent\"              \"GibbsRtIrtLatent2\"            \n[55] \"GibbsRtIrtLatentQr\"            \"GibbsRtIrtNull\"               \n[57] \"include\"                       \"InputData\"                    \n[59] \"InputData4R\"                   \"InputPara\"                    \n[61] \"OutputDic\"                     \"OutputPost\"                   \n[63] \"OutputPostCross\"               \"OutputPostCrossQr\"            \n[65] \"OutputPostMlIrt\"               \"OutputPostRtIrtLatent\"        \n[67] \"OutputPostRtIrtLatentQr\"       \"precis\"                       \n[69] \"sample!\"                       \"setCond\"                      \n[71] \"setTrueParaRtIrt\"              \"SimConditions\"                \n[73] \"SimEvaluation\""
  }
]