[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "jw tsai",
    "section": "",
    "text": "About this jw tsai."
  },
  {
    "objectID": "posts_zh/1stBayes10.html",
    "href": "posts_zh/1stBayes10.html",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\né€™é‚Šå¯ä»¥æ³¨æ„å¹¾ä»¶äº‹é—œæ–¼ juliaã€‚\n\nå¦‚æœæ˜¯è¦ç”Ÿæˆä¸€å€‹ scalerï¼Œé‚£å¯« rand(Normal()) å°±å¯ä»¥äº†ã€‚å¯« rand(Normal(),1) æœƒç”¢ç”Ÿä¸€å€‹å‘é‡ vectorï¼Œå°è‡´å¾Œé¢çš„å‡½æ•¸ç„¡æ³•æ¥å—ã€‚\nlogpdf.(Normal(___, ___)), y) ç”±æ–¼å¾Œé¢çš„ y æ˜¯ä¸€å€‹å‘é‡ï¼Œæ‰€ä»¥éœ€è¦å¯«æˆ logpdf. åšå»£æ’­è¨ˆç®—ï¼Œå¦å‰‡è¨ˆç®—æœƒå‡ºå•é¡Œã€‚\nÎ˜ = Float64[] å’Œ push!(Î˜, theta) çš„æ­é…ã€‚æˆ‘ä¸çŸ¥é“é€™æ¨£åšè·Ÿ Array{Float64}(undef, S) èª°çš„æ•ˆèƒ½æ¯”è¼ƒå¥½ï¼Ÿå¯èƒ½æ˜¯å¾Œè€…å§ã€‚ä½†å› ç‚ºæ²’æœ‰é«”æ„Ÿå·®åˆ¥ï¼Œæ‰€ä»¥ä¸ç¢ºå®šã€‚\n\n\n\nCode\n#-------\n\n# åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n\"\"\"\n(s2, t2, mu, y, delta2, S)\né€™å¹¾å€‹å¼•æ•¸ï¼Œå¯å¯«å¯ä¸å¯«ã€‚\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # èª¿æ•´é€™å€‹åƒæ•¸ï¼Œå¯æ”¹è®Šæ¥å—ç‡ã€‚\n    S = 10_000\n    theta = 0.0\n    Î˜ = Float64[]  # ç”¨äºå­˜å‚¨ theta çš„æ•°ç»„\n    accept_count = 0  # åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # å¢åŠ æ¥å—è®¡æ•°å™¨\n        end\n\n        push!(Î˜, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Î˜, accept_rate\nend\n\n\n# è°ƒç”¨å‡½æ•°\nÎ˜, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7753\n\n\n\nå¦‚æœ Î´2 = 0.1ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.70 å·¦å³ã€‚\nå¦‚æœ Î´2 = 1.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.50 - 0.60ã€‚\nå¦‚æœ Î´2 = 2.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.3 å·¦å³ã€‚\n\nèª¿é«˜ä¸€é»ï¼Œ\nä¾†ç•«é»åœ–ã€‚ é€™æ¨£æ²’éŒ¯ï¼Œè·Ÿæ›¸ä¸Šçš„æ˜¯ä¸€æ¨£çš„ã€‚\n\n\nCode\nplot(Î˜)"
  },
  {
    "objectID": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "href": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\né€™é‚Šå¯ä»¥æ³¨æ„å¹¾ä»¶äº‹é—œæ–¼ juliaã€‚\n\nå¦‚æœæ˜¯è¦ç”Ÿæˆä¸€å€‹ scalerï¼Œé‚£å¯« rand(Normal()) å°±å¯ä»¥äº†ã€‚å¯« rand(Normal(),1) æœƒç”¢ç”Ÿä¸€å€‹å‘é‡ vectorï¼Œå°è‡´å¾Œé¢çš„å‡½æ•¸ç„¡æ³•æ¥å—ã€‚\nlogpdf.(Normal(___, ___)), y) ç”±æ–¼å¾Œé¢çš„ y æ˜¯ä¸€å€‹å‘é‡ï¼Œæ‰€ä»¥éœ€è¦å¯«æˆ logpdf. åšå»£æ’­è¨ˆç®—ï¼Œå¦å‰‡è¨ˆç®—æœƒå‡ºå•é¡Œã€‚\nÎ˜ = Float64[] å’Œ push!(Î˜, theta) çš„æ­é…ã€‚æˆ‘ä¸çŸ¥é“é€™æ¨£åšè·Ÿ Array{Float64}(undef, S) èª°çš„æ•ˆèƒ½æ¯”è¼ƒå¥½ï¼Ÿå¯èƒ½æ˜¯å¾Œè€…å§ã€‚ä½†å› ç‚ºæ²’æœ‰é«”æ„Ÿå·®åˆ¥ï¼Œæ‰€ä»¥ä¸ç¢ºå®šã€‚\n\n\n\nCode\n#-------\n\n# åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n\"\"\"\n(s2, t2, mu, y, delta2, S)\né€™å¹¾å€‹å¼•æ•¸ï¼Œå¯å¯«å¯ä¸å¯«ã€‚\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # èª¿æ•´é€™å€‹åƒæ•¸ï¼Œå¯æ”¹è®Šæ¥å—ç‡ã€‚\n    S = 10_000\n    theta = 0.0\n    Î˜ = Float64[]  # ç”¨äºå­˜å‚¨ theta çš„æ•°ç»„\n    accept_count = 0  # åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # å¢åŠ æ¥å—è®¡æ•°å™¨\n        end\n\n        push!(Î˜, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Î˜, accept_rate\nend\n\n\n# è°ƒç”¨å‡½æ•°\nÎ˜, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7753\n\n\n\nå¦‚æœ Î´2 = 0.1ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.70 å·¦å³ã€‚\nå¦‚æœ Î´2 = 1.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.50 - 0.60ã€‚\nå¦‚æœ Î´2 = 2.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.3 å·¦å³ã€‚\n\nèª¿é«˜ä¸€é»ï¼Œ\nä¾†ç•«é»åœ–ã€‚ é€™æ¨£æ²’éŒ¯ï¼Œè·Ÿæ›¸ä¸Šçš„æ˜¯ä¸€æ¨£çš„ã€‚\n\n\nCode\nplot(Î˜)"
  },
  {
    "objectID": "posts_zh/journal_mbr.html",
    "href": "posts_zh/journal_mbr.html",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "",
    "text": "Multivariate Behavioral Research å¤šè®Šé‡è¡Œç‚ºç ”ç©¶ï¼Œæ˜¯ Society of Multivariate Experimental Psychology (SMEP) å¤šè®Šé‡å¯¦é©—å¿ƒç†å­¸æœƒçš„æœŸåˆŠï¼Œç”± Taylor & Francis å‡ºç‰ˆã€‚æ¯å¹´ 6 æœŸã€‚"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#ä¸»è¦è³‡è¨Š",
    "href": "posts_zh/journal_mbr.html#ä¸»è¦è³‡è¨Š",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "ä¸»è¦è³‡è¨Š",
    "text": "ä¸»è¦è³‡è¨Š\n\næˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠçš„ä¸€äº›æœ‰è¶£æ•¸æ“šã€‚åœ¨ 2010-2023 å¹´ï¼Œé€™æœŸåˆŠå…±æœ‰ 865 ç¯‡æ–‡ç« ï¼Œå…¶ä¸­åˆè‘—ä½œè€…å¹³å‡ 2.72 ä½ã€‚æ¯å¹´æˆé•·ç‡ -6.48%ï¼Œæ–‡ç« å¹³å‡å¹´é½¡ 6.68 æ­²ï¼Œæ¯ç¯‡æ–‡ç« å¹³å‡è¢«å¼•ç”¨ 34.93 æ¬¡ã€‚é€™äº›æ•¸å­—èƒŒå¾Œåæ˜ äº†é€™å€‹é ˜åŸŸçš„ç ”ç©¶æ´»åŠ›å’Œå½±éŸ¿åŠ›ã€‚"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "href": "posts_zh/journal_mbr.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "ç ”ç©¶è©±é¡Œåœ°åœ–",
    "text": "ç ”ç©¶è©±é¡Œåœ°åœ–\n\nã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠå¾ 2010 å¹´ä»¥ä¾†ï¼Œæœ€é—œéµçš„ 20+ ç¯‡æ–‡ç« ï¼Œå¤§è‡´å‘ˆç¾å‡º 5 æ¢ä¸»è¦çš„ç ”ç©¶è©±é¡Œã€‚ä»¥åŠè¨±å¤šå–®ç¨çš„é‡è¦æ–‡ç« ã€‚æˆ‘å€‘ä¹Ÿæä¾›æ–‡ç»ç¸®å¯«ï¼ˆå’Œä¸Šåœ–å°æ‡‰ï¼‰ï¼Œdoiï¼Œä»¥åŠä¸­æ–‡ç¿»è­¯åç¨±ã€‚æ–‡ç»æ’åºå’Œåœ–ä¸Šä¸€æ¨£ï¼Œç”±ä¸Šè‡³ä¸‹ã€‚æœ‰èˆˆè¶£çš„åŒå­¸å¯ä»¥ç”¨ doi å»æ‰¾åˆ°å°æ‡‰çš„æ–‡ç« ã€‚\n\nè©±é¡Œ 1ï¼šå› ç´ åˆ†æï¼ˆç´…è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nSASS DA, 2010, DOI 10.1080/00273170903504810 æ¢ç´¢æ€§å› ç´ åˆ†æä¸­æ—‹è½‰æ¨™æº–çš„æ¯”è¼ƒç ”ç©¶\nMOORE TM, 2015, DOI 10.1080/00273171.2014.973990 éƒ¨åˆ†æŒ‡å®šç›®æ¨™çŸ©é™£çš„å ä»£ï¼š æ¢ç´¢æ€§å’Œè²è‘‰æ–¯ç¢ºè­‰å› å­åˆ†æä¸­çš„æ‡‰ç”¨\n\n\n\nè©±é¡Œ 2ï¼šé•·æœŸè¿½è¹¤è³‡æ–™è­°é¡Œï¼ˆè—è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nMAXWELL SE, 2011, DOI 10.1080/00273171.2011.606716 ç¸±å‘ä¸­ä»‹æ©«æ–·é¢åˆ†æä¸­çš„åå·®ï¼šè‡ªå›æ­¸æ¨¡å‹ä¸‹çš„éƒ¨åˆ†ä¸­ä»‹å’Œå®Œå…¨ä¸­ä»‹ ğŸŒŸ æœ€å‡ºåœˆæ–‡ç« ï¼(GCS: 833) ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 15)\nREICHARDT CS, 2011, DOI 10.1080/00273171.2011.606740 è©•è«–ï¼šä¸‰æ³¢æ•¸æ“šæ˜¯å¦è¶³ä»¥è©•ä¼°ä¸­ä»‹ä½œç”¨ï¼Ÿ\n\n\n\nè©±é¡Œ 3ï¼šå¯†é›†å‹è¿½è¹¤è³‡æ–™ã€å‹•æ…‹çµæ§‹æ–¹ç¨‹å¼ï¼ˆç¶ è‰²ï¼Œæ¯”é‡ 43.75%ï¼‰\n\nSTEELE JS, 2011, DOI 10.1080/00273171.2011.625305 è‡ªæˆ‘èª¿ç¯€å’Œæ ¸å¿ƒèª¿ç¯€æƒ…æ„Ÿéç¨‹çš„æ½›å¾®åˆ†æ–¹ç¨‹æ¨¡å‹\nCHOW SM, 2011, DOI 10.1080/00273171.2011.563697 å…·æœ‰æ™‚è®Šåƒæ•¸çš„å‹•æ…‹å› ç´ åˆ†ææ¨¡å‹\nFERRER E, 2012, DOI 10.1080/00273171.2012.640605 åˆ©ç”¨å€‹é«”å…§éƒ¨å’Œå€‹é«”ä¹‹é–“çš„è®Šç•°æ¨¡å¼åˆ†ææƒ…æ„ŸäºŒå…ƒäº’å‹•çš„å‹•æ…‹æ€§\nVOELKLE MC, 2014, DOI 10.1080/00273171.2014.889593 ç‚ºç ”ç©¶äººèˆ‡äººä¹‹é–“å’Œäººèˆ‡äººä¹‹é–“çš„çµæ§‹å»ºç«‹çµ±ä¸€çš„æ¡†æ¶ï¼š åœ¨å…©ç¨®ç ”ç©¶ç¯„å¼ä¹‹é–“æ¶èµ·ä¸€åº§æ©‹æ¢\nJONGERLING J, 2015, DOI 10.1080/00273171.2014.1003772 å¤šå±¤æ¬¡ ar(1) æ¨¡å‹ï¼šè€ƒæ…®ç‰¹è³ªåˆ†æ•¸ã€æ…£æ€§å’Œå‰µæ–°è®Šç•°çš„å€‹é«”é–“å·®ç•°\nHAMAKER EL, 2018, DOI 10.1080/00273171.2018.1446819 å¯†é›†ç¸±å‘æ•¸æ“šå»ºæ¨¡çš„å‰æ²¿ï¼šCogito ç ”ç©¶ä¸­æƒ…æ„Ÿæ¸¬é‡çš„å‹•æ…‹çµæ§‹æ–¹ç¨‹æ¨¡å‹\nBRINGMANN LF, 2018, DOI 10.1080/00273171.2018.1439722 ä½¿ç”¨æ™‚è®Šå‘é‡è‡ªå›æ­¸æ¨¡å‹å»ºæ¨¡äºŒäººçµ„ä¸­çš„éå¹³ç©©æƒ…ç·’å‹•æ…‹\n\n\n\nè©±é¡Œ 4ï¼šè²æ°è³‡æ–™åˆ†æè­°é¡Œï¼ˆç´«è‰²ï¼Œæ¯”é‡ 18.75%ï¼‰\n\nSONG HR, 2012, DOI 10.1080/00273171.2012.640593 éš¨æ©Ÿç³»æ•¸å‹•æ…‹å› å­æ¨¡å‹çš„è²è‘‰æ–¯ä¼°è¨ˆ\nSCHUURMAN NK, 2016, DOI 10.1080/00273171.2015.1065398 å¤šç´šè‡ªå›æ­¸æ¨¡å‹ä¸­å”æ–¹å·®çŸ©é™£çš„é€† Wishart å…ˆé©—è¦ç¯„æ¯”è¼ƒ\nEPSKAMP S, 2018, DOI 10.1080/00273171.2018.1454823 æ©«æˆªé¢å’Œæ™‚é–“åºåˆ—æ•¸æ“šä¸­çš„é«˜æ–¯åœ–å½¢æ¨¡å‹ ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 15)\n\n\n\nè©±é¡Œ 5ï¼šæ©Ÿå™¨å­¸ç¿’è­°é¡Œï¼ˆæ©˜è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nMCNEISH DM, 2015, DOI 10.1080/00273171.2015.1036965 ä½¿ç”¨lassoé€²è¡Œé æ¸¬å› å­é¸æ“‡ä¸¦ç·©è§£éåº¦æ“¬åˆï¼š è¡Œç‚ºç§‘å­¸ä¸­é•·æœŸè¢«å¿½è¦–çš„æ–¹æ³•\nWILLIAMS DR, 2019, DOI 10.1080/00273171.2019.1575716 é—œæ–¼å¿ƒç†ç¶²çµ¡çš„éè¦å‰‡åŒ–ä¼°è¨ˆ"
  },
  {
    "objectID": "posts/note0716.html",
    "href": "posts/note0716.html",
    "title": "Note0716 (mediation analysis with pymc)",
    "section": "",
    "text": "Code\nimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n#dat = pd.read_csv('data1463_fin3.csv')\n\n\nç‚ºäº†ä¹‹å¾Œé‚„å¯ä»¥è·‘é€™å€‹æ¨¡å‹ï¼Œæ ¹æ“šåŸæœ¬çš„è³‡æ–™é‡æ–°æ¨¡æ“¬ä¸€ç­†è³‡æ–™ã€‚\n(ä½†é€™é‚Šæ²’è€ƒæ…®åˆ°åŸæœ¬æ•¸å€¼ä¹‹é–“çš„ç›¸é—œæƒ…å½¢) å·²è€ƒæ…®é€²å»ï¼ï¼ç”¨å¤šå…ƒå¸¸æ…‹åˆ†é…æ¨¡æ“¬äº†ã€‚\n\nacd1eap \\(\\sim N(2.8510193121856177e-05, 0.8921482479092293)\\)\nscleap \\(\\sim N(1.9250494837216173e-06, 0.7154395945457549)\\)\nc1 \\(\\sim Ber(1,0.19822282980177716)\\)\nc2 \\(\\sim Ber(1,0.11483253588516747)\\)\nc3 \\(\\sim Ber(1,0.11551606288448393)\\)\n\n\n\nCode\n'''\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\n'''\n\n\n'\\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\\n'\n\n\n\n\nCode\ndat_mn = np.array(\n    [2.8510193121856177e-05,\n     1.9250494837216173e-06,\n     0.19822282980177716,\n     0.11483253588516747,\n     0.11551606288448393]\n)\ndat_cov = np.array([\n    [ 0.7959285 ,  0.16304568, -0.01541732, -0.01689872, -0.02973076],\n    [ 0.16304568,  0.51185381, -0.05390401, -0.00531492, -0.02327021],\n    [-0.01541732, -0.05390401,  0.15903925, -0.022778  , -0.02291358],\n    [-0.01689872, -0.00531492, -0.022778  ,  0.10171555, -0.01327408],\n    [-0.02973076, -0.02327021, -0.02291358, -0.01327408,  0.10224199]\n])\ndat = np.random.multivariate_normal(dat_mn, dat_cov, 1000)\ndat = pd.DataFrame(dat, columns=['acd1_eap', 'scl_eap', 'c1', 'c2', 'c3'])\ndat['c1'] = dat['c1'] &gt; 0.5\ndat['c2'] = dat['c2'] &gt; 0.5\ndat['c3'] = dat['c3'] &gt; 0.5\n\n\n\n\nCode\ndat\n\n\n\n\n\n\n\n\n\n\nacd1_eap\nscl_eap\nc1\nc2\nc3\n\n\n\n\n0\n0.346316\n-0.792151\nTrue\nFalse\nFalse\n\n\n1\n-0.305374\n1.621382\nFalse\nTrue\nFalse\n\n\n2\n0.563012\n0.828286\nFalse\nFalse\nFalse\n\n\n3\n0.058392\n-0.276411\nFalse\nFalse\nFalse\n\n\n4\n0.181640\n-1.291429\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.717853\n-0.833434\nTrue\nFalse\nFalse\n\n\n996\n-0.812274\n-0.038150\nFalse\nFalse\nFalse\n\n\n997\n-0.461906\n-0.635552\nFalse\nFalse\nFalse\n\n\n998\n0.294100\n0.594206\nFalse\nFalse\nFalse\n\n\n999\n-0.543131\n-0.928300\nTrue\nFalse\nFalse\n\n\n\n\n1000 rows Ã— 5 columns\n\n\n\n\n\n\nCode\n'''\ndat_dict = {\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \n}\ndat = pd.DataFrame(dat_dict)\n'''\n\n\n\"\\ndat_dict = {\\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \\n}\\ndat = pd.DataFrame(dat_dict)\\n\"\n\n\n\n\nCode\nwith pm.Model() as model:\n    acd1eap = pm.ConstantData('acd1eap', dat.acd1_eap)\n    scleap = pm.ConstantData('scleap', dat.scl_eap)\n    c1 = pm.ConstantData('c1', dat.c1)\n    c2 = pm.ConstantData('c2', dat.c2)\n    c3 = pm.ConstantData('c3', dat.c3)\n\n    # intercept\n    acd1eap_Intercept = pm.Normal('acd1eap_Intercept', mu=0, sigma=100)\n    scleap_Intercept = pm.Normal('scleap_Intercept', mu=0, sigma=100)\n    \n    # noise\n    acd1eap_Sigma = pm.HalfCauchy(\"acd1eap_Sigma\", 1)\n    scleap_Sigma = pm.HalfCauchy(\"scleap_Sigma\", 1)\n\n    # slope\n    acd1eap_scleap = pm.Normal('acd1eap_scleap', mu=0, sigma=100)\n    acd1eap_c1 = pm.Normal('acd1eap_c1', mu=0, sigma=100)\n    acd1eap_c2 = pm.Normal('acd1eap_c2', mu=0, sigma=100)\n    acd1eap_c3 = pm.Normal('acd1eap_c3', mu=0, sigma=100)\n    scleap_c1 = pm.Normal('scleap_c1', mu=0, sigma=100)\n    scleap_c2 = pm.Normal('scleap_c2', mu=0, sigma=100)\n    scleap_c3 = pm.Normal('scleap_c3', mu=0, sigma=100)\n\n    # likelihood\n    pm.Normal(\"y_likelihood\", mu=acd1eap_Intercept + acd1eap_scleap * scleap  + acd1eap_c1 * c1 + acd1eap_c2 * c2 + acd1eap_c3 * c3, sigma =  acd1eap_Sigma, observed = acd1eap  )\n    pm.Normal('m_likelihood', mu=scleap_Intercept + scleap_c1 * c1 + scleap_c2 * c2 + scleap_c3 * c3, sigma = scleap_Sigma, observed = scleap)\n    \n    trace_med = pm.sample(2000, chains=4, cores=4)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [acd1eap_Intercept, scleap_Intercept, acd1eap_Sigma, scleap_Sigma, acd1eap_scleap, acd1eap_c1, acd1eap_c2, acd1eap_c3, scleap_c1, scleap_c2, scleap_c3]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\nCode\npm.model_to_graphviz(model)\n\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n&lt;graphviz.graphs.Digraph at 0x17209fb80&gt;\n\n\n\n\nCode\naz.summary(trace_med)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nacd1eap_Intercept\n0.042\n0.036\n-0.025\n0.109\n0.000\n0.000\n9687.0\n6732.0\n1.0\n\n\nscleap_Intercept\n0.045\n0.030\n-0.013\n0.102\n0.000\n0.000\n10276.0\n6795.0\n1.0\n\n\nacd1eap_scleap\n0.374\n0.038\n0.305\n0.445\n0.000\n0.000\n13151.0\n6209.0\n1.0\n\n\nacd1eap_c1\n0.002\n0.067\n-0.126\n0.122\n0.001\n0.001\n11284.0\n6502.0\n1.0\n\n\nacd1eap_c2\n-0.008\n0.085\n-0.169\n0.148\n0.001\n0.001\n12240.0\n6047.0\n1.0\n\n\nacd1eap_c3\n-0.049\n0.088\n-0.210\n0.120\n0.001\n0.001\n12138.0\n5986.0\n1.0\n\n\nscleap_c1\n-0.181\n0.055\n-0.285\n-0.081\n0.001\n0.000\n11615.0\n6883.0\n1.0\n\n\nscleap_c2\n0.031\n0.071\n-0.091\n0.173\n0.001\n0.001\n11680.0\n7188.0\n1.0\n\n\nscleap_c3\n-0.076\n0.073\n-0.225\n0.053\n0.001\n0.001\n12466.0\n6045.0\n1.0\n\n\nacd1eap_Sigma\n0.881\n0.020\n0.845\n0.919\n0.000\n0.000\n14248.0\n6463.0\n1.0\n\n\nscleap_Sigma\n0.734\n0.017\n0.703\n0.765\n0.000\n0.000\n12699.0\n6039.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{tsai2023,\n  author = {Tsai, JW},\n  title = {Note0716 (Mediation Analysis with Pymc)},\n  date = {2023-07-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTsai, J. (2023, July 16). Note0716 (mediation analysis with\npymc)."
  },
  {
    "objectID": "posts/Lorem Ipsum.html",
    "href": "posts/Lorem Ipsum.html",
    "title": "Lorem Ipsum",
    "section": "",
    "text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industryâ€™s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#what-is-lorem-ipsum",
    "href": "posts/Lorem Ipsum.html#what-is-lorem-ipsum",
    "title": "Lorem Ipsum",
    "section": "",
    "text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industryâ€™s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#why-do-we-use-it",
    "href": "posts/Lorem Ipsum.html#why-do-we-use-it",
    "title": "Lorem Ipsum",
    "section": "Why do we use it?",
    "text": "Why do we use it?\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using â€˜Content here, content hereâ€™, making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for â€˜lorem ipsumâ€™ will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like)."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#where-does-it-come-from",
    "href": "posts/Lorem Ipsum.html#where-does-it-come-from",
    "title": "Lorem Ipsum",
    "section": "Where does it come from?",
    "text": "Where does it come from?\nContrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of â€œde Finibus Bonorum et Malorumâ€ (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, â€œLorem ipsum dolor sit amet..â€, comes from a line in section 1.10.32.\nThe standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from â€œde Finibus Bonorum et Malorumâ€ by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham."
  },
  {
    "objectID": "posts/Lorem Ipsum.html#where-can-i-get-some",
    "href": "posts/Lorem Ipsum.html#where-can-i-get-some",
    "title": "Lorem Ipsum",
    "section": "Where can I get some?",
    "text": "Where can I get some?\nThere are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which donâ€™t look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isnâ€™t anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc."
  },
  {
    "objectID": "posts/note0322.html",
    "href": "posts/note0322.html",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "",
    "text": "Goal. In this note, we will demonstrate how to use the output from brms to make (simple slope) testings and plots."
  },
  {
    "objectID": "posts/note0322.html#make-data",
    "href": "posts/note0322.html#make-data",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make data",
    "text": "Make data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayestestR)\nlibrary(rstan)\nlibrary(mvtnorm)\n\n\nNow we have to make a data set including 4 variables: Y, X, M, and W.\nSuppose these four variables follow a multivariate-normal distribution as follows,\nLet X is a treatment (binary data), and M is a response time data (lognormal).\n\\[\\begin{equation}\n\\begin{bmatrix}\nY \\\\ X \\\\M \\\\W\n\\end{bmatrix}\n= \\text{MVN}\\left(\n\\begin{bmatrix}\n0 \\\\0 \\\\ 0\\\\ 0\n\\end{bmatrix},\n\\begin{bmatrix}\n1 & 0.1 & -0.8 & 0.8  \\\\\n0.1 & 1 & -0.6 & 0\\\\\n-0.8 & -0.6 & 1 & 0.6\\\\\n0.8 & 0 & 0.6 & 1\n\\end{bmatrix}\n\\right)\n\\end{equation}\\]\n\n\nCode\nset.seed(12345)\nreal_sigma &lt;- matrix(c(1, 0.1, -0.8, 0.8,\n                      0.1, 1, -0.6, 0,\n                      -0.8, -0.6, 1, 0.6,\n                      0.8, 0, 0.6, 1), nrow = 4)\nreal_mean &lt;- c(0,0,0,0)\n\nreal_data &lt;- rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma)\n\n\nWarning in rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma): sigma is\nnumerically not positive semidefinite\n\n\nCode\ndat &lt;- data.frame(\n  ID = paste0('s', str_pad(1:1000, width = 4, side = 'left', pad = 0)),\n  Y = real_data[,1],\n  X = real_data[,2] &gt; mean(real_data[,2]),\n  M = exp(real_data[,3]),\n  W = real_data[,4]\n)\n\nhead(dat)\n\n\n     ID          Y     X         M          W\n1 s0001  0.3617174  TRUE 0.4790760 -0.1639895\n2 s0002  0.1110080 FALSE 2.0490296  0.2046531\n3 s0003  0.6187169 FALSE 2.6247616  1.4401394\n4 s0004  1.0347476  TRUE 0.5084054  0.6434533\n5 s0005 -1.1477318 FALSE 4.8633851  0.2686688\n6 s0006  0.2802449  TRUE 0.1476185 -1.2412565"
  },
  {
    "objectID": "posts/note0322.html#fit-bayesian-model-in-brms",
    "href": "posts/note0322.html#fit-bayesian-model-in-brms",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Fit Bayesian model in brms",
    "text": "Fit Bayesian model in brms\nNow we specify the formula as follows (in Bayesian).\n\\[\\begin{align}\n\\text{Likelihood.}\\\\\nY &\\sim N(\\mu_y, \\sigma_y^2) \\\\\nM &\\sim \\log N(\\mu_m, \\sigma_m^2) \\\\\n\n\\mu_y &= \\beta_{01} + \\beta_x X + \\beta_m M + \\beta_w W + \\beta _{mw}M \\cdot W \\\\\n\\mu_m &= \\beta_{02} + \\beta_x X \\\\ \\\\\n\n\\text{Priors.}\\\\\n\n\\sigma_y^2, \\sigma_m^2 & \\sim \\text{Exp}(1) \\\\\n\\beta_{01}, ..., \\beta _{x} &\\sim N(0,5)\n\\end{align}\\]\n\n\nCode\nbf1 &lt;- bf(Y~X+M+W+M*W, family = gaussian())\nbf2 &lt;- bf(M~X, family = lognormal())\npriors &lt;- prior(normal(0,5), class = b, resp = Y) + \n  prior(normal(0,5), class = b, resp = M) + \n  prior(exponential(1), class = sigma, resp = Y) +\n  prior(exponential(1), class = sigma, resp = M) \n\n\n\nfit &lt;- brm(\n  bf1+bf2+set_rescor(FALSE), \n  data = dat,\n  cores = 4\n)\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nCode\nprint(fit, digits = 3)\n\n\n Family: MV(gaussian, lognormal) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: Y ~ X + M + W + M * W \n         M ~ X \n   Data: dat (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nY_Intercept    0.648     0.035    0.580    0.716 1.001     4338     3242\nM_Intercept    0.490     0.043    0.407    0.574 1.002     4917     3090\nY_XTRUE       -0.164     0.039   -0.241   -0.086 1.000     4874     3349\nY_M           -0.366     0.010   -0.386   -0.346 1.001     3476     3265\nY_W            0.604     0.020    0.565    0.641 1.000     4427     3384\nY_M:W          0.100     0.006    0.088    0.112 1.000     2928     2864\nM_XTRUE       -0.856     0.059   -0.970   -0.742 1.001     5208     2953\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma_Y    0.570     0.013    0.544    0.597 1.000     5277     2537\nsigma_M    0.962     0.021    0.921    1.005 1.002     5424     2974\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/note0322.html#bayesian-testing",
    "href": "posts/note0322.html#bayesian-testing",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Bayesian testing",
    "text": "Bayesian testing\n\n\nCode\nfit |&gt; \n  describe_posterior(\n    effects = \"all\",\n    component = \"all\",\n    #test = c(\"p_direction\", \"p_significance\"),\n    centrality = \"all\"\n  )\n\n\nWarning: Multivariate response models are not yet supported for tests `rope` and\n  `p_rope`.\n\n\nSummary of Posterior Distribution M\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        M |   0.49 |  0.49 |  0.49 | [ 0.41,  0.57] | 100% | 1.000 | 4898.00\nXTRUE       |        M |  -0.85 | -0.86 | -0.85 | [-0.97, -0.74] | 100% | 0.999 | 5258.00\n\n# Fixed effects sigma M\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        M |   0.96 | 0.96 | 0.96 | [ 0.92,  1.00] | 100% | 1.000 | 5345.00\n\n# Fixed effects Y\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        Y |   0.65 |  0.65 |  0.65 | [ 0.58,  0.72] | 100% | 1.000 | 4325.00\nXTRUE       |        Y |  -0.16 | -0.16 | -0.16 | [-0.24, -0.09] | 100% | 1.000 | 4867.00\nM           |        Y |  -0.37 | -0.37 | -0.36 | [-0.39, -0.35] | 100% | 1.001 | 3499.00\nW           |        Y |   0.60 |  0.60 |  0.60 | [ 0.56,  0.64] | 100% | 1.000 | 4384.00\nM:W         |        Y |   0.10 |  0.10 |  0.10 | [ 0.09,  0.11] | 100% | 1.000 | 2908.00\n\n# Fixed effects sigma Y\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        Y |   0.57 | 0.57 | 0.57 | [ 0.54,  0.60] | 100% | 1.000 | 5226.00\n\n\nThe function hypothesis() can be used to test specific parameter.\n\n\nCode\nfit_hypo &lt;- hypothesis(\n  fit, \n  class = 'b',\n  alpha = .05,\n  hypothesis = \n  c(\n    Low = \"Y_M - Y_M:W = 0\",\n    Medium = \"Y_M = 0\",\n    High = \"Y_M + Y_M:W = 0\")\n  ) \nfit_hypo\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1        Low    -0.47      0.02    -0.50    -0.44         NA        NA    *\n2     Medium    -0.37      0.01    -0.39    -0.35         NA        NA    *\n3       High    -0.27      0.01    -0.28    -0.25         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "posts/note0322.html#make-plots",
    "href": "posts/note0322.html#make-plots",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make plots",
    "text": "Make plots\n\n\nCode\n## plotting ----\ncond_plot &lt;- conditional_effects(fit)\n\n\n\n\nCode\ncond_plot$`Y.Y_M:W` |&gt;\n  ggplot(aes(x = M, y = Y), ) +\n  \n  geom_ribbon(aes(x = effect1__, y = estimate__, linetype = effect2__,\n                  ymin = lower__, ymax = upper__, fill = factor(effect2__)), alpha = 0.5) +\n  geom_line(aes(x = effect1__, y = estimate__, linetype = effect2__)) +\n  scale_fill_manual(name = 'W effects',\n                    values = c(\"coral4\", \"coral3\", \"coral2\"),\n                    labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\"),\n                    ) +\n  scale_linetype_manual(name = 'W effects',\n                        values = c(\"solid\", \"dotted\", \"dashed\"),\n                        labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\")) +\n  labs(x = \"the M\", \n       y = \"the Y\") +\n  ggtitle('M * W') +\n  annotate(\"text\", x=10, y=-8, label= \"Low \\n b=-0.47, [-0.49, -0.44]\") +\n  annotate(\"text\", x=25, y=-7, label= \"Average \\n b=-0.37, [-0.38, -0.35]\") +\n  annotate(\"text\", x=20, y=0, label= \"High \\n b=-0.27, [-0.28, -0.25]\") +\n  \n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "notesZh.html",
    "href": "notesZh.html",
    "title": "ä¸­æ–‡è¨˜äº‹",
    "section": "",
    "text": "å¸Œæœ›ä»¥é€±è¨˜çš„å½¢å¼è¨˜éŒ„ä¸‹é‡è¦çš„å­¸ç¿’æ­·ç¨‹ã€‚\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nÂ \n\n\nè²æ°å­¸ç¿’è³‡æºæ¨è–¦\n\n\n1 min\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nChapter 10\n\n\n2 min\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰\n\n\n1 min\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nÂ \n\n\nLorem Ipsum\n\n\n3 min\n\n\n\n\n\n\n\nMar 27, 2024\n\n\nUnderstand DIC\n\n\n6 min\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nHow to conduct simple slope analysis and make plot with brms\n\n\n4 min\n\n\n\n\n\n\n\nJul 16, 2023\n\n\nNote0716 (mediation analysis with pymc)\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dic_study_2.html",
    "href": "posts/dic_study_2.html",
    "title": "Understand DIC",
    "section": "",
    "text": "This mini-study aims to understand how the DIC (deviance information criterion) index works.\nThe common idea of an information criterion is \\(D + 2pD\\). The \\(D\\) (deviance) can also be presented as \\(-2\\) log-likelihood. Besides, a version of pD (effective number of parameters) of DIC (as same as JAGS program) is defined as the variance of log-likelihood.\nCode\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading iris data set.\nfrom sklearn.datasets import load_iris\niris = load_iris()"
  },
  {
    "objectID": "posts/dic_study_2.html#goals.",
    "href": "posts/dic_study_2.html#goals.",
    "title": "Understand DIC",
    "section": "Goals.",
    "text": "Goals.\né€™å€‹ç ”ç©¶å°±ç°¡å–®æ‹¿ iris è³‡æ–™é›†ä¾†æ¸¬è©¦ã€‚ æˆ‘å€‘çŸ¥é“ iris æœ‰ 4 å€‹ features: èŠ±è¼é•·åº¦ (Sepal.Length), èŠ±è¼å¯¬åº¦ (Sepal.Width), èŠ±ç“£é•·åº¦ (Petal.Length), èŠ±ç“£å¯¬åº¦ (Petal.Width)ã€‚\næˆ‘å€‘ä»Šå¤©å°±ç°¡å–®ç”¨ Sepal.Length ~ Sepal.Width é€™å€‹æ¨¡å¼ä¾†çœ‹çœ‹ DIC æ€éº¼ç®— æ­¤å¤–ï¼Œç‚ºäº†å¢åŠ ä¸€é»åƒæ•¸ï¼Œæˆ‘å€‘å†ä½¿ç”¨ 3 å€‹ targetï¼Œå»ºç«‹éšå±¤ç·šæ€§æ¨¡å¼ã€‚\nå› æ­¤ï¼Œæ¨¡å¼å¦‚ä¸‹ï¼š\n\\[\n\\begin{align}\n\\text{Likelihood:} \\\\\n\\text{Length} &\\sim N(\\mu _w, \\sigma^2) \\\\\n\\mu _w &= \\beta _0 + \\beta _{1i} \\text{Width} \\\\\n\\\\\n\\text{Priors:} \\\\\n\\beta _0, \\beta _{1i} &\\sim N(0,5) \\\\\n\\sigma &\\sim \\text{Exp}(1)\n\\end{align}\n\\]\né€™é‚Šçš„ \\(\\beta _{1i}\\) æ˜¯æ¯ä¸€å€‹ level å°æ‡‰çš„åƒæ•¸ã€‚æ‰€ä»¥æ‡‰è©²æœƒæœ‰ 3 å€‹ã€‚"
  },
  {
    "objectID": "posts/dic_study_2.html#jags",
    "href": "posts/dic_study_2.html#jags",
    "title": "Understand DIC",
    "section": "jags",
    "text": "jags\nLetâ€™s see how to run this model in jags.\nFirstly, we call the iris data set (from R default {datasets})\ndata(iris)\nSecondly, we define the data list and model string in the {R2jags} package. The {R2jags} allows users to write a jags model just like an R function.\n\nThe data list.\n\ndat_list = list(\n  sepal_length = iris$Sepal.Length,\n  sepal_width = iris$Sepal.Width,\n  species = iris$Species,\n  n = 150\n)\n\nThe model string.\n\nmod_string &lt;- \\(){\n  ## priors\n  beta0 ~ dnorm(0,1/5^2)\n  sigma ~ dexp(1)\n  for (j in 1:3){\n    beta1[j] ~ dnorm(0,1/5^2)\n  }\n  \n  ## likelihood\n  for (i in 1:n){\n    mu_w[i] &lt;- beta0 + beta1[species[i]] * sepal_width[i]\n    sepal_length[i] ~ dnorm(mu_w[i], 1/sigma^2) \n  }\n}\nFinally, we run this model through the jags function.\nfit &lt;- jags(data = dat_list, \n     parameters.to.save = c('beta0','beta1','sigma'),\n     model.file = (mod_string)\n     )\nThen, the output of this jags model is shown as follows:\n&gt; print(fit, digits = 3)\nInference for Bugs model at \"/var/folders/1f/8r50hwmn6m5dwrngfgysq4p40000gn/T//Rtmpbvfmga/modelab6b34cc72fd.txt\", fit using jags,\n 3 chains, each with 2000 iterations (first 1000 discarded)\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nbeta0      3.338   0.333   2.680   3.117   3.336   3.559   3.981 1.001  2400\nbeta1[1]   0.488   0.098   0.298   0.424   0.490   0.552   0.685 1.002  1700\nbeta1[2]   0.938   0.120   0.700   0.856   0.938   1.019   1.175 1.001  2400\nbeta1[3]   1.091   0.113   0.870   1.015   1.091   1.168   1.310 1.002  1900\nsigma      0.444   0.026   0.397   0.426   0.444   0.461   0.496 1.002  1600\ndeviance 180.851   3.199 176.629 178.531 180.100 182.442 188.810 1.002  1400\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts/dic_study_2.html#pymc",
    "href": "posts/dic_study_2.html#pymc",
    "title": "Understand DIC",
    "section": "pymc",
    "text": "pymc\nNow, we use the sync to replicate these results. We are interested in two things,\n\nRQ1. to compare the parameters of beta0, beta1, and sigma.\nRQ2. to compute the (expected) deviance, pD, and DIC.\n\n\n\nCode\niris_data = pd.DataFrame(iris['data'])\niris_data.columns = iris['feature_names']\niris_data\n\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows Ã— 4 columns\n\n\n\n\n\n\nCode\n#seed=1234\n\ntarget_index, target = pd.Series(iris['target']).factorize()\n#width_index, width = iris_data[1].factorize()\n\n\ndict = {\n    'target': iris['target_names'], \n    'target_index': target_index,\n    #'width_index': width_index\n}\ndict\n\n\n{'target': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'target_index': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}\n\n\n\nRQ1. Compare jags and pymc\n\n\nCode\nwith pm.Model(coords=dict) as iris_model:   \n    ## data\n    sepal_length = pm.Data('sepal_length', iris_data['sepal length (cm)'])\n    sepal_width = pm.Data('sepal_width', iris_data['sepal width (cm)'])\n    \n\n    ## priors\n    beta0 = pm.Normal('Î²0', 0,5)\n    beta1 = pm.Normal('Î²1', 0,5, shape=3)\n    sigma = pm.Exponential('Ïƒ',1)\n\n    ## likelihood\n    mu_w = beta0 + beta1[target_index] * sepal_width\n    Length = pm.Normal('length', mu_w, sigma, observed=sepal_length)\n\n    ## sampling\n    iris_post = pm.sample( draws=3000, chains=4, cores=4) \n    pm.compute_log_likelihood(iris_post)\n    #ra_4pl_predict = pm.sample_posterior_predictive(ra_4pl_post)\n\n\n/Users/garden/Library/Python/3.9/lib/python/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Î²0, Î²1, Ïƒ]\nSampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [16000/16000 00:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(iris_post)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nÎ²0\n3.344\n0.329\n2.739\n3.968\n0.007\n0.005\n2393.0\n2772.0\n1.0\n\n\nÎ²1[0]\n0.487\n0.097\n0.296\n0.657\n0.002\n0.001\n2425.0\n2847.0\n1.0\n\n\nÎ²1[1]\n0.935\n0.119\n0.713\n1.159\n0.002\n0.002\n2469.0\n2887.0\n1.0\n\n\nÎ²1[2]\n1.089\n0.111\n0.877\n1.294\n0.002\n0.002\n2421.0\n2802.0\n1.0\n\n\nÏƒ\n0.444\n0.026\n0.395\n0.492\n0.000\n0.000\n4241.0\n3826.0\n1.0\n\n\n\n\n\n\n\n\nConcluding remarks. For the RQ1, the outputs from jags and pymc show no significant differences.\n\n\nRQ2. Computing DIC.\nFirstly, letâ€™s see the data structure of log_likelihood from the pm.compute_log_likelihood() function. Itâ€™s a three-way dimensions tensor. The first dim is for (4) chains, the second for (3000) draws, and the third for length of data (150).\n\n\nCode\niris_post.log_likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 3000, length_dim_0: 150)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 ... 2994 2995 2996 2997 2998 2999\n  * length_dim_0  (length_dim_0) int64 0 1 2 3 4 5 6 ... 144 145 146 147 148 149\nData variables:\n    length        (chain, draw, length_dim_0) float64 -0.1501 -0.2302 ... -1.553\nAttributes:\n    created_at:                 2024-03-27T11:15:05.525056\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.0xarray.DatasetDimensions:chain: 4draw: 3000length_dim_0: 150Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])length_dim_0(length_dim_0)int640 1 2 3 4 5 ... 145 146 147 148 149array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149])Data variables: (1)length(chain, draw, length_dim_0)float64-0.1501 -0.2302 ... -1.878 -1.553array([[[-0.15007587, -0.23018022, -0.17186102, ..., -0.15036302,\n         -2.03497421, -1.25868256],\n        [-0.14763492, -0.18836443, -0.24012757, ..., -0.15958522,\n         -1.93430897, -1.20251279],\n        [-0.13546664, -0.24928963, -0.11306264, ..., -0.12683694,\n         -2.32605535, -1.40839025],\n        ...,\n        [-0.14045847, -0.13434047, -0.13613556, ..., -0.14624427,\n         -2.03811634, -1.58747839],\n        [-0.15619394, -0.1480345 , -0.15840489, ..., -0.16924811,\n         -1.99564914, -1.57251438],\n        [-0.06604883, -0.06365481, -0.3631115 , ..., -0.06431746,\n         -1.54910513, -1.23877687]],\n\n       [[-0.2091668 , -0.22398206, -0.26267452, ..., -0.21925268,\n         -1.68602165, -1.25524037],\n        [-0.0207    , -0.00833144, -0.18972243, ..., -0.03910661,\n         -1.76095645, -1.53025309],\n        [-0.07599954, -0.0308677 , -0.08252084, ..., -0.02220397,\n         -1.67739141, -1.4524553 ],\n...\n        [-0.21175487, -0.3262845 , -0.14786837, ..., -0.20980681,\n         -2.44356451, -1.53501204],\n        [-0.10045583, -0.14453996, -0.20253765, ..., -0.12437101,\n         -2.18738956, -1.34751415],\n        [-0.06133903, -0.05676096, -0.27590568, ..., -0.06310933,\n         -1.92064681, -1.27690886]],\n\n       [[-0.12362904, -0.17491691, -0.13462518, ..., -0.09471016,\n         -1.73004549, -1.14761551],\n        [-0.04212361, -0.05487806, -0.19423224, ..., -0.10022129,\n         -2.30313771, -1.63956094],\n        [-0.04604174, -0.07543161, -0.13592791, ..., -0.06340857,\n         -2.12692774, -1.46189584],\n        ...,\n        [-0.10221615, -0.10216825, -0.2916557 , ..., -0.12732807,\n         -1.68477784, -1.35505866],\n        [-0.08520427, -0.07523728, -0.19593031, ..., -0.12138973,\n         -1.90040781, -1.57051397],\n        [-0.07718419, -0.07102337, -0.22110662, ..., -0.11846029,\n         -1.87751136, -1.5528609 ]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))length_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n      dtype='int64', name='length_dim_0', length=150))Attributes: (4)created_at :2024-03-27T11:15:05.525056arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.0\n\n\nSecondly, letâ€™s try to compute the expected deviance (D) from this tensor. Due to the output from jags, we know the correct answer will be close to 180.851.\nNow, we need to compute the D (-2ll) for each point (there are a total of 150 points in this study\nTips. To sum up the dim we are interested in. In this case, we sum up the dim of length_dim_0 (axis=2). Then we can get 4*3000 draws for each points.\n\n\nCode\ny_ll = iris_post.log_likelihood['length'].sum(axis=2)\ny_deviance = -2*y_ll\ny_deviance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' (chain: 4, draw: 3000)&gt;\narray([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 2993 2994 2995 2996 2997 2998 2999xarray.DataArray'length'chain: 4draw: 3000180.1 180.2 181.5 181.0 181.9 183.1 ... 178.5 178.5 178.7 179.0 178.3array([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))Attributes: (0)\n\n\nThen get the posterior mean of it. It is 180.85.\n\n\nCode\ny_deviance.mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(180.83823109)xarray.DataArray'length'180.8array(180.83823109)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nThirdly, we need to compute the pD. We konw the pD will be close to 5.1.\n\n\nCode\ny_deviance.var()/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(5.37651436)xarray.DataArray'length'5.377array(5.37651436)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nFinally, we can compute the DIC. It will be close to 186.0. There are two kind of mthods to compute it,\n\nUsing log-likelihood. -2*y_ll.mean() + 2*y_ll.var()\nUsing deviance. y_deviance.mean() + y_deviance.var()/2\n\n\n\nCode\nDIC = y_deviance.mean() + y_deviance.var()/2\nDIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(186.21474545)xarray.DataArray'length'186.2array(186.21474545)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nYes!! Bingo!!"
  },
  {
    "objectID": "posts/dic_study_2.html#the-easy-function.",
    "href": "posts/dic_study_2.html#the-easy-function.",
    "title": "Understand DIC",
    "section": "The easy function.",
    "text": "The easy function.\nFurthermore, we write a function to output the strings like the jags program.\nIt will look like,\nDIC info (using the rule, pD = var(deviance)/2)\ndeviance = 180.85, pD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCode\ndef get_dic(posterior_tensor, var_names):\n    y_ll = posterior_tensor.log_likelihood[var_names].sum(axis=2).to_numpy()\n    y_deviance = -2*y_ll.mean()\n    y_pd = 2*y_ll.var()\n    y_dic = y_deviance + y_pd\n\n    y_print =   'DIC info (using the rule, pD = var(deviance)/2) \\n' +\\\n                'mean deviance = {:.3f}, pD = {:.3f} and DIC = {:.3f} \\n'.format(y_deviance, y_pd, y_dic) +\\\n                'DIC is an estimate of expected predictive error (lower deviance is better).'\n            \n    return print(y_print)\n\n\n\n\nCode\nget_dic(iris_post, var_names='length')\n\n\nDIC info (using the rule, pD = var(deviance)/2) \nmean deviance = 180.838, pD = 5.377 and DIC = 186.215 \nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts_zh/bayes.html",
    "href": "posts_zh/bayes.html",
    "title": "è²æ°å­¸ç¿’è³‡æºæ¨è–¦",
    "section": "",
    "text": "CitationBibTeX citation:@online{untitled,\n  author = {, JT},\n  title = {è²æ°å­¸ç¿’è³‡æºæ¨è–¦},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJ. (n.d.). è²æ°å­¸ç¿’è³‡æºæ¨è–¦."
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html",
    "href": "posts_zh/journal_jedubehstats.html",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "",
    "text": "Journal Of Educational And Behavioral Statistics æ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸æœŸåˆŠ æ˜¯ American Educational Research Association, AERA ç¾åœ‹æ•™è‚²ç ”ç©¶å”æœƒçš„ä¸»è¦æœŸåˆŠã€‚ç”± Sage å‡ºç‰ˆã€‚ä¸€å¹´ 6 æœŸã€‚"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#ä¸»è¦è³‡è¨Š",
    "href": "posts_zh/journal_jedubehstats.html#ä¸»è¦è³‡è¨Š",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "ä¸»è¦è³‡è¨Š",
    "text": "ä¸»è¦è³‡è¨Š\n\næˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠçš„ä¸€äº›æœ‰è¶£æ•¸æ“šã€‚åœ¨ 2010-2023 å¹´ï¼Œé€™æœŸåˆŠå…±æœ‰ 467 ç¯‡æ–‡ç« ï¼Œå…¶ä¸­åˆè‘—ä½œè€…å¹³å‡ 2.33 ä½ã€‚æ¯å¹´æˆé•·ç‡ -9.1%ï¼Œæ–‡ç« å¹³å‡å¹´é½¡ 7.16 æ­²ï¼Œæ¯ç¯‡æ–‡ç« å¹³å‡è¢«å¼•ç”¨ 17.38 æ¬¡ã€‚é€™äº›æ•¸å­—èƒŒå¾Œåæ˜ äº†é€™å€‹é ˜åŸŸçš„ç ”ç©¶æ´»åŠ›å’Œå½±éŸ¿åŠ›ã€‚"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "href": "posts_zh/journal_jedubehstats.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "ç ”ç©¶è©±é¡Œåœ°åœ–",
    "text": "ç ”ç©¶è©±é¡Œåœ°åœ–\n\nã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠå¾ 2010 å¹´ä»¥ä¾†ï¼Œæœ€é—œéµçš„ 20+ ç¯‡æ–‡ç« ï¼Œå¤§è‡´å‘ˆç¾å‡º 6 æ¢ä¸»è¦çš„ç ”ç©¶è©±é¡Œã€‚ä»¥åŠè¨±å¤šå–®ç¨çš„é‡è¦æ–‡ç« ã€‚æˆ‘å€‘ä¹Ÿæä¾›æ–‡ç»ç¸®å¯«ï¼ˆå’Œä¸Šåœ–å°æ‡‰ï¼‰ï¼Œdoiï¼Œä»¥åŠä¸­æ–‡ç¿»è­¯åç¨±ã€‚æ–‡ç»æ’åºå’Œåœ–ä¸Šä¸€æ¨£ï¼Œç”±ä¸Šè‡³ä¸‹ã€‚æœ‰èˆˆè¶£çš„åŒå­¸å¯ä»¥ç”¨ doi å»æ‰¾åˆ°å°æ‡‰çš„æ–‡ç« ã€‚\n\nè©±é¡Œï¼‘ ç¼ºå¤±è³‡æ–™å’Œæ½›åœ¨è®Šé …ï¼ˆç´…è‰²ï¼Œæ¯”é‡ 23.5%ï¼‰\n\nCAI L, 2010, DOI 10.3102/1076998609353115 ç”¨æ–¼ç¢ºèªæ€§é …ç›®å› ç´ åˆ†æçš„ Metropolis-hastings Robbins-monro ç®—æ³• ğŸŒŸ æœ€å‡ºåœˆæ–‡ç« ï¼(GCS: 156)\nVON DAVIER M, 2010, DOI 10.3102/1076998609346970 æ½›åœ¨å›æ­¸é …ç›®åæ‡‰æ¨¡å‹çš„éš¨æ©Ÿé€¼è¿‘æ–¹æ³•\nSI YJ, 2013, DOI 10.3102/1076998613480394 å¤§è¦æ¨¡è©•ä¼°èª¿æŸ¥ä¸­ä¸å®Œæ•´åˆ†é¡è®Šé‡çš„éåƒæ•¸è²è‘‰æ–¯å¤šé‡ä¼°ç®—\nDRECHSLER J, 2015, DOI 10.3102/1076998614563393 å¤šå±¤æ¬¡ç¼ºå¤±æ•¸æ“šçš„å¤šé‡ä¼°ç®—â€“åš´è¬¹æ€§èˆ‡ç°¡ä¾¿æ€§çš„æ¯”è¼ƒ\n\n\n\nè©±é¡Œï¼’ è©¦é¡Œåæ‡‰ç†è«–ï¼ˆè—è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nJOHNSON TR, 2010, DOI 10.3102/1076998609340529 ä½¿ç”¨å› å­åˆ†æå¤šäºŒé …å°æ•¸é …ç›®åæ‡‰æ¨¡å‹ä¾†è€ƒæ…®åæ‡‰é¢¨æ ¼çš„å€‹é«”å·®ç•°\nTHISSEN-ROE A, 2013, DOI 10.3102/1076998613481500 å°å–œæ­¡é¡å‹é …ç›®çš„åæ‡‰çš„é›™æ±ºå®šæ¨¡å‹\n\n\n\nè©±é¡Œï¼“ åæ‡‰æ™‚é–“æ¨¡å¼ï¼ˆç¶ è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nFAN ZW, 2012, DOI 10.3102/1076998611422912 åˆ©ç”¨åæ‡‰æ™‚é–“åˆ†å¸ƒé€²è¡Œè²“çš„é …ç›®é¸æ“‡\nWANG C, 2013, DOI 10.3102/1076998612461831 åœ¨è¨ˆç®—æ©ŸåŒ–æ¸¬è©¦ä¸­è¯åˆåˆ†æåæ‡‰æ™‚é–“å’Œæº–ç¢ºæ€§çš„åŠåƒæ•¸æ¨¡å‹\n\n\n\nè©±é¡Œï¼” æ¸¬é©—åˆ†æ•¸æ‡‰ç”¨ï¼ˆç´«è‰²ï¼Œæ¯”é‡ 29.4%ï¼‰\n\nHO AD, 2012, DOI 10.3102/1076998611411918 å¾ â€œèƒ½åŠ›â€ç­‰ç´šå ±å‘Šçš„æ¸¬é©—åˆ†æ•¸ä¸­ä¼°è¨ˆæˆç¸¾å·®è·\nLOCKWOOD JR, 2014, DOI 10.3102/1076998613509405 åœ¨ç”¨æ–¼ä¼°è¨ˆæ²»ç™‚æ•ˆæœçš„ ancova æ¨¡å‹ä¸­ç³¾æ­£æ¸¬é©—åˆ†æ•¸æ¸¬é‡èª¤å·®\nLECKIE G, 2014, DOI 10.3102/1076998614546494 ç‚ºå…©ç´šæ¨¡å‹ä¸­çš„ç•°è³ªæ–¹å·®-å”æ–¹å·®æˆåˆ†å»ºæ¨¡\nREARDON SF, 2017, DOI 10.3102/1076998616666279 ä½¿ç”¨ç•°æ–¹å·®æœ‰åº probit æ¨¡å‹å¾ç²—ç•¥æ•¸æ“šä¸­æ¢è¦†é€£çºŒæ¸¬é©—åˆ†æ•¸åˆ†å¸ƒçš„çŸ©\nLOCKWOOD JR, 2018, DOI 10.3102/1076998618795124 åˆ©ç”¨éˆæ´»çš„è²è‘‰æ–¯æ¨¡å‹å¾ç²—ç•¥çš„ç¾¤é«”æ°´å¹³æˆç¸¾æ•¸æ“šä¸­é€²è¡Œæ¨æ–·\n\n\n\nè©±é¡Œï¼• è²æ°ç›¸é—œæŠ€è¡“ï¼ˆæ©˜è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nCULPEPPER SA, 2015, DOI 10.3102/1076998615595403 åˆ©ç”¨ Gibbs æŠ½æ¨£å° Dina æ¨¡å‹é€²è¡Œè²è‘‰æ–¯ä¼°è¨ˆ ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 11)\nWANG SY, 2018, DOI 10.3102/1076998617719727 åˆ©ç”¨èªçŸ¥è¨ºæ–·æ¨¡å‹è·Ÿè¹¤æŠ€èƒ½ç¿’å¾—ï¼š å¸¶æœ‰å”è®Šé‡çš„é«˜éšéš±é¦¬çˆ¾å¯å¤«æ¨¡å‹\n\n\n\nè©±é¡Œï¼– ä¸€äº›æª¢é©—æ–¹æ³•ï¼ˆç´…è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nROMERO M, 2015, DOI 10.3102/1076998615595628 ç­”æ¡ˆè¦†åˆ¶æŒ‡æ•¸çš„æœ€å„ªæ€§ï¼š ç†è«–èˆ‡å¯¦è¸\nSINHARAY S, 2017, DOI 10.3102/1076998616673872 åˆ©ç”¨ä¼¼ç„¶æ¯”æª¢é©—å’Œåˆ†æ•¸æª¢é©—æª¢æ¸¬é …ç›®é çŸ¥èƒ½åŠ›"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi,\nIâ€™m Jie-Wen Tsai.",
    "section": "",
    "text": "Psychometrics. CRAN Task View: Psychometric Models and Methods.\nBayesian data analysis. CRAN Task View: Bayesian Inference."
  },
  {
    "objectID": "index.html#education.",
    "href": "index.html#education.",
    "title": "Hi,\nIâ€™m Jie-Wen Tsai.",
    "section": "Education.",
    "text": "Education.\n\nEducation, National Chengchi University, Taiwan."
  },
  {
    "objectID": "index.html#interesting.",
    "href": "index.html#interesting.",
    "title": "Hi,\nIâ€™m Jie-Wen Tsai.",
    "section": "",
    "text": "Psychometrics. CRAN Task View: Psychometric Models and Methods.\nBayesian data analysis. CRAN Task View: Bayesian Inference."
  }
]