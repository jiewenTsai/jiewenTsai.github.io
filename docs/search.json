[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Softwares",
    "section": "",
    "text": "ExtendedRtIrtModeling.jl.  Extended Response Time Item Response Models with Polya-Gamma Sampler and Bayesian Quantile Regression. See more: Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR"
  },
  {
    "objectID": "software.html#julia",
    "href": "software.html#julia",
    "title": "Softwares",
    "section": "",
    "text": "ExtendedRtIrtModeling.jl.  Extended Response Time Item Response Models with Polya-Gamma Sampler and Bayesian Quantile Regression. See more: Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR"
  },
  {
    "objectID": "software.html#quarto",
    "href": "software.html#quarto",
    "title": "Softwares",
    "section": "Quarto",
    "text": "Quarto\n\nquarto-revealjs-yangchou"
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Softwares",
    "section": "R",
    "text": "R\n\nbibliometrixExtra.  R for synonyms (Extended functions for bibliometrix)."
  },
  {
    "objectID": "software.html#typst",
    "href": "software.html#typst",
    "title": "Softwares",
    "section": "Typst",
    "text": "Typst\n\n(â€¦)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jie-Wen Tsai.",
    "section": "",
    "text": "A ninja of educational measurement."
  },
  {
    "objectID": "index.html#see-also.",
    "href": "index.html#see-also.",
    "title": "Jie-Wen Tsai.",
    "section": "See Also.",
    "text": "See Also.\n\nNinja Notes."
  },
  {
    "objectID": "index.html#interesting.",
    "href": "index.html#interesting.",
    "title": "Jie-Wen Tsai.",
    "section": "Interesting.",
    "text": "Interesting.\n\nPsychometrics. CRAN Task View: Psychometric Models and Methods.\nBayesian data analysis. CRAN Task View: Bayesian Inference."
  },
  {
    "objectID": "index.html#education.",
    "href": "index.html#education.",
    "title": "Jie-Wen Tsai.",
    "section": "Education.",
    "text": "Education.\n\nEducation, National Chengchi University, Taiwan."
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html",
    "href": "posts_zh/journal_jedubehstats.html",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "",
    "text": "Journal Of Educational And Behavioral Statistics æ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸æœŸåˆŠ æ˜¯ American Educational Research Association, AERA ç¾åœ‹æ•™è‚²ç ”ç©¶å”æœƒçš„ä¸»è¦æœŸåˆŠã€‚ç”± Sage å‡ºç‰ˆã€‚ä¸€å¹´ 6 æœŸã€‚"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#ä¸»è¦è³‡è¨Š",
    "href": "posts_zh/journal_jedubehstats.html#ä¸»è¦è³‡è¨Š",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "ä¸»è¦è³‡è¨Š",
    "text": "ä¸»è¦è³‡è¨Š\n\næˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠçš„ä¸€äº›æœ‰è¶£æ•¸æ“šã€‚åœ¨ 2010-2023 å¹´ï¼Œé€™æœŸåˆŠå…±æœ‰ 467 ç¯‡æ–‡ç« ï¼Œå…¶ä¸­åˆè‘—ä½œè€…å¹³å‡ 2.33 ä½ã€‚æ¯å¹´æˆé•·ç‡ -9.1%ï¼Œæ–‡ç« å¹³å‡å¹´é½¡ 7.16 æ­²ï¼Œæ¯ç¯‡æ–‡ç« å¹³å‡è¢«å¼•ç”¨ 17.38 æ¬¡ã€‚é€™äº›æ•¸å­—èƒŒå¾Œåæ˜ äº†é€™å€‹é ˜åŸŸçš„ç ”ç©¶æ´»åŠ›å’Œå½±éŸ¿åŠ›ã€‚"
  },
  {
    "objectID": "posts_zh/journal_jedubehstats.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "href": "posts_zh/journal_jedubehstats.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "title": "ã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰",
    "section": "ç ”ç©¶è©±é¡Œåœ°åœ–",
    "text": "ç ”ç©¶è©±é¡Œåœ°åœ–\n\nã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠå¾ 2010 å¹´ä»¥ä¾†ï¼Œæœ€é—œéµçš„ 20+ ç¯‡æ–‡ç« ï¼Œå¤§è‡´å‘ˆç¾å‡º 6 æ¢ä¸»è¦çš„ç ”ç©¶è©±é¡Œã€‚ä»¥åŠè¨±å¤šå–®ç¨çš„é‡è¦æ–‡ç« ã€‚æˆ‘å€‘ä¹Ÿæä¾›æ–‡ç»ç¸®å¯«ï¼ˆå’Œä¸Šåœ–å°æ‡‰ï¼‰ï¼Œdoiï¼Œä»¥åŠä¸­æ–‡ç¿»è­¯åç¨±ã€‚æ–‡ç»æ’åºå’Œåœ–ä¸Šä¸€æ¨£ï¼Œç”±ä¸Šè‡³ä¸‹ã€‚æœ‰èˆˆè¶£çš„åŒå­¸å¯ä»¥ç”¨ doi å»æ‰¾åˆ°å°æ‡‰çš„æ–‡ç« ã€‚\n\nè©±é¡Œï¼‘ ç¼ºå¤±è³‡æ–™å’Œæ½›åœ¨è®Šé …ï¼ˆç´…è‰²ï¼Œæ¯”é‡ 23.5%ï¼‰\n\nCAI L, 2010, DOI 10.3102/1076998609353115 ç”¨æ–¼ç¢ºèªæ€§é …ç›®å› ç´ åˆ†æçš„ Metropolis-hastings Robbins-monro ç®—æ³• ğŸŒŸ æœ€å‡ºåœˆæ–‡ç« ï¼(GCS: 156)\nVON DAVIER M, 2010, DOI 10.3102/1076998609346970 æ½›åœ¨å›æ­¸é …ç›®åæ‡‰æ¨¡å‹çš„éš¨æ©Ÿé€¼è¿‘æ–¹æ³•\nSI YJ, 2013, DOI 10.3102/1076998613480394 å¤§è¦æ¨¡è©•ä¼°èª¿æŸ¥ä¸­ä¸å®Œæ•´åˆ†é¡è®Šé‡çš„éåƒæ•¸è²è‘‰æ–¯å¤šé‡ä¼°ç®—\nDRECHSLER J, 2015, DOI 10.3102/1076998614563393 å¤šå±¤æ¬¡ç¼ºå¤±æ•¸æ“šçš„å¤šé‡ä¼°ç®—â€“åš´è¬¹æ€§èˆ‡ç°¡ä¾¿æ€§çš„æ¯”è¼ƒ\n\n\n\nè©±é¡Œï¼’ è©¦é¡Œåæ‡‰ç†è«–ï¼ˆè—è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nJOHNSON TR, 2010, DOI 10.3102/1076998609340529 ä½¿ç”¨å› å­åˆ†æå¤šäºŒé …å°æ•¸é …ç›®åæ‡‰æ¨¡å‹ä¾†è€ƒæ…®åæ‡‰é¢¨æ ¼çš„å€‹é«”å·®ç•°\nTHISSEN-ROE A, 2013, DOI 10.3102/1076998613481500 å°å–œæ­¡é¡å‹é …ç›®çš„åæ‡‰çš„é›™æ±ºå®šæ¨¡å‹\n\n\n\nè©±é¡Œï¼“ åæ‡‰æ™‚é–“æ¨¡å¼ï¼ˆç¶ è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nFAN ZW, 2012, DOI 10.3102/1076998611422912 åˆ©ç”¨åæ‡‰æ™‚é–“åˆ†å¸ƒé€²è¡Œè²“çš„é …ç›®é¸æ“‡\nWANG C, 2013, DOI 10.3102/1076998612461831 åœ¨è¨ˆç®—æ©ŸåŒ–æ¸¬è©¦ä¸­è¯åˆåˆ†æåæ‡‰æ™‚é–“å’Œæº–ç¢ºæ€§çš„åŠåƒæ•¸æ¨¡å‹\n\n\n\nè©±é¡Œï¼” æ¸¬é©—åˆ†æ•¸æ‡‰ç”¨ï¼ˆç´«è‰²ï¼Œæ¯”é‡ 29.4%ï¼‰\n\nHO AD, 2012, DOI 10.3102/1076998611411918 å¾ â€œèƒ½åŠ›â€ç­‰ç´šå ±å‘Šçš„æ¸¬é©—åˆ†æ•¸ä¸­ä¼°è¨ˆæˆç¸¾å·®è·\nLOCKWOOD JR, 2014, DOI 10.3102/1076998613509405 åœ¨ç”¨æ–¼ä¼°è¨ˆæ²»ç™‚æ•ˆæœçš„ ancova æ¨¡å‹ä¸­ç³¾æ­£æ¸¬é©—åˆ†æ•¸æ¸¬é‡èª¤å·®\nLECKIE G, 2014, DOI 10.3102/1076998614546494 ç‚ºå…©ç´šæ¨¡å‹ä¸­çš„ç•°è³ªæ–¹å·®-å”æ–¹å·®æˆåˆ†å»ºæ¨¡\nREARDON SF, 2017, DOI 10.3102/1076998616666279 ä½¿ç”¨ç•°æ–¹å·®æœ‰åº probit æ¨¡å‹å¾ç²—ç•¥æ•¸æ“šä¸­æ¢è¦†é€£çºŒæ¸¬é©—åˆ†æ•¸åˆ†å¸ƒçš„çŸ©\nLOCKWOOD JR, 2018, DOI 10.3102/1076998618795124 åˆ©ç”¨éˆæ´»çš„è²è‘‰æ–¯æ¨¡å‹å¾ç²—ç•¥çš„ç¾¤é«”æ°´å¹³æˆç¸¾æ•¸æ“šä¸­é€²è¡Œæ¨æ–·\n\n\n\nè©±é¡Œï¼• è²æ°ç›¸é—œæŠ€è¡“ï¼ˆæ©˜è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nCULPEPPER SA, 2015, DOI 10.3102/1076998615595403 åˆ©ç”¨ Gibbs æŠ½æ¨£å° Dina æ¨¡å‹é€²è¡Œè²è‘‰æ–¯ä¼°è¨ˆ ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 11)\nWANG SY, 2018, DOI 10.3102/1076998617719727 åˆ©ç”¨èªçŸ¥è¨ºæ–·æ¨¡å‹è·Ÿè¹¤æŠ€èƒ½ç¿’å¾—ï¼š å¸¶æœ‰å”è®Šé‡çš„é«˜éšéš±é¦¬çˆ¾å¯å¤«æ¨¡å‹\n\n\n\nè©±é¡Œï¼– ä¸€äº›æª¢é©—æ–¹æ³•ï¼ˆç´…è‰²ï¼Œæ¯”é‡ 11.7%ï¼‰\n\nROMERO M, 2015, DOI 10.3102/1076998615595628 ç­”æ¡ˆè¦†åˆ¶æŒ‡æ•¸çš„æœ€å„ªæ€§ï¼š ç†è«–èˆ‡å¯¦è¸\nSINHARAY S, 2017, DOI 10.3102/1076998616673872 åˆ©ç”¨ä¼¼ç„¶æ¯”æª¢é©—å’Œåˆ†æ•¸æª¢é©—æª¢æ¸¬é …ç›®é çŸ¥èƒ½åŠ›"
  },
  {
    "objectID": "posts_zh/bayes.html",
    "href": "posts_zh/bayes.html",
    "title": "è²æ°å­¸ç¿’è³‡æºæ¨è–¦",
    "section": "",
    "text": "CitationBibTeX citation:@online{tsai,\n  author = {TSAI, JW},\n  title = {è²æ°å­¸ç¿’è³‡æºæ¨è–¦},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTSAI, J. (n.d.). è²æ°å­¸ç¿’è³‡æºæ¨è–¦."
  },
  {
    "objectID": "posts/note0716.html",
    "href": "posts/note0716.html",
    "title": "Note0716 (mediation analysis with pymc)",
    "section": "",
    "text": "Code\nimport pymc as pm\nimport numpy as np\nimport pandas as pd\nimport arviz as az\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\n#dat = pd.read_csv('data1463_fin3.csv')\n\n\nç‚ºäº†ä¹‹å¾Œé‚„å¯ä»¥è·‘é€™å€‹æ¨¡å‹ï¼Œæ ¹æ“šåŸæœ¬çš„è³‡æ–™é‡æ–°æ¨¡æ“¬ä¸€ç­†è³‡æ–™ã€‚\n(ä½†é€™é‚Šæ²’è€ƒæ…®åˆ°åŸæœ¬æ•¸å€¼ä¹‹é–“çš„ç›¸é—œæƒ…å½¢) å·²è€ƒæ…®é€²å»ï¼ï¼ç”¨å¤šå…ƒå¸¸æ…‹åˆ†é…æ¨¡æ“¬äº†ã€‚\n\nacd1eap \\(\\sim N(2.8510193121856177e-05, 0.8921482479092293)\\)\nscleap \\(\\sim N(1.9250494837216173e-06, 0.7154395945457549)\\)\nc1 \\(\\sim Ber(1,0.19822282980177716)\\)\nc2 \\(\\sim Ber(1,0.11483253588516747)\\)\nc3 \\(\\sim Ber(1,0.11551606288448393)\\)\n\n\n\nCode\n'''\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\n'''\n\n\n'\\n[dat.acd1_eap.mean(), dat.scl_eap.mean(),dat.c1.mean(),dat.c2.mean(),dat.c3.mean()]\\nnp.cov([dat.acd1_eap, dat.scl_eap, dat.c1, dat.c2, dat.c3])\\n'\n\n\n\n\nCode\ndat_mn = np.array(\n    [2.8510193121856177e-05,\n     1.9250494837216173e-06,\n     0.19822282980177716,\n     0.11483253588516747,\n     0.11551606288448393]\n)\ndat_cov = np.array([\n    [ 0.7959285 ,  0.16304568, -0.01541732, -0.01689872, -0.02973076],\n    [ 0.16304568,  0.51185381, -0.05390401, -0.00531492, -0.02327021],\n    [-0.01541732, -0.05390401,  0.15903925, -0.022778  , -0.02291358],\n    [-0.01689872, -0.00531492, -0.022778  ,  0.10171555, -0.01327408],\n    [-0.02973076, -0.02327021, -0.02291358, -0.01327408,  0.10224199]\n])\ndat = np.random.multivariate_normal(dat_mn, dat_cov, 1000)\ndat = pd.DataFrame(dat, columns=['acd1_eap', 'scl_eap', 'c1', 'c2', 'c3'])\ndat['c1'] = dat['c1'] &gt; 0.5\ndat['c2'] = dat['c2'] &gt; 0.5\ndat['c3'] = dat['c3'] &gt; 0.5\n\n\n\n\nCode\ndat\n\n\n\n\n\n\n\n\n\n\nacd1_eap\nscl_eap\nc1\nc2\nc3\n\n\n\n\n0\n0.346316\n-0.792151\nTrue\nFalse\nFalse\n\n\n1\n-0.305374\n1.621382\nFalse\nTrue\nFalse\n\n\n2\n0.563012\n0.828286\nFalse\nFalse\nFalse\n\n\n3\n0.058392\n-0.276411\nFalse\nFalse\nFalse\n\n\n4\n0.181640\n-1.291429\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\n0.717853\n-0.833434\nTrue\nFalse\nFalse\n\n\n996\n-0.812274\n-0.038150\nFalse\nFalse\nFalse\n\n\n997\n-0.461906\n-0.635552\nFalse\nFalse\nFalse\n\n\n998\n0.294100\n0.594206\nFalse\nFalse\nFalse\n\n\n999\n-0.543131\n-0.928300\nTrue\nFalse\nFalse\n\n\n\n\n1000 rows Ã— 5 columns\n\n\n\n\n\n\nCode\n'''\ndat_dict = {\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \n}\ndat = pd.DataFrame(dat_dict)\n'''\n\n\n\"\\ndat_dict = {\\n    'acd1_eap': np.random.normal(loc=2.8510193121856177e-05, scale=0.8921482479092293, size=1000),\\n    'scl_eap': np.random.normal(loc=1.9250494837216173e-06, scale=0.7154395945457549, size=1000),\\n    'c1': np.random.binomial(n=1, p=0.19822282980177716, size=1000),\\n    'c2': np.random.binomial(n=1, p=0.11483253588516747, size=1000),\\n    'c3': np.random.binomial(n=1, p=0.11551606288448393, size=1000),    \\n}\\ndat = pd.DataFrame(dat_dict)\\n\"\n\n\n\n\nCode\nwith pm.Model() as model:\n    acd1eap = pm.ConstantData('acd1eap', dat.acd1_eap)\n    scleap = pm.ConstantData('scleap', dat.scl_eap)\n    c1 = pm.ConstantData('c1', dat.c1)\n    c2 = pm.ConstantData('c2', dat.c2)\n    c3 = pm.ConstantData('c3', dat.c3)\n\n    # intercept\n    acd1eap_Intercept = pm.Normal('acd1eap_Intercept', mu=0, sigma=100)\n    scleap_Intercept = pm.Normal('scleap_Intercept', mu=0, sigma=100)\n    \n    # noise\n    acd1eap_Sigma = pm.HalfCauchy(\"acd1eap_Sigma\", 1)\n    scleap_Sigma = pm.HalfCauchy(\"scleap_Sigma\", 1)\n\n    # slope\n    acd1eap_scleap = pm.Normal('acd1eap_scleap', mu=0, sigma=100)\n    acd1eap_c1 = pm.Normal('acd1eap_c1', mu=0, sigma=100)\n    acd1eap_c2 = pm.Normal('acd1eap_c2', mu=0, sigma=100)\n    acd1eap_c3 = pm.Normal('acd1eap_c3', mu=0, sigma=100)\n    scleap_c1 = pm.Normal('scleap_c1', mu=0, sigma=100)\n    scleap_c2 = pm.Normal('scleap_c2', mu=0, sigma=100)\n    scleap_c3 = pm.Normal('scleap_c3', mu=0, sigma=100)\n\n    # likelihood\n    pm.Normal(\"y_likelihood\", mu=acd1eap_Intercept + acd1eap_scleap * scleap  + acd1eap_c1 * c1 + acd1eap_c2 * c2 + acd1eap_c3 * c3, sigma =  acd1eap_Sigma, observed = acd1eap  )\n    pm.Normal('m_likelihood', mu=scleap_Intercept + scleap_c1 * c1 + scleap_c2 * c2 + scleap_c3 * c3, sigma = scleap_Sigma, observed = scleap)\n    \n    trace_med = pm.sample(2000, chains=4, cores=4)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [acd1eap_Intercept, scleap_Intercept, acd1eap_Sigma, scleap_Sigma, acd1eap_scleap, acd1eap_c1, acd1eap_c2, acd1eap_c3, scleap_c1, scleap_c2, scleap_c3]\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 3 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\nCode\npm.model_to_graphviz(model)\n\n\nExecutableNotFound: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH\n\n\n&lt;graphviz.graphs.Digraph at 0x17209fb80&gt;\n\n\n\n\nCode\naz.summary(trace_med)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nacd1eap_Intercept\n0.042\n0.036\n-0.025\n0.109\n0.000\n0.000\n9687.0\n6732.0\n1.0\n\n\nscleap_Intercept\n0.045\n0.030\n-0.013\n0.102\n0.000\n0.000\n10276.0\n6795.0\n1.0\n\n\nacd1eap_scleap\n0.374\n0.038\n0.305\n0.445\n0.000\n0.000\n13151.0\n6209.0\n1.0\n\n\nacd1eap_c1\n0.002\n0.067\n-0.126\n0.122\n0.001\n0.001\n11284.0\n6502.0\n1.0\n\n\nacd1eap_c2\n-0.008\n0.085\n-0.169\n0.148\n0.001\n0.001\n12240.0\n6047.0\n1.0\n\n\nacd1eap_c3\n-0.049\n0.088\n-0.210\n0.120\n0.001\n0.001\n12138.0\n5986.0\n1.0\n\n\nscleap_c1\n-0.181\n0.055\n-0.285\n-0.081\n0.001\n0.000\n11615.0\n6883.0\n1.0\n\n\nscleap_c2\n0.031\n0.071\n-0.091\n0.173\n0.001\n0.001\n11680.0\n7188.0\n1.0\n\n\nscleap_c3\n-0.076\n0.073\n-0.225\n0.053\n0.001\n0.001\n12466.0\n6045.0\n1.0\n\n\nacd1eap_Sigma\n0.881\n0.020\n0.845\n0.919\n0.000\n0.000\n14248.0\n6463.0\n1.0\n\n\nscleap_Sigma\n0.734\n0.017\n0.703\n0.765\n0.000\n0.000\n12699.0\n6039.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{tsai2023,\n  author = {Tsai, JW},\n  title = {Note0716 (Mediation Analysis with Pymc)},\n  date = {2023-07-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nTsai, J. (2023, July 16). Note0716 (mediation analysis with\npymc)."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html",
    "href": "posts/IntroJuliaConnectoR.html",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "",
    "text": "Iâ€™ve just updated my Julia package, ExtendedRtIrtModeling.jl, to version 0.2.0. There are a few new features in there that Iâ€™ll run through in the next few sections.\nBut thatâ€™s not all! If youâ€™re an R user, Iâ€™ll introduce you to an R package called JuliaConnectoR that lets you run Julia programs in R. Itâ€™ll bridge the two languages seamlessly."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#using-in-julia",
    "href": "posts/IntroJuliaConnectoR.html#using-in-julia",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "Using in Julia",
    "text": "Using in Julia\nSee the Github page."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#using-in-r",
    "href": "posts/IntroJuliaConnectoR.html#using-in-r",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "Using in R",
    "text": "Using in R\nAll you have to do is to install and library the JuliaConnectoR as usual, and then you can use the juliaImport function to import any Julia package. It seems like the packageâ€™s version you get depends on which copy version youâ€™ve had on your computer (confirmed). The great thing is, itâ€™ll always download the newest version from Github, but not the stable one.\n\n\nCode\nlibrary(tidyverse)\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(JuliaConnectoR)\n\n\nWarning: package 'JuliaConnectoR' was built under R version 4.4.1\n\n\nIâ€™ve got a toy data set to test, but Iâ€™ll run through the demo anyway. As you can see from the data below, the data set includes 25 columns: one for ID, 10 for item responses, 10 for (log-)response time, and four for explanatory variables.\n\n\nCode\ndemo &lt;- read.csv('https://raw.githubusercontent.com/jiewenTsai/ExtendedRtIrtModeling.jl/refs/heads/main/data/demo.csv')\nhead(demo)\n\n\n    id y1 y2 y3 y4 y5 y6 y7 y8 y9 y10    t1    t2    t3    t4    t5    t6    t7\n1 s001  0  0  0  0  1  0  0  0  0   0 2.961 4.225 3.322 2.164 2.273 2.631 2.505\n2 s002  0  0  0  0  1  1  1  0  0   0 3.848 3.996 4.434 3.246 2.663 3.819 2.158\n3 s003  0  0  0  0  0  1  1  1  0   0 3.122 3.273 4.489 3.891 3.410 3.879 2.951\n4 s004  0  1  0  0  0  0  1  0  0   0 3.515 3.162 4.151 3.371 2.885 3.026 1.439\n5 s005  0  0  0  0  0  0  1  0  1   0 3.060 3.962 4.058 3.696 2.732 2.560 2.517\n6 s006  0  0  0  0  1  0  0  0  0   0 3.546 3.360 3.382 3.262 1.931 1.629 1.463\n     t8    t9   t10 x1     x2     x3    x4\n1 3.924 2.198 2.878  0 -3.362 -0.200 1.063\n2 2.848 2.492 4.038  1 -0.081  2.347 1.063\n3 3.891 3.877 3.553  0 -0.829 -1.068 1.063\n4 3.243 2.694 3.603  1 -0.829 -0.676 1.063\n5 3.339 3.421 3.314  0 -0.829  0.097 1.063\n6 2.475 2.373 2.909  1 -1.251 -3.096 1.063\n\n\nCode\nglimpse(demo)\n\n\nRows: 300\nColumns: 25\n$ id  &lt;chr&gt; \"s001\", \"s002\", \"s003\", \"s004\", \"s005\", \"s006\", \"s007\", \"s008\", \"sâ€¦\n$ y1  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, â€¦\n$ y2  &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ y3  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ y4  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ y5  &lt;int&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, â€¦\n$ y6  &lt;int&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, â€¦\n$ y7  &lt;int&gt; 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, â€¦\n$ y8  &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, â€¦\n$ y9  &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, â€¦\n$ y10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ t1  &lt;dbl&gt; 2.961, 3.848, 3.122, 3.515, 3.060, 3.546, 2.579, 3.854, 4.516, 2.9â€¦\n$ t2  &lt;dbl&gt; 4.225, 3.996, 3.273, 3.162, 3.962, 3.360, 3.448, 3.451, 4.943, 4.1â€¦\n$ t3  &lt;dbl&gt; 3.322, 4.434, 4.489, 4.151, 4.058, 3.382, 3.753, 2.875, 2.945, 5.0â€¦\n$ t4  &lt;dbl&gt; 2.164, 3.246, 3.891, 3.371, 3.696, 3.262, 3.575, 3.634, 4.444, 4.0â€¦\n$ t5  &lt;dbl&gt; 2.273, 2.663, 3.410, 2.885, 2.732, 1.931, 2.846, 2.560, 2.718, 2.8â€¦\n$ t6  &lt;dbl&gt; 2.631, 3.819, 3.879, 3.026, 2.560, 1.629, 3.153, 2.499, 1.442, 3.3â€¦\n$ t7  &lt;dbl&gt; 2.505, 2.158, 2.951, 1.439, 2.517, 1.463, 3.370, 2.581, 1.666, 3.2â€¦\n$ t8  &lt;dbl&gt; 3.924, 2.848, 3.891, 3.243, 3.339, 2.475, 2.424, 2.124, 3.301, 3.3â€¦\n$ t9  &lt;dbl&gt; 2.198, 2.492, 3.877, 2.694, 3.421, 2.373, 2.039, 2.749, 1.962, 2.7â€¦\n$ t10 &lt;dbl&gt; 2.878, 4.038, 3.553, 3.603, 3.314, 2.909, 2.923, 2.366, 4.804, 3.6â€¦\n$ x1  &lt;int&gt; 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, â€¦\n$ x2  &lt;dbl&gt; -3.362, -0.081, -0.829, -0.829, -0.829, -1.251, 1.174, -0.829, -1.â€¦\n$ x3  &lt;dbl&gt; -0.200, 2.347, -1.068, -0.676, 0.097, -3.096, -1.944, -1.258, -1.4â€¦\n$ x4  &lt;dbl&gt; 1.063, 1.063, 1.063, 1.063, 1.063, 1.063, 0.104, -0.587, -2.002, -â€¦\n\n\nNext, letâ€™s take a look at how accuracy and speed related to each other, barely using the raw data (mean of y and mean of t) to get a rough idea.\n\n\nCode\ntibble(\n  accuracy = rowMeans(demo[2:11]),\n  speed = -rowMeans(demo[12:21])\n) |&gt;\n  ggplot(aes(x=accuracy, y=speed)) +\n  geom_point() +\n  geom_jitter() +\n  geom_smooth(method = \"loess\") \n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nIntroduction to ExtendedRtIrtModeling.jl through JuliaConnectoR\n\n\n2 min\n\n\n\nNov 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstand DIC\n\n\n6 min\n\n\n\nMar 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to conduct simple slope analysis and make plot with brms\n\n\n4 min\n\n\n\nMar 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNote0716 (mediation analysis with pymc)\n\n\n3 min\n\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notesZh.html",
    "href": "notesZh.html",
    "title": "ä¸­æ–‡è¨˜äº‹",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nè²æ°å­¸ç¿’è³‡æºæ¨è–¦\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\nChapter 10\n\n\n2 min\n\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰\n\n\n1 min\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nã€Œæ•™è‚²èˆ‡è¡Œç‚ºçµ±è¨ˆå­¸ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆJEducBehavStat 2010-2023ï¼‰\n\n\n1 min\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/note0322.html",
    "href": "posts/note0322.html",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "",
    "text": "Goal. In this note, we will demonstrate how to use the output from brms to make (simple slope) testings and plots."
  },
  {
    "objectID": "posts/note0322.html#make-data",
    "href": "posts/note0322.html#make-data",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make data",
    "text": "Make data\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(bayestestR)\nlibrary(rstan)\nlibrary(mvtnorm)\n\n\nNow we have to make a data set including 4 variables: Y, X, M, and W.\nSuppose these four variables follow a multivariate-normal distribution as follows,\nLet X is a treatment (binary data), and M is a response time data (lognormal).\n\\[\\begin{equation}\n\\begin{bmatrix}\nY \\\\ X \\\\M \\\\W\n\\end{bmatrix}\n= \\text{MVN}\\left(\n\\begin{bmatrix}\n0 \\\\0 \\\\ 0\\\\ 0\n\\end{bmatrix},\n\\begin{bmatrix}\n1 & 0.1 & -0.8 & 0.8  \\\\\n0.1 & 1 & -0.6 & 0\\\\\n-0.8 & -0.6 & 1 & 0.6\\\\\n0.8 & 0 & 0.6 & 1\n\\end{bmatrix}\n\\right)\n\\end{equation}\\]\n\n\nCode\nset.seed(12345)\nreal_sigma &lt;- matrix(c(1, 0.1, -0.8, 0.8,\n                      0.1, 1, -0.6, 0,\n                      -0.8, -0.6, 1, 0.6,\n                      0.8, 0, 0.6, 1), nrow = 4)\nreal_mean &lt;- c(0,0,0,0)\n\nreal_data &lt;- rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma)\n\n\nWarning in rmvnorm(n = 1000, mean = real_mean, sigma = real_sigma): sigma is\nnumerically not positive semidefinite\n\n\nCode\ndat &lt;- data.frame(\n  ID = paste0('s', str_pad(1:1000, width = 4, side = 'left', pad = 0)),\n  Y = real_data[,1],\n  X = real_data[,2] &gt; mean(real_data[,2]),\n  M = exp(real_data[,3]),\n  W = real_data[,4]\n)\n\nhead(dat)\n\n\n     ID          Y     X         M          W\n1 s0001  0.3617174  TRUE 0.4790760 -0.1639895\n2 s0002  0.1110080 FALSE 2.0490296  0.2046531\n3 s0003  0.6187169 FALSE 2.6247616  1.4401394\n4 s0004  1.0347476  TRUE 0.5084054  0.6434533\n5 s0005 -1.1477318 FALSE 4.8633851  0.2686688\n6 s0006  0.2802449  TRUE 0.1476185 -1.2412565"
  },
  {
    "objectID": "posts/note0322.html#fit-bayesian-model-in-brms",
    "href": "posts/note0322.html#fit-bayesian-model-in-brms",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Fit Bayesian model in brms",
    "text": "Fit Bayesian model in brms\nNow we specify the formula as follows (in Bayesian).\n\\[\\begin{align}\n\\text{Likelihood.}\\\\\nY &\\sim N(\\mu_y, \\sigma_y^2) \\\\\nM &\\sim \\log N(\\mu_m, \\sigma_m^2) \\\\\n\n\\mu_y &= \\beta_{01} + \\beta_x X + \\beta_m M + \\beta_w W + \\beta _{mw}M \\cdot W \\\\\n\\mu_m &= \\beta_{02} + \\beta_x X \\\\ \\\\\n\n\\text{Priors.}\\\\\n\n\\sigma_y^2, \\sigma_m^2 & \\sim \\text{Exp}(1) \\\\\n\\beta_{01}, ..., \\beta _{x} &\\sim N(0,5)\n\\end{align}\\]\n\n\nCode\nbf1 &lt;- bf(Y~X+M+W+M*W, family = gaussian())\nbf2 &lt;- bf(M~X, family = lognormal())\npriors &lt;- prior(normal(0,5), class = b, resp = Y) + \n  prior(normal(0,5), class = b, resp = M) + \n  prior(exponential(1), class = sigma, resp = Y) +\n  prior(exponential(1), class = sigma, resp = M) \n\n\n\nfit &lt;- brm(\n  bf1+bf2+set_rescor(FALSE), \n  data = dat,\n  cores = 4\n)\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\n\nCode\nprint(fit, digits = 3)\n\n\n Family: MV(gaussian, lognormal) \n  Links: mu = identity; sigma = identity\n         mu = identity; sigma = identity \nFormula: Y ~ X + M + W + M * W \n         M ~ X \n   Data: dat (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nY_Intercept    0.648     0.035    0.580    0.716 1.001     4338     3242\nM_Intercept    0.490     0.043    0.407    0.574 1.002     4917     3090\nY_XTRUE       -0.164     0.039   -0.241   -0.086 1.000     4874     3349\nY_M           -0.366     0.010   -0.386   -0.346 1.001     3476     3265\nY_W            0.604     0.020    0.565    0.641 1.000     4427     3384\nY_M:W          0.100     0.006    0.088    0.112 1.000     2928     2864\nM_XTRUE       -0.856     0.059   -0.970   -0.742 1.001     5208     2953\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma_Y    0.570     0.013    0.544    0.597 1.000     5277     2537\nsigma_M    0.962     0.021    0.921    1.005 1.002     5424     2974\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/note0322.html#bayesian-testing",
    "href": "posts/note0322.html#bayesian-testing",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Bayesian testing",
    "text": "Bayesian testing\n\n\nCode\nfit |&gt; \n  describe_posterior(\n    effects = \"all\",\n    component = \"all\",\n    #test = c(\"p_direction\", \"p_significance\"),\n    centrality = \"all\"\n  )\n\n\nWarning: Multivariate response models are not yet supported for tests `rope` and\n  `p_rope`.\n\n\nSummary of Posterior Distribution M\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        M |   0.49 |  0.49 |  0.49 | [ 0.41,  0.57] | 100% | 1.000 | 4898.00\nXTRUE       |        M |  -0.85 | -0.86 | -0.85 | [-0.97, -0.74] | 100% | 0.999 | 5258.00\n\n# Fixed effects sigma M\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        M |   0.96 | 0.96 | 0.96 | [ 0.92,  1.00] | 100% | 1.000 | 5345.00\n\n# Fixed effects Y\n\nParameter   | Response | Median |  Mean |   MAP |         95% CI |   pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept) |        Y |   0.65 |  0.65 |  0.65 | [ 0.58,  0.72] | 100% | 1.000 | 4325.00\nXTRUE       |        Y |  -0.16 | -0.16 | -0.16 | [-0.24, -0.09] | 100% | 1.000 | 4867.00\nM           |        Y |  -0.37 | -0.37 | -0.36 | [-0.39, -0.35] | 100% | 1.001 | 3499.00\nW           |        Y |   0.60 |  0.60 |  0.60 | [ 0.56,  0.64] | 100% | 1.000 | 4384.00\nM:W         |        Y |   0.10 |  0.10 |  0.10 | [ 0.09,  0.11] | 100% | 1.000 | 2908.00\n\n# Fixed effects sigma Y\n\nParameter | Response | Median | Mean |  MAP |         95% CI |   pd |  Rhat |     ESS\n-------------------------------------------------------------------------------------\nsigma     |        Y |   0.57 | 0.57 | 0.57 | [ 0.54,  0.60] | 100% | 1.000 | 5226.00\n\n\nThe function hypothesis() can be used to test specific parameter.\n\n\nCode\nfit_hypo &lt;- hypothesis(\n  fit, \n  class = 'b',\n  alpha = .05,\n  hypothesis = \n  c(\n    Low = \"Y_M - Y_M:W = 0\",\n    Medium = \"Y_M = 0\",\n    High = \"Y_M + Y_M:W = 0\")\n  ) \nfit_hypo\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1        Low    -0.47      0.02    -0.50    -0.44         NA        NA    *\n2     Medium    -0.37      0.01    -0.39    -0.35         NA        NA    *\n3       High    -0.27      0.01    -0.28    -0.25         NA        NA    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "posts/note0322.html#make-plots",
    "href": "posts/note0322.html#make-plots",
    "title": "How to conduct simple slope analysis and make plot with brms",
    "section": "Make plots",
    "text": "Make plots\n\n\nCode\n## plotting ----\ncond_plot &lt;- conditional_effects(fit)\n\n\n\n\nCode\ncond_plot$`Y.Y_M:W` |&gt;\n  ggplot(aes(x = M, y = Y), ) +\n  \n  geom_ribbon(aes(x = effect1__, y = estimate__, linetype = effect2__,\n                  ymin = lower__, ymax = upper__, fill = factor(effect2__)), alpha = 0.5) +\n  geom_line(aes(x = effect1__, y = estimate__, linetype = effect2__)) +\n  scale_fill_manual(name = 'W effects',\n                    values = c(\"coral4\", \"coral3\", \"coral2\"),\n                    labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\"),\n                    ) +\n  scale_linetype_manual(name = 'W effects',\n                        values = c(\"solid\", \"dotted\", \"dashed\"),\n                        labels = c(\"High \\n(Mean+1SD)\", \"Average \\n(Mean)\", \"Low \\n(Mean-1SD)\")) +\n  labs(x = \"the M\", \n       y = \"the Y\") +\n  ggtitle('M * W') +\n  annotate(\"text\", x=10, y=-8, label= \"Low \\n b=-0.47, [-0.49, -0.44]\") +\n  annotate(\"text\", x=25, y=-7, label= \"Average \\n b=-0.37, [-0.38, -0.35]\") +\n  annotate(\"text\", x=20, y=0, label= \"High \\n b=-0.27, [-0.28, -0.25]\") +\n  \n  theme_minimal(base_size = 16)"
  },
  {
    "objectID": "posts/dic_study_2.html",
    "href": "posts/dic_study_2.html",
    "title": "Understand DIC",
    "section": "",
    "text": "This mini-study aims to understand how the DIC (deviance information criterion) index works.\nThe common idea of an information criterion is \\(D + 2pD\\). The \\(D\\) (deviance) can also be presented as \\(-2\\) log-likelihood. Besides, a version of pD (effective number of parameters) of DIC (as same as JAGS program) is defined as the variance of log-likelihood.\nCode\nimport numpy as np\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading iris data set.\nfrom sklearn.datasets import load_iris\niris = load_iris()"
  },
  {
    "objectID": "posts/dic_study_2.html#goals.",
    "href": "posts/dic_study_2.html#goals.",
    "title": "Understand DIC",
    "section": "Goals.",
    "text": "Goals.\né€™å€‹ç ”ç©¶å°±ç°¡å–®æ‹¿ iris è³‡æ–™é›†ä¾†æ¸¬è©¦ã€‚ æˆ‘å€‘çŸ¥é“ iris æœ‰ 4 å€‹ features: èŠ±è¼é•·åº¦ (Sepal.Length), èŠ±è¼å¯¬åº¦ (Sepal.Width), èŠ±ç“£é•·åº¦ (Petal.Length), èŠ±ç“£å¯¬åº¦ (Petal.Width)ã€‚\næˆ‘å€‘ä»Šå¤©å°±ç°¡å–®ç”¨ Sepal.Length ~ Sepal.Width é€™å€‹æ¨¡å¼ä¾†çœ‹çœ‹ DIC æ€éº¼ç®— æ­¤å¤–ï¼Œç‚ºäº†å¢åŠ ä¸€é»åƒæ•¸ï¼Œæˆ‘å€‘å†ä½¿ç”¨ 3 å€‹ targetï¼Œå»ºç«‹éšå±¤ç·šæ€§æ¨¡å¼ã€‚\nå› æ­¤ï¼Œæ¨¡å¼å¦‚ä¸‹ï¼š\n\\[\n\\begin{align}\n\\text{Likelihood:} \\\\\n\\text{Length} &\\sim N(\\mu _w, \\sigma^2) \\\\\n\\mu _w &= \\beta _0 + \\beta _{1i} \\text{Width} \\\\\n\\\\\n\\text{Priors:} \\\\\n\\beta _0, \\beta _{1i} &\\sim N(0,5) \\\\\n\\sigma &\\sim \\text{Exp}(1)\n\\end{align}\n\\]\né€™é‚Šçš„ \\(\\beta _{1i}\\) æ˜¯æ¯ä¸€å€‹ level å°æ‡‰çš„åƒæ•¸ã€‚æ‰€ä»¥æ‡‰è©²æœƒæœ‰ 3 å€‹ã€‚"
  },
  {
    "objectID": "posts/dic_study_2.html#jags",
    "href": "posts/dic_study_2.html#jags",
    "title": "Understand DIC",
    "section": "jags",
    "text": "jags\nLetâ€™s see how to run this model in jags.\nFirstly, we call the iris data set (from R default {datasets})\ndata(iris)\nSecondly, we define the data list and model string in the {R2jags} package. The {R2jags} allows users to write a jags model just like an R function.\n\nThe data list.\n\ndat_list = list(\n  sepal_length = iris$Sepal.Length,\n  sepal_width = iris$Sepal.Width,\n  species = iris$Species,\n  n = 150\n)\n\nThe model string.\n\nmod_string &lt;- \\(){\n  ## priors\n  beta0 ~ dnorm(0,1/5^2)\n  sigma ~ dexp(1)\n  for (j in 1:3){\n    beta1[j] ~ dnorm(0,1/5^2)\n  }\n  \n  ## likelihood\n  for (i in 1:n){\n    mu_w[i] &lt;- beta0 + beta1[species[i]] * sepal_width[i]\n    sepal_length[i] ~ dnorm(mu_w[i], 1/sigma^2) \n  }\n}\nFinally, we run this model through the jags function.\nfit &lt;- jags(data = dat_list, \n     parameters.to.save = c('beta0','beta1','sigma'),\n     model.file = (mod_string)\n     )\nThen, the output of this jags model is shown as follows:\n&gt; print(fit, digits = 3)\nInference for Bugs model at \"/var/folders/1f/8r50hwmn6m5dwrngfgysq4p40000gn/T//Rtmpbvfmga/modelab6b34cc72fd.txt\", fit using jags,\n 3 chains, each with 2000 iterations (first 1000 discarded)\n n.sims = 3000 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nbeta0      3.338   0.333   2.680   3.117   3.336   3.559   3.981 1.001  2400\nbeta1[1]   0.488   0.098   0.298   0.424   0.490   0.552   0.685 1.002  1700\nbeta1[2]   0.938   0.120   0.700   0.856   0.938   1.019   1.175 1.001  2400\nbeta1[3]   1.091   0.113   0.870   1.015   1.091   1.168   1.310 1.002  1900\nsigma      0.444   0.026   0.397   0.426   0.444   0.461   0.496 1.002  1600\ndeviance 180.851   3.199 176.629 178.531 180.100 182.442 188.810 1.002  1400\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts/dic_study_2.html#pymc",
    "href": "posts/dic_study_2.html#pymc",
    "title": "Understand DIC",
    "section": "pymc",
    "text": "pymc\nNow, we use the sync to replicate these results. We are interested in two things,\n\nRQ1. to compare the parameters of beta0, beta1, and sigma.\nRQ2. to compute the (expected) deviance, pD, and DIC.\n\n\n\nCode\niris_data = pd.DataFrame(iris['data'])\niris_data.columns = iris['feature_names']\niris_data\n\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows Ã— 4 columns\n\n\n\n\n\n\nCode\n#seed=1234\n\ntarget_index, target = pd.Series(iris['target']).factorize()\n#width_index, width = iris_data[1].factorize()\n\n\ndict = {\n    'target': iris['target_names'], \n    'target_index': target_index,\n    #'width_index': width_index\n}\ndict\n\n\n{'target': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'),\n 'target_index': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}\n\n\n\nRQ1. Compare jags and pymc\n\n\nCode\nwith pm.Model(coords=dict) as iris_model:   \n    ## data\n    sepal_length = pm.Data('sepal_length', iris_data['sepal length (cm)'])\n    sepal_width = pm.Data('sepal_width', iris_data['sepal width (cm)'])\n    \n\n    ## priors\n    beta0 = pm.Normal('Î²0', 0,5)\n    beta1 = pm.Normal('Î²1', 0,5, shape=3)\n    sigma = pm.Exponential('Ïƒ',1)\n\n    ## likelihood\n    mu_w = beta0 + beta1[target_index] * sepal_width\n    Length = pm.Normal('length', mu_w, sigma, observed=sepal_length)\n\n    ## sampling\n    iris_post = pm.sample( draws=3000, chains=4, cores=4) \n    pm.compute_log_likelihood(iris_post)\n    #ra_4pl_predict = pm.sample_posterior_predictive(ra_4pl_post)\n\n\n/Users/garden/Library/Python/3.9/lib/python/site-packages/pymc/data.py:433: UserWarning: The `mutable` kwarg was not specified. Before v4.1.0 it defaulted to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default changed to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Use `pm.ConstantData`/`pm.MutableData` or pass `pm.Data(..., mutable=False/True)` to avoid this warning.\n  warnings.warn(\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Î²0, Î²1, Ïƒ]\nSampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 5 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [16000/16000 00:04&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(iris_post)\n\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nÎ²0\n3.344\n0.329\n2.739\n3.968\n0.007\n0.005\n2393.0\n2772.0\n1.0\n\n\nÎ²1[0]\n0.487\n0.097\n0.296\n0.657\n0.002\n0.001\n2425.0\n2847.0\n1.0\n\n\nÎ²1[1]\n0.935\n0.119\n0.713\n1.159\n0.002\n0.002\n2469.0\n2887.0\n1.0\n\n\nÎ²1[2]\n1.089\n0.111\n0.877\n1.294\n0.002\n0.002\n2421.0\n2802.0\n1.0\n\n\nÏƒ\n0.444\n0.026\n0.395\n0.492\n0.000\n0.000\n4241.0\n3826.0\n1.0\n\n\n\n\n\n\n\n\nConcluding remarks. For the RQ1, the outputs from jags and pymc show no significant differences.\n\n\nRQ2. Computing DIC.\nFirstly, letâ€™s see the data structure of log_likelihood from the pm.compute_log_likelihood() function. Itâ€™s a three-way dimensions tensor. The first dim is for (4) chains, the second for (3000) draws, and the third for length of data (150).\n\n\nCode\niris_post.log_likelihood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 3000, length_dim_0: 150)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 ... 2994 2995 2996 2997 2998 2999\n  * length_dim_0  (length_dim_0) int64 0 1 2 3 4 5 6 ... 144 145 146 147 148 149\nData variables:\n    length        (chain, draw, length_dim_0) float64 -0.1501 -0.2302 ... -1.553\nAttributes:\n    created_at:                 2024-03-27T11:15:05.525056\n    arviz_version:              0.16.1\n    inference_library:          pymc\n    inference_library_version:  5.9.0xarray.DatasetDimensions:chain: 4draw: 3000length_dim_0: 150Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])length_dim_0(length_dim_0)int640 1 2 3 4 5 ... 145 146 147 148 149array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n        42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n        84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n        98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n       112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149])Data variables: (1)length(chain, draw, length_dim_0)float64-0.1501 -0.2302 ... -1.878 -1.553array([[[-0.15007587, -0.23018022, -0.17186102, ..., -0.15036302,\n         -2.03497421, -1.25868256],\n        [-0.14763492, -0.18836443, -0.24012757, ..., -0.15958522,\n         -1.93430897, -1.20251279],\n        [-0.13546664, -0.24928963, -0.11306264, ..., -0.12683694,\n         -2.32605535, -1.40839025],\n        ...,\n        [-0.14045847, -0.13434047, -0.13613556, ..., -0.14624427,\n         -2.03811634, -1.58747839],\n        [-0.15619394, -0.1480345 , -0.15840489, ..., -0.16924811,\n         -1.99564914, -1.57251438],\n        [-0.06604883, -0.06365481, -0.3631115 , ..., -0.06431746,\n         -1.54910513, -1.23877687]],\n\n       [[-0.2091668 , -0.22398206, -0.26267452, ..., -0.21925268,\n         -1.68602165, -1.25524037],\n        [-0.0207    , -0.00833144, -0.18972243, ..., -0.03910661,\n         -1.76095645, -1.53025309],\n        [-0.07599954, -0.0308677 , -0.08252084, ..., -0.02220397,\n         -1.67739141, -1.4524553 ],\n...\n        [-0.21175487, -0.3262845 , -0.14786837, ..., -0.20980681,\n         -2.44356451, -1.53501204],\n        [-0.10045583, -0.14453996, -0.20253765, ..., -0.12437101,\n         -2.18738956, -1.34751415],\n        [-0.06133903, -0.05676096, -0.27590568, ..., -0.06310933,\n         -1.92064681, -1.27690886]],\n\n       [[-0.12362904, -0.17491691, -0.13462518, ..., -0.09471016,\n         -1.73004549, -1.14761551],\n        [-0.04212361, -0.05487806, -0.19423224, ..., -0.10022129,\n         -2.30313771, -1.63956094],\n        [-0.04604174, -0.07543161, -0.13592791, ..., -0.06340857,\n         -2.12692774, -1.46189584],\n        ...,\n        [-0.10221615, -0.10216825, -0.2916557 , ..., -0.12732807,\n         -1.68477784, -1.35505866],\n        [-0.08520427, -0.07523728, -0.19593031, ..., -0.12138973,\n         -1.90040781, -1.57051397],\n        [-0.07718419, -0.07102337, -0.22110662, ..., -0.11846029,\n         -1.87751136, -1.5528609 ]]])Indexes: (3)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))length_dim_0PandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n      dtype='int64', name='length_dim_0', length=150))Attributes: (4)created_at :2024-03-27T11:15:05.525056arviz_version :0.16.1inference_library :pymcinference_library_version :5.9.0\n\n\nSecondly, letâ€™s try to compute the expected deviance (D) from this tensor. Due to the output from jags, we know the correct answer will be close to 180.851.\nNow, we need to compute the D (-2ll) for each point (there are a total of 150 points in this study\nTips. To sum up the dim we are interested in. In this case, we sum up the dim of length_dim_0 (axis=2). Then we can get 4*3000 draws for each points.\n\n\nCode\ny_ll = iris_post.log_likelihood['length'].sum(axis=2)\ny_deviance = -2*y_ll\ny_deviance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' (chain: 4, draw: 3000)&gt;\narray([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])\nCoordinates:\n  * chain    (chain) int64 0 1 2 3\n  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 2993 2994 2995 2996 2997 2998 2999xarray.DataArray'length'chain: 4draw: 3000180.1 180.2 181.5 181.0 181.9 183.1 ... 178.5 178.5 178.7 179.0 178.3array([[180.13014469, 180.20560338, 181.51113052, ..., 180.46864777,\n        182.32817656, 184.06096534],\n       [179.44237102, 181.94479594, 187.25324731, ..., 179.46417839,\n        183.18168135, 180.72672825],\n       [177.94773331, 177.25331248, 179.26326997, ..., 185.02514987,\n        181.1540135 , 180.74335949],\n       [180.20963194, 177.73703712, 177.10687626, ..., 178.72855249,\n        178.97973022, 178.32675483]])Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 2996 2997 2998 2999array([   0,    1,    2, ..., 2997, 2998, 2999])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999],\n      dtype='int64', name='draw', length=3000))Attributes: (0)\n\n\nThen get the posterior mean of it. It is 180.85.\n\n\nCode\ny_deviance.mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(180.83823109)xarray.DataArray'length'180.8array(180.83823109)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nThirdly, we need to compute the pD. We konw the pD will be close to 5.1.\n\n\nCode\ny_deviance.var()/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(5.37651436)xarray.DataArray'length'5.377array(5.37651436)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nFinally, we can compute the DIC. It will be close to 186.0. There are two kind of mthods to compute it,\n\nUsing log-likelihood. -2*y_ll.mean() + 2*y_ll.var()\nUsing deviance. y_deviance.mean() + y_deviance.var()/2\n\n\n\nCode\nDIC = y_deviance.mean() + y_deviance.var()/2\nDIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'length' ()&gt;\narray(186.21474545)xarray.DataArray'length'186.2array(186.21474545)Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\nYes!! Bingo!!"
  },
  {
    "objectID": "posts/dic_study_2.html#the-easy-function.",
    "href": "posts/dic_study_2.html#the-easy-function.",
    "title": "Understand DIC",
    "section": "The easy function.",
    "text": "The easy function.\nFurthermore, we write a function to output the strings like the jags program.\nIt will look like,\nDIC info (using the rule, pD = var(deviance)/2)\ndeviance = 180.85, pD = 5.1 and DIC = 186.0\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCode\ndef get_dic(posterior_tensor, var_names):\n    y_ll = posterior_tensor.log_likelihood[var_names].sum(axis=2).to_numpy()\n    y_deviance = -2*y_ll.mean()\n    y_pd = 2*y_ll.var()\n    y_dic = y_deviance + y_pd\n\n    y_print =   'DIC info (using the rule, pD = var(deviance)/2) \\n' +\\\n                'mean deviance = {:.3f}, pD = {:.3f} and DIC = {:.3f} \\n'.format(y_deviance, y_pd, y_dic) +\\\n                'DIC is an estimate of expected predictive error (lower deviance is better).'\n            \n    return print(y_print)\n\n\n\n\nCode\nget_dic(iris_post, var_names='length')\n\n\nDIC info (using the rule, pD = var(deviance)/2) \nmean deviance = 180.838, pD = 5.377 and DIC = 186.215 \nDIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "posts_zh/post0704.html",
    "href": "posts_zh/post0704.html",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "",
    "text": "ä»Šæ—¥é€™ç¯‡æ–‡ç« é—œæ–¼åŠæ©‹æ‰‹å†Š (Cambridge New Handbook of Mathematical Psychology)ï¼Œåªæ˜¯ç‚ºäº†è¨˜éŒ„ã€Œæ•¸å­¸å¿ƒç†å­¸ã€çš„å…§å®¹ã€‚æ—¥å¾Œä¹Ÿè¨±é‚„è¦è¨˜ä¸‹CRC pressçš„ã€ŒIRTæ‰‹å†Šã€ã€‚è®€é€™äº›æ‰‹å†Šï¼Œå€’ä¹Ÿæœ‰è¶£ï¼Œèƒ½äº†è§£ç•¶ä»Šç™¼å±•çš„ä¸»é¡Œï¼Œçœ‹çœ‹æœªä¾†é‚„æœ‰ä»€éº¼å¯åšçš„ã€‚"
  },
  {
    "objectID": "posts_zh/post0704.html#å·ä¸€åŸºç¤èˆ‡æ–¹æ³•",
    "href": "posts_zh/post0704.html#å·ä¸€åŸºç¤èˆ‡æ–¹æ³•",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "å·ä¸€ï¼ŒåŸºç¤èˆ‡æ–¹æ³•ã€‚",
    "text": "å·ä¸€ï¼ŒåŸºç¤èˆ‡æ–¹æ³•ã€‚\n\næ©Ÿç‡ä¸­çš„ç²¾é¸æ¦‚å¿µã€‚ Selected concepts from probability, pp 1-84\næ©Ÿç‡ã€éš¨æ©Ÿè®Šæ•¸ã€é¸æ“‡æ€§ã€‚ Probability, random variables, and selectivity, pp 85-150\nå‡½æ•¸æ–¹ç¨‹å¼ã€‚ Functional equations, pp 151-193\nç¶²è·¯åˆ†æã€‚ Network analysis, pp 194-273\nçŸ¥è­˜ç©ºé–“èˆ‡å­¸ç¿’ç©ºé–“ã€‚ Knowledge spaces and learning spaces, pp 274-321\næ¼”åŒ–å°å±€è«–ï¼ˆåšå¼ˆè«–ï¼‰ã€‚ Evolutionary game theory, pp 322-373\né¸æ“‡ã€åå¥½ã€æ•ˆç”¨ï¼šæ©Ÿç‡æ€§å’Œç¢ºå®šæ€§è¡¨é”ã€‚ Choice, preference, and utility: probabilistic and deterministic representations, pp 374-453\nèªçŸ¥çš„é›¢æ•£ç‹€æ…‹æ¨¡å¼ã€‚ Discrete state models of cognition, pp 454-503\nèªçŸ¥çš„è²æ°éšå±¤æ¨¡å¼ã€‚ Bayesian hierarchical models of cognition, pp 504-551\næ¨¡å¼è©•ä¼°å’Œé¸æ“‡ã€‚ Model evaluation and selection, pp 552-598"
  },
  {
    "objectID": "posts_zh/post0704.html#å·äºŒå»ºæ¨¡èˆ‡æ¸¬é‡",
    "href": "posts_zh/post0704.html#å·äºŒå»ºæ¨¡èˆ‡æ¸¬é‡",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "å·äºŒï¼Œå»ºæ¨¡èˆ‡æ¸¬é‡ã€‚",
    "text": "å·äºŒï¼Œå»ºæ¨¡èˆ‡æ¸¬é‡ã€‚\n\næ±ºç­–å»ºæ¨¡çš„éš¨æ©Ÿæ–¹æ³•ã€‚ Stochastic Methods for Modeling Decision-making, pp 1-70\nå¾ç†æ€§è§€é»çœ‹åŠ é€Ÿé¸æ“‡çš„æ“´æ•£æ¨¡å¼ã€‚ The Diffusion Model of Speeded Choice, from a Rational Perspective, pp 71-103\nåŸºæœ¬å¿ƒç†æ¶æ§‹çš„éš¨æ©ŸåŸºç¤ã€‚ Stochastic Foundations of Elementary Mental Architectures, pp 104-127\næ©Ÿç‡æ¨¡å¼çš„å¯è­˜åˆ¥æ€§ï¼ŒçŸ¥è­˜çµæ§‹ç†è«–çš„ä¾‹å­ã€‚ Identifiability of Probabilistic Models, with Examples from Knowledge Structure Theory, pp 128-184\nèªçŸ¥èˆ‡æ±ºç­–çš„é‡å­æ¨¡å¼ã€‚ Quantum Models of Cognition and Decision, pp 185-222\nè¨ˆç®—èªçŸ¥ç¥ç¶“ç§‘å­¸ã€‚ Computational Cognitive Neuroscience, pp 223-270\né€éæŠ•ç¥¨ç™¼ç¾èšåˆå±¬æ€§ã€‚ Discovering Aggregation Properties via Voting, pp 271-321\nåŸºæ–¼ç›¸ä¼¼æ€§å’Œç‰¹å¾µçš„åˆ†é¡ï¼šé‡ç¾ RKBS æ–¹æ³•ã€‚ Categorization Based on Similarity and Features: The Reproducing Kernel Banach Space (RKBS) Approach, pp 322-373\nç§‘å­¸å’Œå¹¾ä½•å­¸ä¸­çš„æ„ç¾©å…¬ç†ã€‚ The Axiom of Meaningfulness in Science and Geometry, pp 374-456"
  },
  {
    "objectID": "posts_zh/post0704.html#å·ä¸‰çŸ¥è¦ºèˆ‡èªçŸ¥éç¨‹",
    "href": "posts_zh/post0704.html#å·ä¸‰çŸ¥è¦ºèˆ‡èªçŸ¥éç¨‹",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "å·ä¸‰ï¼ŒçŸ¥è¦ºèˆ‡èªçŸ¥éç¨‹ã€‚",
    "text": "å·ä¸‰ï¼ŒçŸ¥è¦ºèˆ‡èªçŸ¥éç¨‹ã€‚\n\nåˆå§‹è¦–è¦ºç·¨ç¢¼çš„åŸç†å’Œçµæœã€‚ Principles and Consequences of the Initial Visual Encoding, pp 1-41\nå†é¸å®šç¯„å¼ä¸­æ¸¬é‡å¤šæ„Ÿå®˜æ•´åˆã€‚ Measuring Multisensory Integration in Selected Paradigms, pp 42-79\nè²»å¸Œç´åº¦é‡æ³•ï¼šå·®ç•°ç´¯ç©ç†è«–ã€‚ Fechnerian Scaling: Dissimilarity Cumulation Theory, pp 80-162\näººé¡å­¸ç¿’çš„æ•¸å­¸æ¨¡å¼ã€‚ Mathematical Models of Human Learning, pp 163-217\nåŸºæ–¼æ™‚è®Šè¡¨é”çš„è¨˜æ†¶å½¢å¼æ¨¡å¼ã€‚ Formal Models of Memory Based on Temporally-Varying Representations, pp 218-264\nçµ±è¨ˆæ±ºç­–ç†è«–ã€‚ Statistical Decision Theory, pp 265-310\nåœæ­¢ä¿¡è™Ÿä»»å‹™ä¸­çš„åæ‡‰æŠ‘åˆ¶å»ºæ¨¡ã€‚ Modeling Response Inhibition in the Stop-Signal Task, pp 311-356\nè¿‘ä¼¼è²æ°è¨ˆç®—ã€‚ Approximate Bayesian Computation, pp 357-384\nèªçŸ¥è¨ºæ–·æ¨¡å¼ã€‚ Cognitive Diagnosis Models, pp 385-420\nç¥ç¶“å½±åƒå­¸ä¸­çš„ç·¨ç¢¼æ¨¡å¼ã€‚ Encoding Models in Neuroimaging, pp 421-472\n\n\né™¤äº†æ•¸å­¸å¿ƒç†å­¸ï¼Œçµ±è¨ˆå¿ƒç†å­¸æˆ–é€™è£¡æ‰€èªªçš„é‡åŒ–å¿ƒç†å­¸ã€è¨ˆé‡å¿ƒç†å­¸ï¼Œæˆ–è¨±æ‰æ˜¯æ›´ç‚ºå¸¸è¦‹çš„ç ”ç©¶é ˜åŸŸã€‚æˆ‘å€‘é€™è£¡è¨˜éŒ„çš„æ˜¯ç‰›æ´¥æ‰‹å†Š (The Oxford Handbook of Quantitative Methods in Psycholog) ä¸­çš„å…§å®¹ã€‚åªæ˜¯ä»–å¹´ä»£ç¨å¾®æ—©ä¸€é»ï¼Œæ˜¯ 2013 å¹´å‡ºç‰ˆçš„ã€‚\n\nå·ä¸€ï¼ŒåŸºç¤ã€‚ Foundations, 2013.\nå·äºŒï¼Œçµ±è¨ˆåˆ†æã€‚ Statistical Analysis, 2013."
  },
  {
    "objectID": "posts_zh/post0704.html#å·ä¸€åŸºç¤",
    "href": "posts_zh/post0704.html#å·ä¸€åŸºç¤",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "å·ä¸€ï¼ŒåŸºç¤ã€‚",
    "text": "å·ä¸€ï¼ŒåŸºç¤ã€‚\n\nå‰è¨€ã€‚ Introduction\né‡åŒ–æ–¹æ³•çš„å“²å­¸ã€‚ The Philosophy of Quantitative Methods\né‡åŒ–æ–¹æ³•çš„å€«ç†ã€‚ Quantitative Methods and Ethics\nç‰¹å®šç¾¤é«”ã€‚ Special Populations\nç†è«–å»ºæ§‹ã€æ¨¡å¼å»ºç«‹ã€æ¨¡å¼é¸æ“‡ã€‚ Theory Construction, Model Building, and Model Selection\né‡åŒ–å¿ƒç†å­¸æ•™å­¸ã€‚ Teaching Quantitative Psychology\nç•¶ä»£æ¸¬é©—ç†è«–ã€‚ Modern Test Theory\nIRT å‚³çµ±èˆ‡å…¶æ‡‰ç”¨ã€‚ The IRT Tradition and its Applications\nèª¿æŸ¥è¨­è¨ˆå’Œæ¸¬é‡ç™¼å±•ã€‚ Survey Design and Measure Development\né«˜é¢¨éšªæ¸¬é©—çš„å»ºæ§‹å’Œä½¿ç”¨ã€‚ High-Stakes Test Construction and Test Use\næ•ˆæœé‡å’Œæ¨£æœ¬æ•¸è¦åŠƒã€‚ Effect Size and Sample Size Planning\nå› æœæ¨è«–çš„å¯¦é©—è¨­è¨ˆï¼šè‡¨åºŠè©¦é©—å’Œè¿´æ­¸ä¸é€£çºŒè¨­è¨ˆã€‚ Experimental Design for Causal Inference: Clinical Trials and Regression Discontinuity Designs\né…å°å’Œå‚¾å‘åˆ†æ•¸ã€‚ Matching and Propensity Scores\nåæ‡‰æ™‚é–“å¯¦é©—è¨­è¨ˆèˆ‡åˆ†æã€‚ Designs for and Analyses of Response Time Experiments\nè§€å¯Ÿæ€§æ–¹æ³•ã€‚ Observational Methods\næµè¡Œç—…å­¸æ–¹æ³•ã€æ¦‚å¿µèˆ‡åˆ†æï¼Œå¿ƒç†å­¸ä¸­çš„é€²éšæ‡‰ç”¨ç‚ºä¾‹å­ã€‚ A Primer of Epidemiologic Methods, Concepts, and Analysis With Examples and More Advanced Applications Within Psychology\næ–¹æ¡ˆè©•ä¼°ï¼šåŸç†ã€ç¨‹åºèˆ‡å¯¦è¸ã€‚ Program Evaluation: Principles, Procedures, and Practices\nçµ±è¨ˆä¼°è¨ˆæ–¹æ³•æ¦‚è¿°ã€‚ Overview of Statistical Estimation Methods\nç©©å¥çµ±è¨ˆä¼°è¨ˆã€‚ Robust Statistical Estimation\nè²æ°çµ±è¨ˆæ–¹æ³•ã€‚ Bayesian Statistical Methods\næ•¸å­¸å»ºæ¨¡ã€‚ Mathematical Modeling\nå­¸è¡“ç ”ç©¶ä¸­çš„è’™åœ°å¡ç¾…åˆ†æã€‚ Monte Carlo Analysis in Academic Research\nç¶²è·¯åˆ†æï¼šé‡è¦æ¦‚å¿µå®šç¾©æŒ‡å—ã€‚ Network Analysis: A Definitional Guide to Important Concepts"
  },
  {
    "objectID": "posts_zh/post0704.html#å·äºŒçµ±è¨ˆåˆ†æ",
    "href": "posts_zh/post0704.html#å·äºŒçµ±è¨ˆåˆ†æ",
    "title": "ã€ŒåŠæ©‹æ–°ç‰ˆæ•¸å­¸å¿ƒç†å­¸æ‰‹å†Šã€èˆ‡ã€Œç‰›æ´¥å¿ƒç†å­¸é‡åŒ–æ–¹æ³•æ‰‹å†Šã€çš„å…§å®¹",
    "section": "å·äºŒï¼Œçµ±è¨ˆåˆ†æã€‚",
    "text": "å·äºŒï¼Œçµ±è¨ˆåˆ†æã€‚\n\nå‰è¨€ã€‚ Introduction\nå‚³çµ±ã€å¤å…¸æ–¹æ³•æ¦‚è¿°ã€‚ Overview of Traditional/Classical Statistical Approaches\nå»£ç¾©ç·šæ€§æ¨¡å¼ã€‚ Generalized Linear Models\né¡åˆ¥æ–¹æ³•ï¼ˆé¡åˆ¥è³‡æ–™åˆ†æï¼‰ã€‚ Categorical Methods\né…ç½®é »æ¬¡åˆ†æã€‚ Configural Frequency Analysis\néåƒæ•¸çµ±è¨ˆæŠ€è¡“ã€‚ Nonparametric Statistical Techniques\nå°æ‡‰åˆ†æã€‚ Correspondence Analysis\nç©ºé–“åˆ†æã€‚ Spatial Analysis\nå½±åƒè³‡æ–™åˆ†æã€‚ Analysis of Imaging Data\né›™èƒèƒç ”ç©¶èˆ‡è¡Œç‚ºéºå‚³å­¸ã€‚ Twin Studies and Behavior Genetics\nåŸºå› é‡åŒ–åˆ†æã€‚ Quantitative Analysis of Genes\nå¤šå…ƒåº¦é‡æ³•ã€‚ Multidimensional Scaling\næ½›åœ¨è®Šé …æ¸¬é‡æ¨¡å¼ã€‚ Latent Variable Measurement Models\nå¤šå±¤æ¬¡è¿´æ­¸å’Œå¤šå±¤æ¬¡çµæ§‹æ–¹ç¨‹å¼ã€‚ Multilevel Regression and Multilevel Structural Equation Modeling\nçµæ§‹æ–¹ç¨‹å¼ã€‚ Structural Equation Models\nä¸­ä»‹åˆ†æçš„ç™¼å±•ã€‚ Developments in Mediation Analysis\nèª¿ç¯€åˆ†æã€‚ Moderation\nç¸±å‘è³‡æ–™åˆ†æï¼ˆé•·æœŸè¿½è¹¤è³‡æ–™åˆ†æï¼‰ã€‚ Longitudinal Data Analysis\nå‹•æ…‹ç³»çµ±å’Œé€£çºŒæ™‚é–“æ¨¡å¼ã€‚ Dynamical Systems and Models of Continuous Time\nå¯†é›†å‹è¿½è¹¤è³‡æ–™ã€‚ Intensive Longitudinal Data\nå‹•æ…‹å› ç´ åˆ†æï¼šå€‹äººéç¨‹å»ºæ¨¡ã€‚ Dynamic Factor Analysis: Modeling Person-Specific Process\næ™‚é–“åºåˆ—åˆ†æã€‚ Time Series Analysis\näº‹ä»¶å²è³‡æ–™åˆ†æã€‚ Analyzing Event History Data\nèšé¡å’Œåˆ†é¡ã€‚ Clustering and Classification\næ½›åœ¨é¡åˆ¥åˆ†æå’Œæœ‰é™æ··åˆæ¨¡å¼ã€‚ Latent Class Analysis and Finite Mixture Modeling\nåˆ†é¡è¨ˆé‡å­¸ã€‚ Taxometrics\néºæ¼å€¼æ–¹æ³•ã€‚ Missing Data Methods\næ¬¡ç´šè³‡æ–™åˆ†æã€‚ Secondary Data Analysis\nè³‡æ–™æ¡ç¤¦ã€‚ Data Mining\nå¾Œè¨­åˆ†æèˆ‡é‡åŒ–ç ”ç©¶ç¶œè¿°ã€‚ Meta-Analysis and Quantitative Research Synthesis\né‡åŒ–æ–¹æ³•ä¸­çš„å¸¸è¦‹è¬¬èª¤ã€‚ Common Fallacies in Quantitative Research Methodology"
  },
  {
    "objectID": "posts_zh/journal_mbr.html",
    "href": "posts_zh/journal_mbr.html",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "",
    "text": "Multivariate Behavioral Research å¤šè®Šé‡è¡Œç‚ºç ”ç©¶ï¼Œæ˜¯ Society of Multivariate Experimental Psychology (SMEP) å¤šè®Šé‡å¯¦é©—å¿ƒç†å­¸æœƒçš„æœŸåˆŠï¼Œç”± Taylor & Francis å‡ºç‰ˆã€‚æ¯å¹´ 6 æœŸã€‚"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#ä¸»è¦è³‡è¨Š",
    "href": "posts_zh/journal_mbr.html#ä¸»è¦è³‡è¨Š",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "ä¸»è¦è³‡è¨Š",
    "text": "ä¸»è¦è³‡è¨Š\n\næˆ‘å€‘ä¾†çœ‹ä¸€ä¸‹ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠçš„ä¸€äº›æœ‰è¶£æ•¸æ“šã€‚åœ¨ 2010-2023 å¹´ï¼Œé€™æœŸåˆŠå…±æœ‰ 865 ç¯‡æ–‡ç« ï¼Œå…¶ä¸­åˆè‘—ä½œè€…å¹³å‡ 2.72 ä½ã€‚æ¯å¹´æˆé•·ç‡ -6.48%ï¼Œæ–‡ç« å¹³å‡å¹´é½¡ 6.68 æ­²ï¼Œæ¯ç¯‡æ–‡ç« å¹³å‡è¢«å¼•ç”¨ 34.93 æ¬¡ã€‚é€™äº›æ•¸å­—èƒŒå¾Œåæ˜ äº†é€™å€‹é ˜åŸŸçš„ç ”ç©¶æ´»åŠ›å’Œå½±éŸ¿åŠ›ã€‚"
  },
  {
    "objectID": "posts_zh/journal_mbr.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "href": "posts_zh/journal_mbr.html#ç ”ç©¶è©±é¡Œåœ°åœ–",
    "title": "ã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠæœ€æœ‰å½±éŸ¿åŠ›çš„ 20 ç¯‡æ–‡ç« ï¼ˆMultivariateBehavRes 2010-2023ï¼‰",
    "section": "ç ”ç©¶è©±é¡Œåœ°åœ–",
    "text": "ç ”ç©¶è©±é¡Œåœ°åœ–\n\nã€Œå¤šè®Šé‡è¡Œç‚ºç ”ç©¶ã€æœŸåˆŠå¾ 2010 å¹´ä»¥ä¾†ï¼Œæœ€é—œéµçš„ 20+ ç¯‡æ–‡ç« ï¼Œå¤§è‡´å‘ˆç¾å‡º 5 æ¢ä¸»è¦çš„ç ”ç©¶è©±é¡Œã€‚ä»¥åŠè¨±å¤šå–®ç¨çš„é‡è¦æ–‡ç« ã€‚æˆ‘å€‘ä¹Ÿæä¾›æ–‡ç»ç¸®å¯«ï¼ˆå’Œä¸Šåœ–å°æ‡‰ï¼‰ï¼Œdoiï¼Œä»¥åŠä¸­æ–‡ç¿»è­¯åç¨±ã€‚æ–‡ç»æ’åºå’Œåœ–ä¸Šä¸€æ¨£ï¼Œç”±ä¸Šè‡³ä¸‹ã€‚æœ‰èˆˆè¶£çš„åŒå­¸å¯ä»¥ç”¨ doi å»æ‰¾åˆ°å°æ‡‰çš„æ–‡ç« ã€‚\n\nè©±é¡Œ 1ï¼šå› ç´ åˆ†æï¼ˆç´…è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nSASS DA, 2010, DOI 10.1080/00273170903504810 æ¢ç´¢æ€§å› ç´ åˆ†æä¸­æ—‹è½‰æ¨™æº–çš„æ¯”è¼ƒç ”ç©¶\nMOORE TM, 2015, DOI 10.1080/00273171.2014.973990 éƒ¨åˆ†æŒ‡å®šç›®æ¨™çŸ©é™£çš„å ä»£ï¼š æ¢ç´¢æ€§å’Œè²è‘‰æ–¯ç¢ºè­‰å› å­åˆ†æä¸­çš„æ‡‰ç”¨\n\n\n\nè©±é¡Œ 2ï¼šé•·æœŸè¿½è¹¤è³‡æ–™è­°é¡Œï¼ˆè—è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nMAXWELL SE, 2011, DOI 10.1080/00273171.2011.606716 ç¸±å‘ä¸­ä»‹æ©«æ–·é¢åˆ†æä¸­çš„åå·®ï¼šè‡ªå›æ­¸æ¨¡å‹ä¸‹çš„éƒ¨åˆ†ä¸­ä»‹å’Œå®Œå…¨ä¸­ä»‹ ğŸŒŸ æœ€å‡ºåœˆæ–‡ç« ï¼(GCS: 833) ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 15)\nREICHARDT CS, 2011, DOI 10.1080/00273171.2011.606740 è©•è«–ï¼šä¸‰æ³¢æ•¸æ“šæ˜¯å¦è¶³ä»¥è©•ä¼°ä¸­ä»‹ä½œç”¨ï¼Ÿ\n\n\n\nè©±é¡Œ 3ï¼šå¯†é›†å‹è¿½è¹¤è³‡æ–™ã€å‹•æ…‹çµæ§‹æ–¹ç¨‹å¼ï¼ˆç¶ è‰²ï¼Œæ¯”é‡ 43.75%ï¼‰\n\nSTEELE JS, 2011, DOI 10.1080/00273171.2011.625305 è‡ªæˆ‘èª¿ç¯€å’Œæ ¸å¿ƒèª¿ç¯€æƒ…æ„Ÿéç¨‹çš„æ½›å¾®åˆ†æ–¹ç¨‹æ¨¡å‹\nCHOW SM, 2011, DOI 10.1080/00273171.2011.563697 å…·æœ‰æ™‚è®Šåƒæ•¸çš„å‹•æ…‹å› ç´ åˆ†ææ¨¡å‹\nFERRER E, 2012, DOI 10.1080/00273171.2012.640605 åˆ©ç”¨å€‹é«”å…§éƒ¨å’Œå€‹é«”ä¹‹é–“çš„è®Šç•°æ¨¡å¼åˆ†ææƒ…æ„ŸäºŒå…ƒäº’å‹•çš„å‹•æ…‹æ€§\nVOELKLE MC, 2014, DOI 10.1080/00273171.2014.889593 ç‚ºç ”ç©¶äººèˆ‡äººä¹‹é–“å’Œäººèˆ‡äººä¹‹é–“çš„çµæ§‹å»ºç«‹çµ±ä¸€çš„æ¡†æ¶ï¼š åœ¨å…©ç¨®ç ”ç©¶ç¯„å¼ä¹‹é–“æ¶èµ·ä¸€åº§æ©‹æ¢\nJONGERLING J, 2015, DOI 10.1080/00273171.2014.1003772 å¤šå±¤æ¬¡ ar(1) æ¨¡å‹ï¼šè€ƒæ…®ç‰¹è³ªåˆ†æ•¸ã€æ…£æ€§å’Œå‰µæ–°è®Šç•°çš„å€‹é«”é–“å·®ç•°\nHAMAKER EL, 2018, DOI 10.1080/00273171.2018.1446819 å¯†é›†ç¸±å‘æ•¸æ“šå»ºæ¨¡çš„å‰æ²¿ï¼šCogito ç ”ç©¶ä¸­æƒ…æ„Ÿæ¸¬é‡çš„å‹•æ…‹çµæ§‹æ–¹ç¨‹æ¨¡å‹\nBRINGMANN LF, 2018, DOI 10.1080/00273171.2018.1439722 ä½¿ç”¨æ™‚è®Šå‘é‡è‡ªå›æ­¸æ¨¡å‹å»ºæ¨¡äºŒäººçµ„ä¸­çš„éå¹³ç©©æƒ…ç·’å‹•æ…‹\n\n\n\nè©±é¡Œ 4ï¼šè²æ°è³‡æ–™åˆ†æè­°é¡Œï¼ˆç´«è‰²ï¼Œæ¯”é‡ 18.75%ï¼‰\n\nSONG HR, 2012, DOI 10.1080/00273171.2012.640593 éš¨æ©Ÿç³»æ•¸å‹•æ…‹å› å­æ¨¡å‹çš„è²è‘‰æ–¯ä¼°è¨ˆ\nSCHUURMAN NK, 2016, DOI 10.1080/00273171.2015.1065398 å¤šç´šè‡ªå›æ­¸æ¨¡å‹ä¸­å”æ–¹å·®çŸ©é™£çš„é€† Wishart å…ˆé©—è¦ç¯„æ¯”è¼ƒ\nEPSKAMP S, 2018, DOI 10.1080/00273171.2018.1454823 æ©«æˆªé¢å’Œæ™‚é–“åºåˆ—æ•¸æ“šä¸­çš„é«˜æ–¯åœ–å½¢æ¨¡å‹ ğŸŒŸ åœˆå…§äººæœ€æ„›ï¼(LCS: 15)\n\n\n\nè©±é¡Œ 5ï¼šæ©Ÿå™¨å­¸ç¿’è­°é¡Œï¼ˆæ©˜è‰²ï¼Œæ¯”é‡ 12.5%ï¼‰\n\nMCNEISH DM, 2015, DOI 10.1080/00273171.2015.1036965 ä½¿ç”¨lassoé€²è¡Œé æ¸¬å› å­é¸æ“‡ä¸¦ç·©è§£éåº¦æ“¬åˆï¼š è¡Œç‚ºç§‘å­¸ä¸­é•·æœŸè¢«å¿½è¦–çš„æ–¹æ³•\nWILLIAMS DR, 2019, DOI 10.1080/00273171.2019.1575716 é—œæ–¼å¿ƒç†ç¶²çµ¡çš„éè¦å‰‡åŒ–ä¼°è¨ˆ"
  },
  {
    "objectID": "posts_zh/1stBayes10.html",
    "href": "posts_zh/1stBayes10.html",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\né€™é‚Šå¯ä»¥æ³¨æ„å¹¾ä»¶äº‹é—œæ–¼ juliaã€‚\n\nå¦‚æœæ˜¯è¦ç”Ÿæˆä¸€å€‹ scalerï¼Œé‚£å¯« rand(Normal()) å°±å¯ä»¥äº†ã€‚å¯« rand(Normal(),1) æœƒç”¢ç”Ÿä¸€å€‹å‘é‡ vectorï¼Œå°è‡´å¾Œé¢çš„å‡½æ•¸ç„¡æ³•æ¥å—ã€‚\nlogpdf.(Normal(___, ___)), y) ç”±æ–¼å¾Œé¢çš„ y æ˜¯ä¸€å€‹å‘é‡ï¼Œæ‰€ä»¥éœ€è¦å¯«æˆ logpdf. åšå»£æ’­è¨ˆç®—ï¼Œå¦å‰‡è¨ˆç®—æœƒå‡ºå•é¡Œã€‚\nÎ˜ = Float64[] å’Œ push!(Î˜, theta) çš„æ­é…ã€‚æˆ‘ä¸çŸ¥é“é€™æ¨£åšè·Ÿ Array{Float64}(undef, S) èª°çš„æ•ˆèƒ½æ¯”è¼ƒå¥½ï¼Ÿå¯èƒ½æ˜¯å¾Œè€…å§ã€‚ä½†å› ç‚ºæ²’æœ‰é«”æ„Ÿå·®åˆ¥ï¼Œæ‰€ä»¥ä¸ç¢ºå®šã€‚\n\n\n\nCode\n#-------\n\n# åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n\"\"\"\n(s2, t2, mu, y, delta2, S)\né€™å¹¾å€‹å¼•æ•¸ï¼Œå¯å¯«å¯ä¸å¯«ã€‚\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # èª¿æ•´é€™å€‹åƒæ•¸ï¼Œå¯æ”¹è®Šæ¥å—ç‡ã€‚\n    S = 10_000\n    theta = 0.0\n    Î˜ = Float64[]  # ç”¨äºå­˜å‚¨ theta çš„æ•°ç»„\n    accept_count = 0  # åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # å¢åŠ æ¥å—è®¡æ•°å™¨\n        end\n\n        push!(Î˜, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Î˜, accept_rate\nend\n\n\n# è°ƒç”¨å‡½æ•°\nÎ˜, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7848\n\n\n\nå¦‚æœ Î´2 = 0.1ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.70 å·¦å³ã€‚\nå¦‚æœ Î´2 = 1.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.50 - 0.60ã€‚\nå¦‚æœ Î´2 = 2.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.3 å·¦å³ã€‚\n\nèª¿é«˜ä¸€é»ï¼Œ\nä¾†ç•«é»åœ–ã€‚ é€™æ¨£æ²’éŒ¯ï¼Œè·Ÿæ›¸ä¸Šçš„æ˜¯ä¸€æ¨£çš„ã€‚\n\n\nCode\nplot(Î˜)"
  },
  {
    "objectID": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "href": "posts_zh/1stBayes10.html#the-metropolis-algorithm",
    "title": "Chapter 10",
    "section": "",
    "text": "Code\nusing Random, Distributions, Plots\n\n\né€™é‚Šå¯ä»¥æ³¨æ„å¹¾ä»¶äº‹é—œæ–¼ juliaã€‚\n\nå¦‚æœæ˜¯è¦ç”Ÿæˆä¸€å€‹ scalerï¼Œé‚£å¯« rand(Normal()) å°±å¯ä»¥äº†ã€‚å¯« rand(Normal(),1) æœƒç”¢ç”Ÿä¸€å€‹å‘é‡ vectorï¼Œå°è‡´å¾Œé¢çš„å‡½æ•¸ç„¡æ³•æ¥å—ã€‚\nlogpdf.(Normal(___, ___)), y) ç”±æ–¼å¾Œé¢çš„ y æ˜¯ä¸€å€‹å‘é‡ï¼Œæ‰€ä»¥éœ€è¦å¯«æˆ logpdf. åšå»£æ’­è¨ˆç®—ï¼Œå¦å‰‡è¨ˆç®—æœƒå‡ºå•é¡Œã€‚\nÎ˜ = Float64[] å’Œ push!(Î˜, theta) çš„æ­é…ã€‚æˆ‘ä¸çŸ¥é“é€™æ¨£åšè·Ÿ Array{Float64}(undef, S) èª°çš„æ•ˆèƒ½æ¯”è¼ƒå¥½ï¼Ÿå¯èƒ½æ˜¯å¾Œè€…å§ã€‚ä½†å› ç‚ºæ²’æœ‰é«”æ„Ÿå·®åˆ¥ï¼Œæ‰€ä»¥ä¸ç¢ºå®šã€‚\n\n\n\nCode\n#-------\n\n# åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n\"\"\"\n(s2, t2, mu, y, delta2, S)\né€™å¹¾å€‹å¼•æ•¸ï¼Œå¯å¯«å¯ä¸å¯«ã€‚\n\"\"\"\n\ny = [9.31, 10.18, 9.16, 11.60, 10.33]\n\nfunction mh_sampler(y, delta2)\n    s2 = 1\n    t2 = 10\n    mu = 5\n    theta = 0\n    #delta2 = 0.5 # èª¿æ•´é€™å€‹åƒæ•¸ï¼Œå¯æ”¹è®Šæ¥å—ç‡ã€‚\n    S = 10_000\n    theta = 0.0\n    Î˜ = Float64[]  # ç”¨äºå­˜å‚¨ theta çš„æ•°ç»„\n    accept_count = 0  # åˆå§‹åŒ–æ¥å—è®¡æ•°å™¨\n    for s in 1:S\n        theta_star = theta + rand(Normal(0, sqrt(delta2)))\n\n        log_r = (sum(logpdf.(Normal(theta_star, sqrt(s2)), y)) + \n        logpdf(Normal(mu, sqrt(t2)), theta_star) - \n        sum(logpdf.(Normal(theta, sqrt(s2)), y)) - \n        logpdf(Normal(mu, sqrt(t2)), theta))\n\n        if log(rand(Uniform())) &lt; log_r\n            theta = theta_star\n            accept_count += 1 # å¢åŠ æ¥å—è®¡æ•°å™¨\n        end\n\n        push!(Î˜, theta)\n    end\n\n    accept_rate = accept_count/S\n    return Î˜, accept_rate\nend\n\n\n# è°ƒç”¨å‡½æ•°\nÎ˜, acceptance_rate = mh_sampler(y, 0.1)\nacceptance_rate\n\n\n0.7848\n\n\n\nå¦‚æœ Î´2 = 0.1ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.70 å·¦å³ã€‚\nå¦‚æœ Î´2 = 1.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.50 - 0.60ã€‚\nå¦‚æœ Î´2 = 2.0ï¼Œæ¥å—ç‡å¤§ç´„åœ¨ 0.3 å·¦å³ã€‚\n\nèª¿é«˜ä¸€é»ï¼Œ\nä¾†ç•«é»åœ–ã€‚ é€™æ¨£æ²’éŒ¯ï¼Œè·Ÿæ›¸ä¸Šçš„æ˜¯ä¸€æ¨£çš„ã€‚\n\n\nCode\nplot(Î˜)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "jw tsai",
    "section": "",
    "text": "About this jw tsai."
  },
  {
    "objectID": "posts/IntroJuliaConnectoR.html#the-modeling-part.",
    "href": "posts/IntroJuliaConnectoR.html#the-modeling-part.",
    "title": "Introduction to ExtendedRtIrtModeling.jl through JuliaConnectoR",
    "section": "The modeling part.",
    "text": "The modeling part.\nLetâ€™s follow the example from the Github readme post to show how a basic RT-IRT model works. This package is version 0.2.0.\n\n\nCode\n## You have to give a name to the Julia Environment.\nex &lt;- juliaImport(\"ExtendedRtIrtModeling\")\n\n\nStarting Julia ...\n\n\nCode\njuliaEval('Pkg.status(\"ExtendedRtIrtModeling\")')\n\n\nStatus `~/.julia/environments/v1.10/Project.toml`\n  [1fd685a6] ExtendedRtIrtModeling v0.2.0 `https://github.com/jiewenTsai/ExtendedRtIrtModeling.jl#main`\n\n\nIn the original Github post, the user doesnâ€™t have to fill in all five arguments of InputData because the values for \\(\\kappa\\) and logT are automatically calculated by Y and T. However, to get it working with R, Iâ€™ve come up with another struct called InputData4R, which is specifically for R users.\n\n\nCode\n## import your data set\nCond = ex$setCond(\n  nChain=3, \n  nIter=3000,\n  nSubj=300,\n  nItem=10,\n  nFeat=4\n  )\nData = ex$InputData4R(\n    Y = as.matrix(demo[2:11]),\n    # you must write this line!\n    Îº = as.matrix(demo[2:11]-0.5),\n    T = as.matrix(exp(demo[12:21])),\n    logT = as.matrix(demo[12:21]),\n    X = as.matrix(demo[22:25])\n)\n\n## build a model and sample it!\nMCMC = ex$GibbsRtIrt(Cond, Data=Data)\n## Notice the `` style for sample!\nex$`sample!`(MCMC)\n\n\n&lt;Julia object of type ExtendedRtIrtModeling.GibbsRtIrt&gt;\nExtendedRtIrtModeling.GibbsRtIrt(ExtendedRtIrtModeling.SimConditions(300, 10, 4, 3000, 3, 1500, 1, 10, 0.5, 0.5), ExtendedRtIrtModeling.InputData4R([0 0 â€¦ 0 0; 0 0 â€¦ 0 0; â€¦ ; 0 0 â€¦ 1 0; 0 1 â€¦ 1 0], [-0.5 -0.5 â€¦ -0.5 -0.5; -0.5 -0.5 â€¦ -0.5 -0.5; â€¦ ; -0.5 -0.5 â€¦ 0.5 -0.5; -0.5 0.5 â€¦ 0.5 -0.5], [19.31727937946127 68.3745035485581 â€¦ 9.006981510434914 17.778680238058843; 46.89917102861648 54.38019363641357 â€¦ 12.085422811327593 56.71280369681312; â€¦ ; 27.194098966393437 42.182270397465956 â€¦ 25.584840292351714 119.46219960089533; 35.05786562994266 82.26946350420168 â€¦ 43.991656898689 24.754319523777408], [2.961 4.225 â€¦ 2.198 2.878; 3.848 3.996 â€¦ 2.492 4.038; â€¦ ; 3.303 3.742 â€¦ 3.242 4.783; 3.557 4.41 â€¦ 3.784 3.209], [0.0 -3.362 -0.2 1.063; 1.0 -0.081 2.347 1.063; â€¦ ; 1.0 -0.064 0.445 -0.318; 0.0 -0.449 -0.676 1.063]), Float64[], ExtendedRtIrtModeling.InputPara([0.3137423041364331 0.3415237894886167 â€¦ 0.11183446557866958 0.05790546531201218; 0.25638289531148994 0.3566247394185408 â€¦ 0.09091133809565621 0.05204210938445989; â€¦ ; 0.16332544992167716 0.32252191915665784 â€¦ 0.3726970452403918 0.20414636916984366; 0.16949649737031303 0.33905817515050696 â€¦ 0.09826986769390608 0.265528224767851], [-1.578676215807743; -0.9203745423441859; â€¦ ; -0.10664096875232022; -1.0594805763603632;;], [1.0445433708534635; 1.2644831163990256; â€¦ ; 1.5193557431767508; 1.4485813534579228;;], [-0.38647204662333506; -1.2538527924663074; â€¦ ; -0.7752956269598285; 0.14931123796642268;;], [0.43162195514686763; 0.21061616328551705; â€¦ ; -0.22905611375794022; 0.08899580003777864;;], [3.30761010661482; 3.927589114707531; â€¦ ; 2.923971130851645; 3.725603662587948;;], [0.39824774784188943; 0.29277495083742106; â€¦ ; 0.2886170757599006; 0.22597973246093783;;], Float64[], [0.0 0.0; -0.1287760810605439 0.2005212948416813; â€¦ ; 0.24017749307214029 -0.03657573094224067; 0.008839392786410502 -0.025342572043680473], Float64[], [1.2044553464348549 -0.02744833933113381; -0.02744833933113381 0.16019891442664336]), ExtendedRtIrtModeling.OutputPost([-2.6646011859395102 -0.7085410419736089 â€¦ -0.7565130647503794 0.3286140470016074; -3.0079544580340505 -1.1650659603554177 â€¦ -2.071385732680604 -0.054216474343903594; â€¦ ; -1.8557839828202527 -0.393936588592598 â€¦ -0.5882928400994076 0.15979568723647153; -3.1041380789618436 -0.35870758669354397 â€¦ -0.6967538992227875 0.06328643828884624;;; -0.579926218388121 -0.7694249965520048 â€¦ -3.1140451246568315 2.7402901051293598; -3.075016116622655 -1.2406363622468877 â€¦ -2.503322534120851 0.13695410653130946; â€¦ ; -2.1335402869236435 -0.43925842963308437 â€¦ -0.6019945753771228 -0.03565198116535539; -2.522017090166128 -0.5649540622562687 â€¦ -0.6170559255436903 0.19848121170487343;;; -0.21804544538641857 -0.676995006519194 â€¦ -3.4915357429600893 0.616787024738047; -3.2312874601622315 -1.0896767020878362 â€¦ -1.6598629898813122 0.24991302410254954; â€¦ ; -2.058305777627975 0.062354750660974304 â€¦ -0.7882966622933067 0.025527360107749916; -1.578676215807743 -0.9203745423441859 â€¦ -0.7752956269598285 0.14931123796642268], [-0.0425113941831008 0.1282335730526704 â€¦ 1.5331777368989858 1.7535271264893457; 0.3638056929725671 -0.21324689798075652 â€¦ 0.3052904922863609 0.24033474721531817; â€¦ ; 0.4500141724394335 0.3545728122723381 â€¦ 0.3455600652811882 0.28449342578509995; 0.47014791738809614 0.019075281312259804 â€¦ 0.3242380318762755 0.2275179082474999;;; 0.17732270162233285 -0.17177064495680683 â€¦ 0.38833920211163014 0.4249138574806367; 0.5220091561873619 0.08732510564249564 â€¦ 0.3201903557326946 0.23361730578487278; â€¦ ; 0.4851239544558618 0.15852466776851742 â€¦ 0.2954354859256739 0.28978480634782655; 0.47977068862041994 0.10186204831034498 â€¦ 0.25952115767849615 0.2565337283085566;;; 0.33191292942991374 0.16440927325200227 â€¦ 0.27058856993690944 0.267348416884789; 0.547237380074301 0.545919153533114 â€¦ 0.26898546105113474 0.30199737061250354; â€¦ ; 0.644551828361199 0.11662762772835153 â€¦ 0.3059250699897882 0.23977366140057393; 0.43162195514686763 0.21061616328551705 â€¦ 0.2886170757599006 0.22597973246093783], [0.0 -0.008071132892489258 â€¦ -0.04385131392470358 1.0198932915748098; 0.0 -0.14345204339751882 â€¦ 0.025654283556167064 0.20401456528600365; â€¦ ; 0.0 -0.0641979689318157 â€¦ -0.05137005697139327 0.16457547267983416; 0.0 -0.12147626570379585 â€¦ -0.0019527630286848394 0.16191524109541255;;; 0.0 -0.18717208184736112 â€¦ -0.07618324595309203 0.3322180492450072; 0.0 -0.09180706041632986 â€¦ 0.004693343977941498 0.20421324356814724; â€¦ ; 0.0 -0.12074793470842118 â€¦ -0.031644537136957834 0.15784539422938612; 0.0 -0.052943956404781334 â€¦ -0.031277558977574706 0.15679446856218407;;; 0.0 -0.17231495142403486 â€¦ -0.08223291509286375 0.22461367602728788; 0.0 -0.05853035695319146 â€¦ -0.018305599288515745 0.1666650137419096; â€¦ ; 0.0 -0.03999951218457838 â€¦ -0.054987581840916844 0.1894554448424559; 0.0 -0.1287760810605439 â€¦ -0.02744833933113381 0.16019891442664336], [-6569.736753186094; -4842.764638091079; â€¦ ; -4473.173949907039; -4487.210078406651;;; -5175.308294452186; -4733.926409728214; â€¦ ; -4481.404160259903; -4462.206318327685;;; -4977.26606658747; -4689.974515372751; â€¦ ; -4478.351068758786; -4489.820864529613], ExtendedRtIrtModeling.InputPara(Float64[], [-2.339140454802746, -0.8909902414379508, -1.1585403208427778, -1.3377675192797214, -1.3198749686220312, -2.1598378084914684, -1.5504785037230635, -0.8934914953299252, -1.4086145497748246, -1.0481788260462772  â€¦  -0.5338470723604528, -0.5671380168192632, -0.30805574544862707, -0.7757490378144447, -0.4395734278679758, -0.642476359839492, -0.8945642160907551, -0.7441127733988433, -0.3685888055857756, -0.8378304510800633], [1.3649196787327174, 1.6242478162295646, 1.8786325288283046, 2.372861758278475, 1.1731708612135332, 1.6563893360202497, 1.929380648727127, 0.8488668227020303, 1.4099641876226139, 1.639109576090664], [-0.46014231920221, -0.9082292498630915, 0.0026549300628270676, -0.03611296191425276, -0.1819282742645006, -0.7138704020640786, -1.7037110085735492, -1.165853395209599, -0.6773706604262375, 0.036880122106446644], [0.5114266254432426, 0.1843530644597257, -0.07963821093280457, 0.40812578462251875, 0.20470059908612467, 0.8011944318530605, 0.4831962809053242, 0.6291542010773029, 0.22979667406161905, -0.023816094617968145  â€¦  -0.6149134860670965, -0.29321154046497633, -0.15129580655926447, -0.4500161352318017, -0.03553249606029144, -0.5698173452921096, -0.014884078023460852, 0.28790323134123963, -0.18763900108587359, -0.2253154411481993], [3.3099877211930604, 3.8859688206968706, 4.584338076511737, 3.8260526200195115, 3.035070871167548, 3.2470790614614486, 2.987014600550239, 3.711404444961663, 2.8721389617221793, 3.742665330848496], [0.3789923746830642, 0.30514640194298825, 0.2964493508162058, 0.3485604176039491, 0.3337010438357073, 0.4805703515210315, 0.26507844934737856, 0.2938075495956295, 0.29601557353109936, 0.25207545571197304], Float64[], [0.0, -0.06682429491245291, 0.5396322579382782, 0.20926088101101464, -0.0055328740907218, 0.0, 0.1925441845060982, 0.003392391671879015, -0.036044007677121966, -0.021190721760734818], Float64[], [1.007975743291263, -0.042295263001564415, -0.042295263001564415, 0.17602999557781845])))\n\n\nCode\nex$coef(MCMC)\n\n\n&gt;&gt; Results for ExtendedRtIrtModeling.GibbsRtIrt.\n1) Item Parameters.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Item â”‚       a â”‚       b â”‚       Î» â”‚     ÏƒÂ²t â”‚\nâ”‚ Int64 â”‚ Float64 â”‚ Float64 â”‚ Float64 â”‚ Float64 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     1 â”‚   1.365 â”‚  -0.460 â”‚   3.310 â”‚   0.379 â”‚\nâ”‚     2 â”‚   1.624 â”‚  -0.908 â”‚   3.886 â”‚   0.305 â”‚\nâ”‚     3 â”‚   1.879 â”‚   0.003 â”‚   4.584 â”‚   0.296 â”‚\nâ”‚     4 â”‚   2.373 â”‚  -0.036 â”‚   3.826 â”‚   0.349 â”‚\nâ”‚     5 â”‚   1.173 â”‚  -0.182 â”‚   3.035 â”‚   0.334 â”‚\nâ”‚     6 â”‚   1.656 â”‚  -0.714 â”‚   3.247 â”‚   0.481 â”‚\nâ”‚     7 â”‚   1.929 â”‚  -1.704 â”‚   2.987 â”‚   0.265 â”‚\nâ”‚     8 â”‚   0.849 â”‚  -1.166 â”‚   3.711 â”‚   0.294 â”‚\nâ”‚     9 â”‚   1.410 â”‚  -0.677 â”‚   2.872 â”‚   0.296 â”‚\nâ”‚    10 â”‚   1.639 â”‚   0.037 â”‚   3.743 â”‚   0.252 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n2) Covariance of Person Parameters.\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Coef â”‚      Î¸ â”‚      Î¶ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    Î¸ â”‚  1.008 â”‚ -0.042 â”‚\nâ”‚    Î¶ â”‚ -0.042 â”‚  0.176 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n3) Regression Coefficients.\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Coef â”‚      Î¸ â”‚      Î¶ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   Î²0 â”‚  0.000 â”‚  0.000 â”‚\nâ”‚   Î²1 â”‚ -0.067 â”‚  0.193 â”‚\nâ”‚   Î²2 â”‚  0.540 â”‚  0.003 â”‚\nâ”‚   Î²3 â”‚  0.209 â”‚ -0.036 â”‚\nâ”‚   Î²4 â”‚ -0.006 â”‚ -0.021 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n4) Criterion.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Deviance â”‚      DIC â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 8960.638 â”‚ 9604.430 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\nex$precis(MCMC)\n\n\n&gt;&gt; Results for ExtendedRtIrtModeling.GibbsRtIrt.\n1) Item Response Model.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ parameters â”‚    mean â”‚     std â”‚      ess â”‚    rhat â”‚     q05 â”‚     q95 â”‚    Sig â”‚\nâ”‚     Symbol â”‚ Float64 â”‚ Float64 â”‚  Float64 â”‚ Float64 â”‚ Float64 â”‚ Float64 â”‚ String â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚         a1 â”‚   1.365 â”‚   0.215 â”‚ 2031.342 â”‚   0.999 â”‚   1.027 â”‚   1.729 â”‚      * â”‚\nâ”‚         a2 â”‚   1.624 â”‚   0.280 â”‚ 1023.142 â”‚   1.000 â”‚   1.213 â”‚   2.120 â”‚      * â”‚\nâ”‚         a3 â”‚   1.879 â”‚   0.284 â”‚ 1340.683 â”‚   1.000 â”‚   1.440 â”‚   2.374 â”‚      * â”‚\nâ”‚         a4 â”‚   2.373 â”‚   0.411 â”‚  796.093 â”‚   1.008 â”‚   1.796 â”‚   3.093 â”‚      * â”‚\nâ”‚         a5 â”‚   1.173 â”‚   0.181 â”‚ 2488.416 â”‚   1.000 â”‚   0.888 â”‚   1.475 â”‚      * â”‚\nâ”‚         a6 â”‚   1.656 â”‚   0.266 â”‚ 1307.783 â”‚   1.001 â”‚   1.256 â”‚   2.143 â”‚      * â”‚\nâ”‚         a7 â”‚   1.929 â”‚   0.438 â”‚  332.926 â”‚   1.006 â”‚   1.309 â”‚   2.761 â”‚      * â”‚\nâ”‚         a8 â”‚   0.849 â”‚   0.173 â”‚ 1622.491 â”‚   1.000 â”‚   0.564 â”‚   1.137 â”‚      * â”‚\nâ”‚         a9 â”‚   1.410 â”‚   0.224 â”‚ 1572.424 â”‚   1.000 â”‚   1.069 â”‚   1.798 â”‚      * â”‚\nâ”‚        a10 â”‚   1.639 â”‚   0.236 â”‚ 1900.847 â”‚   1.000 â”‚   1.275 â”‚   2.048 â”‚      * â”‚\nâ”‚         b1 â”‚  -0.460 â”‚   0.127 â”‚ 2354.512 â”‚   1.000 â”‚  -0.672 â”‚  -0.258 â”‚      * â”‚\nâ”‚         b2 â”‚  -0.908 â”‚   0.133 â”‚ 1369.807 â”‚   1.000 â”‚  -1.136 â”‚  -0.700 â”‚      * â”‚\nâ”‚         b3 â”‚   0.003 â”‚   0.108 â”‚ 2045.609 â”‚   1.000 â”‚  -0.172 â”‚   0.180 â”‚        â”‚\nâ”‚         b4 â”‚  -0.036 â”‚   0.098 â”‚ 1978.555 â”‚   1.000 â”‚  -0.195 â”‚   0.122 â”‚        â”‚\nâ”‚         b5 â”‚  -0.182 â”‚   0.133 â”‚ 2616.222 â”‚   1.000 â”‚  -0.404 â”‚   0.038 â”‚        â”‚\nâ”‚         b6 â”‚  -0.714 â”‚   0.122 â”‚ 2039.870 â”‚   1.000 â”‚  -0.920 â”‚  -0.521 â”‚      * â”‚\nâ”‚         b7 â”‚  -1.704 â”‚   0.206 â”‚  493.275 â”‚   1.003 â”‚  -2.063 â”‚  -1.398 â”‚      * â”‚\nâ”‚         b8 â”‚  -1.166 â”‚   0.263 â”‚ 1989.826 â”‚   1.000 â”‚  -1.635 â”‚  -0.805 â”‚      * â”‚\nâ”‚         b9 â”‚  -0.677 â”‚   0.130 â”‚ 2086.720 â”‚   1.000 â”‚  -0.899 â”‚  -0.470 â”‚      * â”‚\nâ”‚        b10 â”‚   0.037 â”‚   0.110 â”‚ 2444.179 â”‚   1.000 â”‚  -0.142 â”‚   0.215 â”‚        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n2) Response Time Model.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ parameters â”‚    mean â”‚     std â”‚      ess â”‚    rhat â”‚     q05 â”‚     q95 â”‚    Sig â”‚\nâ”‚     Symbol â”‚ Float64 â”‚ Float64 â”‚  Float64 â”‚ Float64 â”‚ Float64 â”‚ Float64 â”‚ String â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚         Î»1 â”‚   3.310 â”‚   0.044 â”‚ 1795.493 â”‚   1.001 â”‚   3.238 â”‚   3.383 â”‚      * â”‚\nâ”‚         Î»2 â”‚   3.886 â”‚   0.041 â”‚ 1878.261 â”‚   1.002 â”‚   3.819 â”‚   3.954 â”‚      * â”‚\nâ”‚         Î»3 â”‚   4.584 â”‚   0.040 â”‚ 1764.888 â”‚   1.001 â”‚   4.519 â”‚   4.650 â”‚      * â”‚\nâ”‚         Î»4 â”‚   3.826 â”‚   0.043 â”‚ 1799.599 â”‚   1.003 â”‚   3.757 â”‚   3.895 â”‚      * â”‚\nâ”‚         Î»5 â”‚   3.035 â”‚   0.042 â”‚ 1748.269 â”‚   1.003 â”‚   2.965 â”‚   3.105 â”‚      * â”‚\nâ”‚         Î»6 â”‚   3.247 â”‚   0.048 â”‚ 2179.353 â”‚   1.002 â”‚   3.169 â”‚   3.326 â”‚      * â”‚\nâ”‚         Î»7 â”‚   2.987 â”‚   0.039 â”‚ 1727.063 â”‚   1.002 â”‚   2.922 â”‚   3.052 â”‚      * â”‚\nâ”‚         Î»8 â”‚   3.711 â”‚   0.041 â”‚ 1675.810 â”‚   1.002 â”‚   3.645 â”‚   3.779 â”‚      * â”‚\nâ”‚         Î»9 â”‚   2.872 â”‚   0.041 â”‚ 1705.345 â”‚   1.001 â”‚   2.805 â”‚   2.942 â”‚      * â”‚\nâ”‚        Î»10 â”‚   3.743 â”‚   0.039 â”‚ 1775.891 â”‚   1.002 â”‚   3.679 â”‚   3.805 â”‚      * â”‚\nâ”‚       ÏƒÂ²t1 â”‚   0.379 â”‚   0.034 â”‚ 4503.800 â”‚   1.000 â”‚   0.326 â”‚   0.438 â”‚      * â”‚\nâ”‚       ÏƒÂ²t2 â”‚   0.305 â”‚   0.027 â”‚ 4309.889 â”‚   1.000 â”‚   0.263 â”‚   0.352 â”‚      * â”‚\nâ”‚       ÏƒÂ²t3 â”‚   0.296 â”‚   0.027 â”‚ 4360.224 â”‚   1.000 â”‚   0.255 â”‚   0.343 â”‚      * â”‚\nâ”‚       ÏƒÂ²t4 â”‚   0.349 â”‚   0.031 â”‚ 4233.063 â”‚   1.001 â”‚   0.301 â”‚   0.403 â”‚      * â”‚\nâ”‚       ÏƒÂ²t5 â”‚   0.334 â”‚   0.030 â”‚ 4470.407 â”‚   1.000 â”‚   0.287 â”‚   0.385 â”‚      * â”‚\nâ”‚       ÏƒÂ²t6 â”‚   0.481 â”‚   0.042 â”‚ 3763.722 â”‚   1.000 â”‚   0.415 â”‚   0.554 â”‚      * â”‚\nâ”‚       ÏƒÂ²t7 â”‚   0.265 â”‚   0.024 â”‚ 4348.786 â”‚   1.000 â”‚   0.228 â”‚   0.309 â”‚      * â”‚\nâ”‚       ÏƒÂ²t8 â”‚   0.294 â”‚   0.026 â”‚ 4179.601 â”‚   1.001 â”‚   0.253 â”‚   0.339 â”‚      * â”‚\nâ”‚       ÏƒÂ²t9 â”‚   0.296 â”‚   0.026 â”‚ 4486.570 â”‚   1.000 â”‚   0.255 â”‚   0.341 â”‚      * â”‚\nâ”‚      ÏƒÂ²t10 â”‚   0.252 â”‚   0.023 â”‚ 4012.436 â”‚   1.000 â”‚   0.216 â”‚   0.293 â”‚      * â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n3) Structural Model.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ parameters â”‚    mean â”‚     std â”‚      ess â”‚    rhat â”‚     q05 â”‚     q95 â”‚    Sig â”‚\nâ”‚     Symbol â”‚ Float64 â”‚ Float64 â”‚  Float64 â”‚ Float64 â”‚ Float64 â”‚ Float64 â”‚ String â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     Î²[0,1] â”‚   0.000 â”‚   0.000 â”‚      NaN â”‚     NaN â”‚   0.000 â”‚   0.000 â”‚      * â”‚\nâ”‚     Î²[1,1] â”‚  -0.067 â”‚   0.064 â”‚ 3287.917 â”‚   1.000 â”‚  -0.172 â”‚   0.037 â”‚        â”‚\nâ”‚     Î²[2,1] â”‚   0.540 â”‚   0.044 â”‚ 1181.655 â”‚   1.001 â”‚   0.469 â”‚   0.612 â”‚      * â”‚\nâ”‚     Î²[3,1] â”‚   0.209 â”‚   0.035 â”‚ 2558.694 â”‚   1.001 â”‚   0.153 â”‚   0.266 â”‚      * â”‚\nâ”‚     Î²[4,1] â”‚  -0.006 â”‚   0.032 â”‚ 3458.413 â”‚   0.999 â”‚  -0.057 â”‚   0.046 â”‚        â”‚\nâ”‚     Î²[0,2] â”‚   0.000 â”‚   0.000 â”‚      NaN â”‚     NaN â”‚   0.000 â”‚   0.000 â”‚      * â”‚\nâ”‚     Î²[1,2] â”‚   0.193 â”‚   0.019 â”‚ 3710.222 â”‚   1.001 â”‚   0.160 â”‚   0.225 â”‚      * â”‚\nâ”‚     Î²[2,2] â”‚   0.003 â”‚   0.010 â”‚ 3848.907 â”‚   1.000 â”‚  -0.013 â”‚   0.019 â”‚        â”‚\nâ”‚     Î²[3,2] â”‚  -0.036 â”‚   0.010 â”‚ 3981.757 â”‚   1.000 â”‚  -0.052 â”‚  -0.020 â”‚      * â”‚\nâ”‚     Î²[4,2] â”‚  -0.021 â”‚   0.010 â”‚ 4063.009 â”‚   1.000 â”‚  -0.037 â”‚  -0.006 â”‚      * â”‚\nâ”‚     Î£[1,1] â”‚   1.008 â”‚   0.114 â”‚ 2541.021 â”‚   1.001 â”‚   0.835 â”‚   1.204 â”‚      * â”‚\nâ”‚     Î£[1,2] â”‚  -0.042 â”‚   0.028 â”‚ 4110.644 â”‚   1.000 â”‚  -0.089 â”‚   0.005 â”‚        â”‚\nâ”‚     Î£[2,1] â”‚  -0.042 â”‚   0.028 â”‚ 4110.644 â”‚   1.000 â”‚  -0.089 â”‚   0.005 â”‚        â”‚\nâ”‚     Î£[2,2] â”‚   0.176 â”‚   0.017 â”‚ 4055.249 â”‚   1.001 â”‚   0.149 â”‚   0.206 â”‚      * â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLastly, letâ€™s check out this package.\n\n\nCode\n## See the objects in ex.\nls(ex)\n\n\n [1] \"coef\"                          \"drawItemDifficulty\"           \n [3] \"drawItemDiscrimination\"        \"drawItemIntensity\"            \n [5] \"drawItemIntensityCross\"        \"drawItemIntensityCrossQr\"     \n [7] \"drawItemTimeResidual\"          \"drawItemTimeResidualCross\"    \n [9] \"drawItemTimeResidualCrossQr\"   \"drawQrWeightsCrossQr\"         \n[11] \"drawQrWeightsLatentQr\"         \"drawRaPgRandomVariable\"       \n[13] \"drawSubjAbility\"               \"drawSubjAbilityNull\"          \n[15] \"drawSubjCoefficients\"          \"drawSubjCorrCross\"            \n[17] \"drawSubjCorrCrossQr\"           \"drawSubjCovariance\"           \n[19] \"drawSubjCovariance2One\"        \"drawSubjCovarianceLatent\"     \n[21] \"drawSubjCovarianceLatentQr\"    \"drawSubjCovarianceNull\"       \n[23] \"drawSubjCovarianceNull2One\"    \"drawSubjSpeed\"                \n[25] \"drawSubjSpeedCross\"            \"drawSubjSpeedCrossQr\"         \n[27] \"drawSubjSpeedLatent\"           \"drawSubjSpeedLatentQr\"        \n[29] \"drawSubjSpeedNull\"             \"eval\"                         \n[31] \"evaluate\"                      \"getBias\"                      \n[33] \"getDic\"                        \"getLogLikelihoodMlIrt\"        \n[35] \"getLogLikelihoodRtIrt\"         \"getLogLikelihoodRtIrtCross\"   \n[37] \"getLogLikelihoodRtIrtCrossQr\"  \"getLogLikelihoodRtIrtLatent\"  \n[39] \"getLogLikelihoodRtIrtLatentQr\" \"getLogLikelihoodRtIrtNull\"    \n[41] \"getPrecisTable\"                \"getRmse\"                      \n[43] \"getSubjCoefficients\"           \"getSubjCoefficientsLatent\"    \n[45] \"getSubjCoefficientsLatentQr\"   \"getSubjCoefficientsMlIrt\"     \n[47] \"GibbsMlIrt\"                    \"GibbsRtIrt\"                   \n[49] \"GibbsRtIrt2\"                   \"GibbsRtIrtCross\"              \n[51] \"GibbsRtIrtCross2\"              \"GibbsRtIrtCrossQr\"            \n[53] \"GibbsRtIrtLatent\"              \"GibbsRtIrtLatent2\"            \n[55] \"GibbsRtIrtLatentQr\"            \"GibbsRtIrtNull\"               \n[57] \"include\"                       \"InputData\"                    \n[59] \"InputData4R\"                   \"InputPara\"                    \n[61] \"OutputDic\"                     \"OutputPost\"                   \n[63] \"OutputPostCross\"               \"OutputPostCrossQr\"            \n[65] \"OutputPostMlIrt\"               \"OutputPostRtIrtLatent\"        \n[67] \"OutputPostRtIrtLatentQr\"       \"precis\"                       \n[69] \"sample!\"                       \"setCond\"                      \n[71] \"setTrueParaRtIrt\"              \"SimConditions\"                \n[73] \"SimEvaluation\""
  }
]